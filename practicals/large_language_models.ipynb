{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# LLMs for everyone\n",
        "\n",
        "<img src=\"https://www.marktechpost.com/wp-content/uploads/2023/05/Blog-Banner-3.jpg\" width=\"60%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "© Deep Learning Indaba 2023. Apache License 2.0.\n",
        "\n",
        "**Authors: Ruan van der Merwe, Marianne Monteiro, Everlyn Asiko Chimoto**\n",
        "\n",
        "**Reviewers:Natasha Latysheva, Amrit Purshotam, Tom Makkink**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Welcome to \"LLMs for Everyone,\" a practical exploration into the captivating world of Large Language Models (LLMs)! This entire introductory block of text was crafted solely by ChatGPT, showcasing the remarkable capabilities of these models. Throughout this tutorial, we will delve into the underlying fundamentals of transformers, the powerful technology that drives models like GPT, and learn how to fine-tune and train our very own Language Models. Let's embark on this exciting journey of understanding and creating LLMs, and discover how such impressive AI text generation is made possible! 🚀📚\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: [<font color='green'>Attention Mechanism</font>, <font color='green'>Transformer Architecture</font>, <font color='blue'>LoRA</font>]\n",
        "\n",
        "Level: <font color='orange'>Beginner</font>, <font color='green'>Intermediate</font>, <font color='blue'>Advanced</font>\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "* Understand the idea behind [Attention](https://arxiv.org/abs/1706.03762) and why it is used.\n",
        "* Present and describe the fundamental building blocks of the [Transformer Architecture](https://arxiv.org/abs/1706.03762) along with an intuition on such an architecture design.\n",
        "* Present and explain intuition behind [LoRA](https://arxiv.org/abs/2106.09685) along with a simplified demo of how to finetune a LLM using LoRA on a single GPU using [Hugging Face](https://huggingface.co)\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "* Introductory knowledge of Deep Learning.\n",
        "* Introductory knowledge of NLP.\n",
        "* Introductory knowledge of sequence to sequence models.\n",
        "* Linear algebra basic understanding.\n",
        "\n",
        "**Outline:**\n",
        "\n",
        ">[LLMs for everyone](#scrollTo=m2s4kN_QPQVe)\n",
        "\n",
        ">>[Installations, Imports and Helper Functions](#scrollTo=6EqhIg1odqg0)\n",
        "\n",
        ">>[Let's kick things off with a Hugging Face Demo! Beginner](#scrollTo=4zu5cg-YG4XU)\n",
        "\n",
        ">>>[Hugging Face](#scrollTo=AwjIIipOG4fz)\n",
        "\n",
        ">>>[Time for a demo! ⏰⚡ Load Hugging Face model and run sample](#scrollTo=eq46TV_0G4f0)\n",
        "\n",
        ">>[1. Attention](#scrollTo=-ZUp8i37dFbU)\n",
        "\n",
        ">>>[Intuition - Beginner](#scrollTo=ygdi884ugGcu)\n",
        "\n",
        ">>>[Sequence to sequence attenion mechanisms - Intermediate](#scrollTo=aQfqM1EJyDXI)\n",
        "\n",
        ">>>[Self-attention to Multihead Attention - Intermediate](#scrollTo=J-MU6rrny8Nj)\n",
        "\n",
        ">>>>[Self-attention](#scrollTo=0AFUEFZGzCTv)\n",
        "\n",
        ">>>>>[Queries, keys and values](#scrollTo=pwOIMtdZzdTf)\n",
        "\n",
        ">>>>>[Scaled dot product attention](#scrollTo=OhGZHFsHz_Qp)\n",
        "\n",
        ">>>>>[Masked attention](#scrollTo=D7B-AgO80gIt)\n",
        "\n",
        ">>>>[Multihead Attention - Advanced](#scrollTo=hNHklaSV1Tej)\n",
        "\n",
        ">>[2. Building your own LLM](#scrollTo=e9NW58_3hAg2)\n",
        "\n",
        ">>>[2.1 High-level overvierw Beginner](#scrollTo=bA_2coZvhAg3)\n",
        "\n",
        ">>>[2.2 Tokenization + Positional encoding Beginner](#scrollTo=fbTsk0MdhAhC)\n",
        "\n",
        ">>>>[2.2.1 Tokenization](#scrollTo=DehUpfym_RF8)\n",
        "\n",
        ">>>>[2.2.2 Positional encodings](#scrollTo=639s7Zuk_RF9)\n",
        "\n",
        ">>>>>[Sine and cosine functions](#scrollTo=rklY-aL-_RF9)\n",
        "\n",
        ">>>[2.3 Transformer block   Intermediate](#scrollTo=SdNPg0pnhAhG)\n",
        "\n",
        ">>>>[2.3.1 Feed Forward Network (FFN) / Multilayer perceptron (MLP) Beginner](#scrollTo=kTURbfr__RF-)\n",
        "\n",
        ">>>>[2.3.2 Add and Norm block Beginner](#scrollTo=Sts5Vr4i_RF-)\n",
        "\n",
        ">>>[2.4 Building the Transformer Decoder / LLM Intermediate](#scrollTo=91dXd29b_RF_)\n",
        "\n",
        ">>>[2.5 Training your LLM](#scrollTo=wmt3tp38G90A)\n",
        "\n",
        ">>>>[2.5.1 Training objective Intermediate](#scrollTo=agLIpsoh_RGA)\n",
        "\n",
        ">>>>[2.5.2 Training models Advanced](#scrollTo=4CSfvGj__RGA)\n",
        "\n",
        ">>>>[2.5.3 Inspecting the trained LLM Beginner](#scrollTo=pGv9c2AFmF4V)\n",
        "\n",
        ">>[Efficiently Finetuning LLMs with Hugging Face](#scrollTo=C4hKnTFbHtdM)\n",
        "\n",
        ">>>[3.1 Adapter and Fine-Tuning methods  Intermediate](#scrollTo=KoTvhvap_RGC)\n",
        "\n",
        ">>>>[3.1.1 Prefix tuning](#scrollTo=znctvjrE_RGC)\n",
        "\n",
        ">>>>[3.2.1 Adapter Methods](#scrollTo=U4sLxSol_RGD)\n",
        "\n",
        ">>>[3.2 LoRA Beginner, Intermediate, Advanced](#scrollTo=MoBc08xY_RGD)\n",
        "\n",
        ">>>>[3.2.1 LoRA implementation Advanced](#scrollTo=ri1FGEh6_RGE)\n",
        "\n",
        ">>>>[3.2.3 🤗 Deep dive into LoRA with Hugging Face! 🤗 Beginner](#scrollTo=mpCz5otl_RGE)\n",
        "\n",
        ">>>>>[Gathering and processing data (optional)](#scrollTo=NwZbrcFY_RGF)\n",
        "\n",
        ">>>>>[Finetune a model with LoRA](#scrollTo=qUAjpRx3_RGG)\n",
        "\n",
        ">>>[⏰⚡ Demo Time with our trained model🚀😰](#scrollTo=JkWj5bxd_RGG)\n",
        "\n",
        ">>[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
        "\n",
        ">[Feedback](#scrollTo=o1ndpYE50BpG)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n",
        "[Any other tasks just before starting.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952qogb79nnY"
      },
      "source": [
        "**Suggested experience level in this topic:**\n",
        "\n",
        "| Level         | Experience                            |\n",
        "| --- | --- |\n",
        "`Beginner`      | It is my first time being introduced to this work. |\n",
        "`Intermediate`  | I have done some basic courses/intros on this topic. |\n",
        "`Advanced`      | I work in this area/topic daily. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YBdDHcI_ArCR"
      },
      "outputs": [],
      "source": [
        "# @title **Paths to follow:** What is your level of experience in the topics presented in this notebook? (Run Cell)\n",
        "experience = \"advanced\" #@param [\"beginner\", \"intermediate\", \"advanced\"]\n",
        "sections_to_follow=\"\"\n",
        "\n",
        "\n",
        "if experience == \"beginner\": sections_to_follow = \"\"\"we recommend you to not attempt to do every coding task but instead, skip through to every section and ensure you interact with the LoRA finetuned LLM presented in the last section as well as with the pretrained LLM to get a practical understanding of how these models behave\"\"\"\n",
        "\n",
        "elif experience == \"intermediate\": sections_to_follow = \"\"\"we recommend you go through every section in this notebook and try the coding tasks tagged as beginner or intermediate. If you get stuck on the code ask a tutor for help or move on to better use the time of the practical\"\"\"\n",
        "\n",
        "elif experience == \"advanced\": sections_to_follow = \"\"\"we recommend you go through every section and try every coding task until you get it to work\"\"\"\n",
        "\n",
        "\n",
        "print(f\"Based on your experience, {sections_to_follow}.\\nNote: this is just a guideline, feel free to explore the colab as you'd like if you feel comfort able!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installations, Imports and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "!pip install transformers datasets\n",
        "!pip install seaborn umap-learn\n",
        "!pip install livelossplot\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install -q peft\n",
        "\n",
        "# Python utils\n",
        "!pip install -q ipdb      # debugging.\n",
        "!pip install -q colorama  # print colors :).\n",
        "\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if os.environ[\"COLAB_GPU\"] and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "\n",
        "# https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html#gpu-memory-allocation\n",
        "# Avoid GPU memory allocation to be done by JAX.\n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
        "\n",
        "import chex\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import optax\n",
        "\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "import peft\n",
        "\n",
        "from PIL import Image\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "# Utils.\n",
        "import colorama\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# download images used in notebook\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80\",\n",
        "    \"cat.png\",\n",
        ")\n",
        "\n",
        "import copy\n",
        "\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"word2vec_sample\")\n",
        "\n",
        "import huggingface_hub\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-9X10jhocGaS"
      },
      "outputs": [],
      "source": [
        "# @title Helper Plotting Functions. (Run Cell)\n",
        "def plot_position_encodings(P, max_tokens, d_model):\n",
        "    \"\"\"Function that takes in a position encoding matrix and plots it.\"\"\"\n",
        "\n",
        "    plt.figure(figsize=(20, np.min([8, max_tokens])))\n",
        "    im = plt.imshow(P, aspect=\"auto\", cmap=\"Blues_r\")\n",
        "    plt.colorbar(im, cmap=\"blue\")\n",
        "\n",
        "    if d_model <= 64:\n",
        "        plt.xticks(range(d_model))\n",
        "    if max_tokens <= 32:\n",
        "        plt.yticks(range(max_tokens))\n",
        "    plt.xlabel(\"Embedding index\")\n",
        "    plt.ylabel(\"Position index\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_image_patches(patches):\n",
        "    \"\"\"Function that takes in a list of patches and plots them.\"\"\"\n",
        "    axes = []\n",
        "    fig = plt.figure(figsize=(25, 25))\n",
        "    for a in range(patches.shape[1]):\n",
        "        axes.append(fig.add_subplot(1, patches.shape[1], a + 1))\n",
        "        plt.imshow(patches[0][a])\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_projected_embeddings(embeddings, labels):\n",
        "    \"\"\"Function that takes in a list of embeddings projects them onto a 2D space and plots them using UMAP.\"\"\"\n",
        "    import umap\n",
        "    import seaborn as sns\n",
        "\n",
        "    projected_embeddings = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.title(\"Projected text embeddings\")\n",
        "    sns.scatterplot(\n",
        "        x=projected_embeddings[:, 0], y=projected_embeddings[:, 1], hue=labels\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
        "    \"\"\"Function that takes in a weight matrix and plots it with custom axis ticks\"\"\"\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
        "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
        "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
        "    plt.title(\"Attention matrix\")\n",
        "    plt.xlabel(\"Attention score\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kMkaKekB_pR4"
      },
      "outputs": [],
      "source": [
        "# @title Helper Text Processing Functions. (Run Cell)\n",
        "\n",
        "def get_word2vec_embedding(words):\n",
        "    \"\"\"\n",
        "    Function that takes in a list of words and returns a list of their embeddings,\n",
        "    based on a pretrained word2vec encoder.\n",
        "    \"\"\"\n",
        "    word2vec_sample = str(find(\"models/word2vec_sample/pruned.word2vec.txt\"))\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "        word2vec_sample, binary=False\n",
        "    )\n",
        "\n",
        "    output = []\n",
        "    words_pass = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            output.append(jnp.array(model.word_vec(word)))\n",
        "            words_pass.append(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    embeddings = jnp.array(output)\n",
        "    del model  # free up space again\n",
        "    return embeddings, words_pass\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Function that takes in a string and removes all punctuation.\"\"\"\n",
        "    import re\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def print_sample(prompt: str, sample: str):\n",
        "  print(colorama.Fore.MAGENTA + prompt, end=\"\")\n",
        "  print(colorama.Fore.BLUE + sample)\n",
        "  print(colorama.Fore.RESET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zu5cg-YG4XU"
      },
      "source": [
        "## Let's kick things off with a Hugging Face Demo! <font color='orange'>Beginner</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwjIIipOG4fz"
      },
      "source": [
        "### Hugging Face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2DSHiuhG4f0"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png\" width=\"10%\">\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) is a startup founded in 2016 and, in their own words: \"are on a mission to democratize good machine learning, one commit at a time.\" Currently they are a treasure trove for tools to work on and with Large Language Model (LLMs).\n",
        "\n",
        "They have developed various open-source packages and allow users to easily interact with a large corpus of pretrained transformer models (across all modalities) and datasets to train or fine-tune pre-trained transformers. Their software is used widely in industry and research. For more details on them and usage, refer to [last years transformer practical](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/practicals/attention_and_transformers.ipynb#scrollTo=qFBw8kRx-4Mk).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xdt9PQ6G4f0"
      },
      "source": [
        "In this colab we print prompts in <font color='HotPink'><b>pink</b></font> and samples generated from a model in <font color='blue'><b>blue</b></font>  like in the example below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-8C9SJCG4f0"
      },
      "outputs": [],
      "source": [
        "print_sample(prompt='My fake prompt', sample=' is awesome!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq46TV_0G4f0"
      },
      "source": [
        "### Time for a demo! ⏰⚡ Load Hugging Face model and run sample\n",
        "\n",
        "Here we show how easy is to sample from a model loaded from Hugging Face!\n",
        "\n",
        "In this colab we pre-configured two options for models:\n",
        "* `gpt-neo-125M`: 125M parameters (faster and uses less memory! We recommend trying this one out first! If you'd like to try `gpt2-medium` restart the colab kernel and change the model name in the cell below).\n",
        "* `gpt2-medium`: 355M parameters\n",
        "\n",
        "**Note**: What we do below can even work on models such the Llama 13 billion parameter model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVV28V-TG4f1"
      },
      "outputs": [],
      "source": [
        "model_name = \"EleutherAI/gpt-neo-125M\" # @param [\"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
        "\n",
        "test_prompt = 'What is love?' # @param {type: \"string\"}\n",
        "generator = transformers.pipeline('text-generation', model=model_name)\n",
        "generator(test_prompt, do_sample=True, min_length=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5IEKl4iG4f1"
      },
      "source": [
        "**Tip:** Try running the code above with different prompts of with the same prompt but more than once!\n",
        "\n",
        "**Discussion:** Why do you think the generated text change every time even with the same prompt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfV0Qk6yG4f1"
      },
      "source": [
        "Let's implement our own `generator` like function to make it easier to load different weights for the model and configure how generation is being done. Just run the cells below 😀!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxs5bO_sG4f1"
      },
      "outputs": [],
      "source": [
        "if 'gpt2' in model_name:\n",
        "  tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n",
        "  model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
        "elif model_name == \"EleutherAI/gpt-neo-125M\":\n",
        "  tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "  model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
        "else:\n",
        "  raise NotImplementedError\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.to(\"cuda\")\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsZEwoZJG4f1"
      },
      "outputs": [],
      "source": [
        "def run_sample(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    seed: int | None = None,\n",
        "    temperature: float = 0.6,\n",
        "    top_p: float = 0.9,\n",
        "    max_new_tokens: int = 64,\n",
        ") -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    input_ids = input_ids.to(model.device)\n",
        "    attention_mask = attention_mask.to(model.device)\n",
        "\n",
        "    generation_config = transformers.GenerationConfig(\n",
        "      do_sample=True,\n",
        "      temperature=temperature,\n",
        "      top_p=top_p,\n",
        "      pad_token_id=tokenizer.pad_token_id,\n",
        "      top_k=0,\n",
        "    )\n",
        "\n",
        "    if seed is not None:\n",
        "      torch.manual_seed(seed)\n",
        "\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "    # We assume a single sample is returned to make things simpler.\n",
        "    assert len(generation_output.sequences) == 1\n",
        "    output_sequence = generation_output.sequences[0]\n",
        "    output_string = tokenizer.decode(output_sequence)\n",
        "    response = output_string.split(prompt)[1].rstrip()\n",
        "    print_sample(prompt, response)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yme6VzW4G4f1"
      },
      "outputs": [],
      "source": [
        "_ = run_sample(model, tokenizer, prompt=\"What is love?\", seed=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vnUawyG4f1"
      },
      "source": [
        "By the end of this practical we'll see how we can **finetune** a transformer model in an **efficient way** with a state of the art method called LoRA so it can perform better at a specific task out of the box, a task example is generating lyrics to a song of artists like The Beatles, Michael Jackson or even Tyler the Creator!\n",
        "\n",
        "But before we do so let's look and build an understanding of what are Large Language Models and what are the fundamental Machine Learning building blocks that make this amazing technology possible! At the core of SoTA (state of the art) Large Language Models is the Attention Mechanism and the Transformer Architecture, let's have a look at these concepts in the next sections of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **1. Attention**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acgW1ofF_RFz"
      },
      "source": [
        "The attention mechanism is inspired by how humans would look at an image or read a sentence.\n",
        "\n",
        "Let us take the image of the dog in human clothes below (image and example [source](https://lilianweng.github.io/posts/2018-06-24-attention/)). When paying *attention* to the red blocks of pixels, we will say that the yellow block of pointy ears is something we expected (correlated) but that the grey blocks of human clothes are unexpected for us (uncorrelated). This is *based on what we have seen in the past* when looking at pictures of dogs, specifically one of a Shiba Inu.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iEU7Cph2D2PCXp3YEHj30-EndhHAeB5T\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "Assume we want to identify the dog breed in this image. When we look at the red pixels, we tend to pay more *attention* to relevant pixels that are more similar or relevant to them, which could be the ones in the yellow box. We almost completely remove the snow in the background and the human clothing for this task. However, when we begin looking at the background in an attempt to identify what is in it, we will fade out the dog pixels because they are irrelevant to the current task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usLBF2g0x5gH"
      },
      "source": [
        "The same thing happens when we read. In order to understand the entire sentence, we will learn to correlate and *attend to* certain words based on the context of the entire sentence.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j23kcfu_c3wINU6DUvxzMYNmp4alhHc9\" alt=\"drawing\" width=\"350\"/>\n",
        "\n",
        " For instance, in the first sentence in the image above, when looking at the word \"coding\", we pay more attention to the word \"Apple\" and \"computer\" because we know that when we speak about coding, \"Apple\" is actually referring to the company. However, in the second sentence, we realise we should not consider \" apple \" when looking at \"code\" because given the context of the rest of the sentence, we know that this apple is referring to an actual apple and not a computer.\n",
        "\n",
        "We can build better models by developing mechanisms that mimic attention. It will enable our models to learn better representations of our input data by contextualising what it knows about some parts of the input based on other parts. In the following sections, we will delve deeper into the mechanisms that enable us to train our deep learning models to attend to input data in the context of other input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygdi884ugGcu"
      },
      "source": [
        "### Intuition - <font color='orange'>Beginner</font>\n",
        "\n",
        "Imagine attention as a mechanism that allows a neural network to focus more on certain parts of data. By doing this, the network can enhance its grasp of the problem it's working on, updating its understanding or representations accordingly.\n",
        "\n",
        "One method for implementing attention involves representing each word (or even parts of a word) using different vectors [1]. These vectors are used to measure similarity, often through a process like calculating the dot product. This similarity becomes the \"attention\" measure, which then influences the update of our original vector. To put it simply, when two word representations are similar, they're likely relevant to each other. As a result, they impact each other's representations within our neural network.\n",
        "\n",
        "To illustrate how the dot product can create meaningful attention weights, we'll use pre-trained [word2vec](https://jalammar.github.io/illustrated-word2vec/) embeddings. These word2vec embeddings are generated by a neural network that learned to create similar embeddings for words with similar meanings.\n",
        "\n",
        "Even though we might not be sequentially processing contextual information, the attention matrix should still indicate which words are correlated and therefore should \"attend\" to each other.\n",
        "\n",
        "[1] You can find more details about how this is done for LLMs in the \"Building Your Own LLM\" session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvBYShCFk6WC"
      },
      "source": [
        "**Code task** <font color='blue'>Intermediate</font>: Complete the dot product attention function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrbITGPnk7Ce"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(hidden_states, previous_state):\n",
        "  \"\"\"\n",
        "  Calculate the dot product between the hidden states and previous states.\n",
        "\n",
        "  Args:\n",
        "    hidden_states: A tensor with shape [T_hidden, dm]\n",
        "    previous_state: A tensor with shape [T_previous, dm]\n",
        "  \"\"\"\n",
        "\n",
        "  scores = # FINISH ME\n",
        "  w_n = # FINISH ME\n",
        "  c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "  return w_n, c_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QARgTrNZlIqH"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [2, 2])\n",
        "w_n, c_t = dot_product_attention(x, x)\n",
        "\n",
        "w_n_correct = jnp.array([[0.9567678, 0.04323225], [0.00121029, 0.99878967]])\n",
        "c_t_correct = jnp.array([[0.11144122, 0.95290256], [-1.5571996, -1.5321486]])\n",
        "\n",
        "assert jnp.allclose(w_n_correct, w_n), \"w_n is not calculated correctly\"\n",
        "assert jnp.allclose(c_t_correct, c_t), \"c_t is not calculated correctly\"\n",
        "\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Qa6PyKYnkzUJ"
      },
      "outputs": [],
      "source": [
        "# when changing these words, note that if the word is not in the original\n",
        "# training corpus it will not be shown in the weight matrix plot.\n",
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def dot_product_attention(hidden_states, previous_state):\n",
        "  # [T,d]*[d,N] -> [T,N]\n",
        "  scores = jnp.matmul(previous_state, hidden_states.T)\n",
        "  w_n = jax.nn.softmax(scores)\n",
        "  # [T,N]*[N,d] -> [T,d]\n",
        "  c_t = jnp.matmul(w_n, hidden_states)\n",
        "  return w_n, c_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlHL3e_QhLfq"
      },
      "outputs": [],
      "source": [
        "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
        "word_embeddings, words = get_word2vec_embedding(words)\n",
        "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
        "plot_attention_weight_matrix(weights, words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tItZU09YlhEZ"
      },
      "source": [
        "Looking at the matrix,  we can see which words have similar meanings. The \"royal\" group of words have higher attention scores with each other than the \"food\" words, which all attend to one another. We also see that \"computers\" have very low attention scores for all of them, which shows that they are neither very related to \"royal\" or \"food\" words.  \n",
        "\n",
        "**Group task:**\n",
        "  - Play with the word selections above. See if you can find word combinations whose attention values seem counter-intuitive. Think of possible explanations. Which sense of a word did the attention scores capture?\n",
        "  - Ask your friend if they found examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3iB8hf0hJdX"
      },
      "source": [
        "**Note**: Dot product is only one of the ways to implement the scoring function for attention mechanisms, there is a more extensive list in this [blog](https://lilianweng.github.io/posts/2018-06-24-attention/#summary) post by Dr Lilian Weng.\n",
        "\n",
        "More resources:\n",
        "\n",
        "[A basic encoder-decoder model for machine translation](https://www.youtube.com/watch?v=gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=1)\n",
        "\n",
        "[Training and loss for encoder-decoder models](https://www.youtube.com/watch?v=aBZUTuT1Izs&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=2)\n",
        "\n",
        "[Basic attention](https://www.youtube.com/watch?v=BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQfqM1EJyDXI"
      },
      "source": [
        "### Sequence to sequence attenion mechanisms - <font color='green'>Intermediate</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68QBeG-4yDZ9"
      },
      "source": [
        "The first attention mechanisms were used in sequence-to-sequence models. These models were usually RNN encoder and decoder structures. The input sequence was processed sequentially by an RNN, encoding the sequence in a single context vector, which is then fed into another RNN that generates a new sequence. Below is an example of this ([source](https://lilianweng.github.io/posts/2018-06-24-attention/)).\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FKfaArN1rsLjzVWaJGpMLEcxEshSLXd6\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "Due to there only being one context vector, it was often found that for longer input sequences, information gets lost due to the inability of the encoders to remember longer sequences. The attention mechanism introduced in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) was proposed to solve this.\n",
        "\n",
        "Here, instead of relying on one static context vector, which is also only used once in the decoding process, let us provide information on the entire input sequence at every decoding step using a dynamic context vector. By doing this, the decoder can access a larger \"bank\" of memory and attend to the input's required information based on the current decoder RNN output state, $s_t$. This is shown below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fB5KObXcKo5x35xlIDIcjHTq1q75ejIB\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "In deep learning, attention can be interpreted as a vector of \"importance.\" To predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate how strongly it is correlated with, or \"attends to,\" other elements using the attention vector/weights. These attention weights are then used to generate a new weighted sum of the remaining elements, which represents the target [(source)](https://lilianweng.github.io/posts/2018-06-24-attention/).\n",
        "\n",
        "\n",
        "This, usually, consists of two steps for each decoding step $t$:\n",
        "\n",
        "1. Calculate the score (importance) for each $h_n$, given $s_{t-1}$ and generate an attention vector, $w_{n}$.\n",
        "  - $\\text{score} = a(s_{t−1}, h_{n})$, where $a$ can be any differentiable function\n",
        "  - $w_{n} = \\frac{\\exp \\left\\{a\\left(s_{t-1}, h_{n}\\right)\\right\\}}{\\sum_{j=1}^{N} \\exp \\left\\{a\\left(s_{t-1}, h_{j}\\right)\\right\\}}$, where we use the softmax function to generate relative attention weights\n",
        "2. Generate the final context vector, $c_t$\n",
        "  - $c_t=\\sum_{n=1}^{N} w_n h_{n}$\n",
        "\n",
        "The final state fed into the RNN to generate $s_{t+1}$, is given below, where $f$ can again be any combination method.\n",
        "\n",
        "$s_{t+1} = f\\left ( c_t, s_t \\right)$\n",
        "\n",
        "In Bahdanau et al., 2015, $f$ was a learned feedforward layer taking in the concatenated vector $[c_t; s_t]$, with $a(s_{t−1}, h_{n})$ being the dot product. Next, let us build up this attention schema.\n",
        "\n",
        "In dot product attention, the score is given by\n",
        "\n",
        "$a(s_{t-1}, h_n)=s_{t-1} h_n^\\top$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODURfDvYyev2"
      },
      "source": [
        "In order to show how the dot product can produce attention weights that make sense, let us use pretrained [word2vec](https://jalammar.github.io/illustrated-word2vec/) embeddings. These word2vec embeddings are generated by an encoder network that was trained to generate similar embeddings for words with similar meanings.\n",
        "\n",
        "Even though we are not processing something sequentially that contains context, the attention matrix should indicate which words are correlated—and would thus attend to each other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-MU6rrny8Nj"
      },
      "source": [
        "### Self-attention to Multihead Attention - <font color='blue'>Intermediate</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRuLtxNey_EQ"
      },
      "source": [
        "Self-attention and multi-head attention (MHA) are the core building blocks for the transformer architecture. We will build up the intuition and implementation here in detail. Then in the **Transformers** section, you will see how this mechanism is utilised to build an attention only sequence-to-sequence model.\n",
        "\n",
        "\n",
        "Going forward in this section, we will represent a sentence by splitting it up into a list of words, then using the word2vec model from above to encode each word. In the transformers section, we will dive deeper into how we transform an input into a sequence of vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe1lR5_oynOR"
      },
      "outputs": [],
      "source": [
        "def embed_sentence(sentence):\n",
        "    # Embed a sentence using word2vec; for example use cases only.\n",
        "    sentence = remove_punctuation(sentence)\n",
        "    words = sentence.split()\n",
        "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
        "    return jnp.expand_dims(word_vector_sequence, axis=0), words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AFUEFZGzCTv"
      },
      "source": [
        "#### Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF2V3KI-za9l"
      },
      "source": [
        "Self-attention is an attention mechanism where each vector of a given input sequence attends to the entire sequence. To gain an intuition for why self-attention is important, let us think about the following sentence (example taken from [source](https://jalammar.github.io/illustrated-transformer/)):\n",
        "\n",
        "`\"The animal didn't cross the street because it was too tired.\"`\n",
        "\n",
        "A simple question about this sentence is what the word \"it\" refers to? Even though it might look simple, it can be tough for an algorithm to learn this. This is where self-attention comes in, as it can learn an attention matrix for the word \"it\" where a large weight is assigned to the word \"animal\".\n",
        "\n",
        "Self-attention also allows the model to learn how to interpret words with the same embeddings, such as apple, which can be a company or food, depending on the context. This is very similar to the hidden state found within an RNN, but this process, as you will see, allows the model to attend over the entire sequence in parallel, allowing longer sequences to be utilised.\n",
        "\n",
        "Self-attention consists of three concepts:\n",
        "\n",
        "- Queries, keys and values\n",
        "- Scaled dot product attention\n",
        "- Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwOIMtdZzdTf"
      },
      "source": [
        "##### **Queries, keys and values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEf7QWIWzdo1"
      },
      "source": [
        "Typically all attention mechanisms can be written in terms of `key-value` pairs and `queries` to calculate the attention matrix and new context vector.\n",
        "\n",
        "To gain intuition, one can interpret the `query` vector as containing the information we are interested in, which is used to determine the `values` we should attend to, based on the similarity between the `keys` (which are paired with the `values`) and the `query`. Thus the similarity between the `queries` and `keys` gives us our attention score, where that score then determines the attention put in conjunction with the `values`. Or as [Lena Voita](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) puts it:\n",
        "\n",
        "- Query: asking for information\n",
        "- Key: saying that it has some information\n",
        "- Value: giving the information\n",
        "\n",
        "In transformer architectures, we use learnable weights matrices, represented as $W_Q,W_K,W_V$, to project each sequence vector to unique $q$, $k$, and $v$ vectors.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-96YjPxhcqW6FczUYwErGXHp6YpoLltq\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "You will notice that the vectors $q,k,v$ are smaller in size than the input vectors. This will be covered at a later stage, but just know that it is a design choice for transformers and not required at all to work.\n",
        "\n",
        "This process can also be parallelised, as the input sequence can be represented as a matrix $X$, which can be transformed into query, key, and value matrices $Q$, $K$, and $V$ respectively:\n",
        "\n",
        "$Q=W_QX \\\\ K=W_KX \\\\ V=W_VX$\n",
        "\n",
        "Below we show the code that creates three linear layers, which projects the input data to the $Q,K,V$ matrices, where the output size can be adjusted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc8zjK6eziIV"
      },
      "outputs": [],
      "source": [
        "class SequenceToQKV(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, X):\n",
        "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "\n",
        "    # this can also be one layer, how do you think you would do it?\n",
        "    q_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    k_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    v_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "\n",
        "    Q = q_layer(X)\n",
        "    K = k_layer(X)\n",
        "    V = v_layer(X)\n",
        "\n",
        "    return Q, K, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhGZHFsHz_Qp"
      },
      "source": [
        "##### **Scaled dot product attention**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxycHDUW0BVE"
      },
      "source": [
        "Now that we have our `query`, `key` and `value` matrices, it is time to calculate the attention matrix. Remember, in attention mechanisms; we must first find a score for each sequence vector and then use these scores to create a new context vector. We do this in self-attention using scaled dot product attention with the formula below.\n",
        "\n",
        "$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$\n",
        "\n",
        "What happens here is similar to what we did in the dot product attention in the previous section, just applying the mechanism to the sequence itself. For each element in the sequence, we calculate the attention weight matrix between $q_i$ and $K$. We then multiply $V$ by each weight and finally sum all weighted vectors $v_{weighted}$ together to form a new representation for $q_i$. By doing this, we are essentially drowning out irrelevant vectors and bringing up important vectors in the sequence when our focus is on $q_1$.\n",
        "\n",
        "$QK^\\top$ is scaled by the square root of the dimension of the vectors, $\\sqrt{d_k}$, to ensure more stable gradients during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_UYNzrS0Hga"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    d_k = key.shape[-1]\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "    value = jnp.matmul(attention_weights, value)\n",
        "    return value, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuNaEjIm0PhV"
      },
      "source": [
        "Let's now see scaled dot product attention in action. We will take a sentence, embed each word using word2vec, and see what the final self-attention weights look like.\n",
        "\n",
        "We will not use the linear projection layers as they are not trained. Instead, we are going to make $X=Q=V=K$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oy2sWzR0Ok5"
      },
      "outputs": [],
      "source": [
        "sentence = \"I drink coke, but eat steak\"\n",
        "word_embeddings, words = embed_sentence(sentence)\n",
        "Q = K = V = word_embeddings\n",
        "\n",
        "# calculate weights and plot\n",
        "values, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "words = remove_punctuation(sentence).split()\n",
        "plot_attention_weight_matrix(attention_weights[0], words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG1Kxljr0Vzw"
      },
      "source": [
        "Keep in mind that we have not trained our attention matrix yet. However, we can see that by utilising the word2vec vectors as our sequence, we can see how scaled dot product attention already is capable of attending to \"eat\" when \"steak\" is our query and that the query \"drink\" attends more to \"coke\" and \"eat\".\n",
        "\n",
        "More resources:\n",
        "\n",
        "[Attention with Q,K,V](https://www.youtube.com/watch?v=k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7B-AgO80gIt"
      },
      "source": [
        "##### **Masked attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdRoKsu70gGW"
      },
      "source": [
        "There are cases where applying self-attention over the entire sequence is not practical. These can include:\n",
        "\n",
        "- Uneven length sequences batched together.\n",
        "  - When sending a batch of sequences through a network, the self-attention expects each sequence to be the same length. One handles this by padding the sequence. When calculating attention, ideally, these padding tokens should not be taken into consideration.\n",
        "- Training a decoding model.\n",
        "  - When training decoder models, such as GPT-3, the decoder has access to the entire target sequence when training (as training is done in parallel). In order to prevent the method from cheating by looking at future tokens, we have to mask the future sequence data so that earlier data can not attend to it.\n",
        "\n",
        "By applying a mask to the final score calculated between queries and keys, we mitigate the influence of the unwanted sequence vectors. The vectors are masked by making the score between the query and their respective keys a VERY large negative value. This results in the softmax function pushing the attention weight very close to zero, and the resulting value will be summed out and not influence the final representation.\n",
        "\n",
        "\n",
        "Putting everything together, masked scaled dot product attention visually looks like this:\n",
        "\n",
        "<img src=\"https://windmissing.github.io/NLP-important-papers/AIAYN/assets/5.png\" alt=\"drawing\" width=\"200\"/>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Syx8_5E0eM9"
      },
      "outputs": [],
      "source": [
        "# example of building a mask for tokens of size 32\n",
        "mask = jnp.tril(jnp.ones((32, 32)))\n",
        "sns.heatmap(mask, cmap=\"Blues\")\n",
        "plt.title(\"Example of mask that can be applied\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwTJrQ20gDw"
      },
      "source": [
        "Lets now adapt our scaled dot product attention function to implement masked attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVHpyNs_0ePh"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    d_k = key.shape[-1]\n",
        "    T_k = key.shape[-2]\n",
        "    T_q = query.shape[-2]\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
        "\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "    attention = jnp.matmul(attention_weights, value)\n",
        "    return attention, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNHklaSV1Tej"
      },
      "source": [
        "#### Multihead Attention - <font color='blue'>Advanced</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1pk4C0F1Ta6"
      },
      "source": [
        "Rather than only computing the attention once, the multi-head attention (MHA) mechanism runs through the scaled dot-product attention multiple times in parallel. According to the paper, Attention is all you need, \"multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this\".\n",
        "\n",
        "Multi-head attention can be viewed as a similar strategy to stacking convolution kernels in a CNN layer. This allows the kernels to focus on and learn different features and rules, which is why multiple heads of attention also work. The process for MHA is given below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1q0Oq6IVEkkMfVSpY4LkHBP866mcoIFsh\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "As can be seen from the figure, the scaled dot product attention discussed earlier is just repeated $N$ times, with $3N$ learnable matrices for each head. The outputs from the different heads are then concatenated, whereafter it is fed through a linear projection, which produces the final representation.\n",
        "\n",
        "Due to these large amount of computations and memory requirements, a common design choice is to have the $W_{Qn}, W_{Kn}, W_{Vn}$ matrices produce embeddings of length $d_m/N$, where $d_m$ is the input sequence embedding size and $N$ is the number heads. By doing this, the MHA function is similar computation-wise to using a single head of attention.\n",
        "\n",
        "**Code Task:** Finish the implementation of MHA below. Hint, swapaxes and reshape is a good place to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jS0QesX0eR3"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "    self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "\n",
        "  def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "    if None in [Q, K, V]:\n",
        "      assert not X is None, \"X has to be provided if either Q,K,V not provided\"\n",
        "\n",
        "      # project all data to Q, K, V\n",
        "      Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "    # get the batch size, sequence length and embedding size\n",
        "    B, T, d_m = K.shape\n",
        "\n",
        "    # calculate heads embedding size (d_m/N)\n",
        "    head_size = d_m // self.num_heads\n",
        "\n",
        "    # B,T,d_m -> B, T, N, dm//N -> B, N, T, dm//N\n",
        "    q_heads = Q.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    k_heads = K.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    v_heads = V.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "    attention, attention_weights = scaled_dot_product_attention(\n",
        "        q_heads, k_heads, v_heads, mask\n",
        "    )\n",
        "\n",
        "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, d_m) - re-assemble all head outputs\n",
        "    attention = # FINISH ME\n",
        "\n",
        "    # apply Wo\n",
        "    X_new = self.Wo(attention)\n",
        "\n",
        "    if return_weights:\n",
        "      return X_new, attention_weights\n",
        "    else:\n",
        "      return X_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DQWnHB2jeEBD"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "\n",
        "mha = MultiHeadAttention(2, 8)\n",
        "# initialise model\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [1, 2, 8])\n",
        "params = mha.init(key, x)\n",
        "x_new = mha.apply(params, x)\n",
        "\n",
        "x_correct = jnp.array(\n",
        "    [\n",
        "        [\n",
        "            [\n",
        "              -0.59349924, -0.79245573,  0.64649045, -0.52850205,\n",
        "              -0.4793459 , -0.34167248, -0.45467672,  0.8619362\n",
        "            ],\n",
        "            [\n",
        "              -0.7895622 , -0.9945788 ,  0.7638061 , -0.65239996,\n",
        "              -0.56319916, -0.2351217 , -0.39363512,  0.9993293\n",
        "            ],\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "assert jnp.allclose(x_correct, x_new), \"Not returning the correct value\"\n",
        "print(\n",
        "    \"It seems correct. Look at the answer below to compare methods then move to the transformers section.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kFRtzAFLd1UA"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "    self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "\n",
        "  def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "    if None in [Q, K, V]:\n",
        "      assert not X is None, \"X has to be provided if either Q,K,V not provided\"\n",
        "\n",
        "      # project all data to Q, K, V\n",
        "      Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "    # get the batch size, sequence length and embedding size\n",
        "    B, T, d_m = K.shape\n",
        "\n",
        "    # calculate heads embedding size (d_m/N)\n",
        "    head_size = d_m // self.num_heads\n",
        "\n",
        "    # B,T,d_m -> B, T, N, dm//N -> B, N, T, dm//N\n",
        "    q_heads = Q.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    k_heads = K.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    v_heads = V.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "    attention, attention_weights = scaled_dot_product_attention(\n",
        "        q_heads, k_heads, v_heads, mask\n",
        "    )\n",
        "\n",
        "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, d_m) - re-assemble all head outputs\n",
        "    attention = attention.swapaxes(1, 2).reshape(B, -1, d_m)\n",
        "\n",
        "    # apply Wo\n",
        "    X_new = self.Wo(attention)\n",
        "\n",
        "    if return_weights:\n",
        "      return X_new, attention_weights\n",
        "    else:\n",
        "      return X_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxjK2bv-1iur"
      },
      "source": [
        "Until now, everything covered is not typically used on its own when constructing LLMs. However, they constitute the underlying mechanisms that enable these models to perform at such a high level. By comprehending these mechanisms, you can gain a better understanding of why LLMs may occasionally exhibit peculiar behavior and pinpoint potential starting points for debugging them.\n",
        "\n",
        "\n",
        "There has also been many optimisations to the MHA structure mentioned above, as the machine learning engineers find more and more ways to optimise this compute heavy step to scale the models. These include Multi-query attention (MQA) or grouped query attention (GQA).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **2. Building your own LLM**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA_2coZvhAg3"
      },
      "source": [
        "### 2.1 High-level overvierw <font color='orange'>Beginner</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BflycqAw_RF8"
      },
      "source": [
        "The Transformer Architecture was famously introduced in the paper entitled [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) by Vaswani et al.\n",
        "\n",
        "As the title of the paper suggests, such an architecture consists of basically only attention mechanisms along with feed-forward layers and linear layers, as shown in the diagram below.\n",
        "\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"350\" />\n",
        "\n",
        "Transformers and its variations are in the core of Large Language Models and it's not an exaggeration to say that almost all language models out there are Transformer based architectures.\n",
        "\n",
        "As you can see in the diagram the original Transformer architecture consists of two parts, one that receives inputs usually called encoder and another that receives outputs (i.e. targets) called decoder. This is because the transformer was designed for machine translation.\n",
        "\n",
        "The encoder will receive an input sentence in one language and process it through multiple stacked `encoder blocks`. This creates a final representation, which contains helpful information necessary for the decoding task. This output is then fed into stacked `decoder blocks` that produce new outputs in an autoregressive manner.\n",
        "\n",
        "The encoder consists of $N$ identical blocks, which process a sequence of token vectors sequentially. These blocks consist of 3 parts:\n",
        "\n",
        "1. A multi-head attention block. These are the transformer architecture's backbone. They process the data to generate representations for each token, ensuring that the necessary information for the task at hand is represented in the vectors. These are exactly the MHA we covered in the attention section previously.\n",
        "2. An MLP is applied to each input token separately and identically.\n",
        "3. Residual connection that adds the input tokens to the attended representations and a residual connection between the input to the MLP and its outputs. For both these connections, the result is normalized using layernorm. In certain implementations, these normalization steps are applied to the inputs rather than the outputs. Just like a Resnet, transformers are designed to be very deep models thus, these add and norm blocks are essential for a smooth gradient flow.  \n",
        "\n",
        "Similarly, the decoder block consists of $N$ identical blocks, however there is some variation within these block. Concretely, the different parts are:\n",
        "\n",
        "1. A masked multi-head attention block. This is an MHA block that performs _self-attention_ on the output sequence however this computation is restricted to the inputs that have already been seen. In other words, future tokens are blocked when making predictions.\n",
        "2. A multi-head attention block. This block receives the output of the final encoder block, the transformed tokens, and uses that as the key-value pairs, while using the output of the first MHA block as the query. In doing this, the model attends over the input required to perform the sequence task. This MHA block thus performs _cross-attention_ by looking at the encoder inputs.\n",
        "3. An MLP same as the encoder\n",
        "4. Residual connection same as the encoder.\n",
        "\n",
        "Given this original architecture, there have been several variation with others focusing on the encoder only and others the **decoder only**. Large language models(LLMs) such as GPT-2, GPT-3 and Turing-NLG were born out of decoder only architectures. These architecture look like:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
        "\n",
        "with the cross attention block missing as no encoder output is available. So to build a language model, we will focus on the decoder only architecture as seen above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbTsk0MdhAhC"
      },
      "source": [
        "### 2.2 Tokenization + Positional encoding <font color='orange'>Beginner</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DehUpfym_RF8"
      },
      "source": [
        "#### 2.2.1 Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBiFpVBu_RF9"
      },
      "source": [
        "\n",
        "Transformers cannot handle raw strings of text. So to process text, the text is first split up into tokens. The tokens are then indexed and each token is assigned an embedding of size $d_{model}$. These embeddings can be learned during training or can come from a pretrained vocabulary of embeddings. This new sequence of token embeddings is then fed into the transformer architecture. This idea is visualised below.\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16euh4LADP_mcXywFwKKY3QQQkVplepiI\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "\n",
        "These token IDs are typically predicted when a model generates text, fills in missing words, etc.\n",
        "\n",
        "This process of splitting up text into tokens and assigning an ID to each token is called [tokenisation](https://huggingface.co/docs/transformers/tokenizer_summary). There are various ways to tokenise text, with some methods being trained directly from the data. When using pre-trained transformers, it is crucial to use the same tokeniser that was used to train the model. The previous link has in-depth descriptions of many widely known techniques.\n",
        "\n",
        "Below we show how the [BERT](https://arxiv.org/abs/1810.04805) model's tokeniser tokenises a sentence. We use [Hugging Face](https://huggingface.co/) for this part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJBMvlUA_RF9"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_input = bert_tokenizer(\"The practical is so much fun\")\n",
        "print(f\"Token IDs: {encoded_input['input_ids']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYbtZTVP_RF9"
      },
      "source": [
        "Here we can see that the tokeniser returns the IDs for each token, as shown in the figure. But counting the number of IDs, we see that it is larger than the number of words in the sentence. Let's print the tokens associated with each ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPZjiLis_RF9"
      },
      "outputs": [],
      "source": [
        "print(f\"Tokens: {bert_tokenizer.decode(encoded_input['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3K8UFlR_RF9"
      },
      "source": [
        "We can see the tokeniser attaches new tokens, `[CLS]` and `[SEP]`, to the start and end of the sequence. This is a BERT-specific requirement for training and inference. Adding special tokens is a very common thing to do. Using special tokens, we can tell a model when a sentence starts or ends or when a new part of the input starts. This can be helpful when performing different tasks.\n",
        "\n",
        "For instance, to pretrain specific transformers, they perform what is known as masked prediction. For this, random tokens in a sequence are replaced by the `[MASK]` token, and the model is trained to predict the correct token ID for the token replaced with that token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djMP4Ijz_RF9"
      },
      "source": [
        "**Drawback of using raw token**:\n",
        "\n",
        "One drawback of using raw tokens is that they lack any indication of the word's position in the sequence. This is evident when considering sentences like \"I am happy\" and \"Am I happy\" - these two phrases have distinct meanings, and the model needs to grasp the word order to understand the intended message accurately.\n",
        "\n",
        "To address this, when converting the inputs into vectors, position vectors are introduced and added to these vectors to indicate the **position** of each word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "639s7Zuk_RF9"
      },
      "source": [
        "#### 2.2.2 Positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-hBFVYo_RF9"
      },
      "source": [
        "In most domains where a transformer can be utilised, there is an underlying order to the tokens produced, be it the order of words in a sentence, the location from which patches are taken in an image or even the steps taken in an RL environment. This order is very important in all cases; just imagine you interpret the sentence \"I have to read this book.\" as \"I have this book to read.\". Both sentences contain the exact same words, yet they have completely different meanings based on the order.\n",
        "\n",
        "As both the encoder and the decoder blocks process all tokens in parallel, the order of tokens is lost in these calculations. To cope with this, the sequence order has to be injected into the tokens directly. This can be done by adding *positional encodings* to the tokens at the start of the encoder and decoder blocks (though some of the latest techniques add positional information in the attention blocks). An example of how positional encodings alter the tokens is shown below.\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eSgnVN2hnEsrjdHygDGwk1kxEi8-dcFo\" alt=\"drawing\" width=\"650\"/>\n",
        "\n",
        "Ideally, these encodings should have these characteristics ([source](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)):\n",
        "* Each time-step should have a unique value\n",
        "* The distance between time steps should stay constant.\n",
        "* The encoding should be able to generalise to longer sequences than seen during training.\n",
        "* The encoding must be deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rklY-aL-_RF9"
      },
      "source": [
        "##### **Sine and cosine functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLcfkMku_RF9"
      },
      "source": [
        "\n",
        "In Attention is All you Need, the authors used a method that can satisfy all these requirements. This involves summing a combination of sine and cosine waves at different frequencies, with the formula for a position encoding at position $D$ shown below, where $i$ is the embedding index and $d_m$ is the token embedding size.\n",
        "\n",
        "\\\\\n",
        "\n",
        "$P_{D}= \\begin{cases}\\sin \\left(\\frac{D}{10000^{i/d_{m}}}\\right), & \\text { if } i \\bmod 2=0 \\\\ \\cos \\left(\\frac{D}{10000^{((i-1)/d_{m}}}\\right), & \\text { otherwise } \\end{cases}$\n",
        "\n",
        "\\\n",
        "\n",
        "Assuming our model as $d_m=8$, the position embedding will look like this:\n",
        "\n",
        "\\\n",
        "$P_{D}=\\left[\\begin{array}{c}\\sin \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{8/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{8/8}}\\right)\\end{array}\\right]$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Let's first create a function that can return these encodings to understand why this will work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5t5D30_RF9"
      },
      "outputs": [],
      "source": [
        "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
        "\n",
        "  assert token_embedding % 2 == 0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
        "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "  i = jnp.arange(0, token_embedding, 2)\n",
        "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
        "  frequencies = positions * frequency_steps\n",
        "\n",
        "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "\n",
        "  return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYW-VDOL_RF-"
      },
      "outputs": [],
      "source": [
        "token_sequence_length = 50  # Number of tokens the model will need to process\n",
        "token_embedding = 10000  # token embedding (and positional encoding) dimensions, ensure it is divisible by two\n",
        "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
        "plot_position_encodings(P, token_sequence_length, token_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mjHEDPO_RF-"
      },
      "source": [
        "Looking at the graph above, we can see that for each position index, there is a unique pattern forming, where each position index will always have the same encoding.\n",
        "\n",
        "**Group task**:\n",
        "\n",
        "- Discuss with your friend why we are seeing that specific pattern when `token_sequence_length` is 1000, and `token_embedding` is 768.\n",
        "- You can try playing around with smaller values for `token_sequence_length` and  `token_embedding` to get a better intuition for the above discussion.\n",
        "- Ask your friend why they think the 10000 constant is used in the functions above.\n",
        "- Make `token_sequence_length` to be 50 and `token_embedding` something large, like 10000. What do you notice? Is a large token embedding always needed?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdNPg0pnhAhG"
      },
      "source": [
        "### 2.3 Transformer block   <font color='green'>Intermediate</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4vSolF2_RF-"
      },
      "source": [
        "Just as a MLP or CNN is network is a stack of layers, transformers are also composed of a stack of transformer blocks. In this section we build out each one of these blocks that are required to form a transformer block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTURbfr__RF-"
      },
      "source": [
        "\n",
        "#### 2.3.1 Feed Forward Network (FFN) / Multilayer perceptron (MLP) <font color='orange'>Beginner</font>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H1pVFxJiSpM_Ozj1eKWNdcFQ5Hn5XsZz\" alt=\"drawing\" width=\"260\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTtFi9AZ_RF-"
      },
      "source": [
        "These blocks are just a single 2-layer MLP that uses ReLU activation in the original model. GeLU has also become very popular, and we will be using it throughout the practical. The formula below represents the feedforward neural network (FFN) with ReLU activation, where input `x` is transformed through two linear layers with weights `W1` and `W2`, followed by bias terms `b1` and `b2`, and the `max` function represents the ReLU activation function.\n",
        "\n",
        "$$\n",
        "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
        "$$\n",
        "\n",
        "One can interpret this block as processing what the MHA block has produced and then projecting these new token representations to a space that the next block can use more optimally. Usually, the first layer is very wide, in the range of 2-8 times the size of the token representations. They do this as it is easier to parallelize computations for a single wider layer during training than to parallelize a feedforward block with multiple layers. Thus they can add in more complexity but keep training and inference optimized.\n",
        "\n",
        "**Code task:** Code up a Flax Module that implements the feed forward block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsho1CnW_RF-"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  A 2-layer MLP which widens then narrows the input.\n",
        "\n",
        "  Args:\n",
        "    widening_factor [optional, default=4]: The size of the hidden layer will be d_model * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  widening_factor: int = 4\n",
        "  init_scale: float = 0.25\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      x: [B, T, d_m]\n",
        "    '''\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self.widening_factor * d_m\n",
        "\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "    )\n",
        "    layer1 = # FINISH ME\n",
        "    layer2 = # FINISH ME\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-qj0nfhH_RF-"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "  \"\"\"A 2-layer MLP which widens then narrows the input.\"\"\"\n",
        "  widening_factor: int = 4\n",
        "  init_scale: float = 0.25\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self.widening_factor * d_m\n",
        "\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "    )\n",
        "    layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
        "    layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sts5Vr4i_RF-"
      },
      "source": [
        "#### 2.3.2 Add and Norm block <font color='orange'>Beginner</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWUpf8wt_RF-"
      },
      "source": [
        "In order to get transformers to go deeper, the residual connections are very important to allow an easier flow of gradients through the network. For normalisation, `layer norm` is used. This normalises each token vector independently in the batch. It is found that normalising the vectors improves the convergence and stability of transformers.\n",
        "\n",
        "There are two learnable parameters in layernorm, `scale` and `bias`, which rescales the normalised value. Thus, for each input token in a batch, we calculate the mean, $\\mu_{i}$ and variance $\\sigma_i^2$. We then normalise the token with:\n",
        "\n",
        "$\\hat{x}_i = \\frac{x_i-\\mu_{i}}{\\sigma_i^2 + ϵ}$.\n",
        "\n",
        "Then $\\hat{x}$ is rescaled using the learned `scale`, $γ$, and `bias` $β$, with:\n",
        "\n",
        "$y_i = γ\\hat{x}_i + β = LN_{γ,β}(x_i)$.\n",
        "\n",
        "So our add norm block can be represented as $LN(x+f(x))$, where $f(x)$ is either a MLP or MHA block.\n",
        "\n",
        "**Code task:** Code up a Flax Module that implements the add norm block. It should take as input the processed and unprocessed tokens. Hint: `hk.LayerNorm `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5bLb5Ly_RF_"
      },
      "outputs": [],
      "source": [
        "class AddNorm(nn.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, processed_x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: Sequence of tokens before feeding into MHA or FF blocks, with shape [B, T, d_m]\n",
        "      x: Sequence of after being processed by MHA or FF blocks, with shape [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      add_norm_x: Transformed tokens with shape [B, T, d_m]\n",
        "    '''\n",
        "\n",
        "    added = # FINISH ME\n",
        "    normalised = #FINISH ME\n",
        "    return normalised(added)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HXSi7BXZ_RF_"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "class AddNorm(nn.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, processed_x):\n",
        "\n",
        "    added = x + processed_x\n",
        "    normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
        "    return normalised(added)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91dXd29b_RF_"
      },
      "source": [
        "### 2.4 Building the Transformer Decoder / LLM <font color='green'>Intermediate</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl0UAyvM_RF_"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
        "\n",
        "Most of the groundwork has happened. We have built the positional encoding block, the MHA block, the feed-forward block and the add&norm block.\n",
        "\n",
        "The only part needed is passing inputs to each decoder block and applying the masked MHA block found in the decoder blocks.\n",
        "\n",
        "**Code task:** Code up a FLAX Module that implements the (FFN(norm(MHA(norm(X))))) for the decoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVmSFKZK_RF_"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer decoder block.\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    d_m: Token embedding size\n",
        "    widening factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      encoder_output: Batch of tokens with was processed by the encoder, with shape [B, T_encoder, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    attention, attention_weights_1 = # FINISH ME\n",
        "\n",
        "    X = # FINISH ME\n",
        "\n",
        "    projection = # FINISH ME\n",
        "    X = # FINISH ME\n",
        "\n",
        "    return (X, attention_weights_1) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "stNZVVv3_RF_"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer decoder block.\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    d_m: Token embedding size\n",
        "    widening factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
        "\n",
        "    X = self.add_norm1(X, attention)\n",
        "\n",
        "    projection = self.MLP(X)\n",
        "    X = self.add_norm2(X, projection)\n",
        "\n",
        "    return (X, attention_weights_1) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXXVWd7_RF_"
      },
      "source": [
        "Next, we just put everything together, adding in the positional encodings as well as stacking multiple transformer blocks and adding our prediction layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XBG24Qs_RF_"
      },
      "outputs": [],
      "source": [
        "class LLM(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer encoder consisting of several layers of decoder blocks.\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    num_layers: The number of decoder blocks to be used.\n",
        "    d_m: Token embedding size\n",
        "    vocab_size: The size of the vocabulary\n",
        "    widening_factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "  num_heads: int\n",
        "  num_layers: int\n",
        "  d_m: int\n",
        "  vocab_size: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.blocks = [\n",
        "        DecoderBlock(self.num_heads, self.d_m, self.widening_factor)\n",
        "        for _ in range(self.num_layers)\n",
        "    ]\n",
        "    self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m) # convert tokens to embedding\n",
        "    self.pred_layer = nn.Dense(self.vocab_size)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weights=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    # convert a token id to a d_m dimensional vector\n",
        "    X = self.embedding(X)\n",
        "    sequence_len = X.shape[-2]\n",
        "    positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "    X = X + positions\n",
        "\n",
        "    if return_att_weights:\n",
        "        att_weights = []\n",
        "    block_n = 0\n",
        "    for block in self.blocks:\n",
        "        out = block(X, mask, return_att_weights)\n",
        "        if return_att_weights:\n",
        "            X = out[0]\n",
        "            att_weights.append(out[1])\n",
        "        else:\n",
        "            X = out\n",
        "\n",
        "    # apply a linear layer and softmax to calculate our logits over tokens\n",
        "    logits = nn.log_softmax(self.pred_layer(X))\n",
        "\n",
        "    return (\n",
        "        logits if not return_att_weights else (logits, jnp.array(att_weights).swapaxes(0, 1))\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sClFLLkU_RF_"
      },
      "source": [
        "If everything is correct, then if we run the code below, everything should run without any issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82CWEa5m_RGA"
      },
      "outputs": [],
      "source": [
        "B, T, d_m, N, vocab_size = 18, 32, 16, 8, 25670\n",
        "\n",
        "llm = LLM(num_heads=1, num_layers=1, d_m=d_m, vocab_size=vocab_size, widening_factor=4)\n",
        "mask = jnp.tril(np.ones((T, T)))\n",
        "\n",
        "# initialise module and get dummy output\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.randint(key, [B, T], 0, vocab_size)\n",
        "params = llm.init(key, X, mask=mask)\n",
        "\n",
        "# extract output from decoder\n",
        "logits, decoder_att_weights = llm.apply(\n",
        "    params,\n",
        "    X,\n",
        "    mask=mask,\n",
        "    return_att_weights=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gve7ssD__RGA"
      },
      "source": [
        "As a final sanity check, we can see that our attention weights behave as expected for now. The encoder weights can attend to all input sequences, and our decoder only attends to previous tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4NpywYv_RGA"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "plt.suptitle(\"LLM attention weights\")\n",
        "sns.heatmap(decoder_att_weights[0, 0, 0, ...], ax=ax, cmap=\"Blues\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmt3tp38G90A"
      },
      "source": [
        "### 2.5 Training your LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agLIpsoh_RGA"
      },
      "source": [
        "#### 2.5.1 Training objective <font color='green'>Intermediate</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOSv1-3B_RGA"
      },
      "source": [
        "A sentence is nothing but a string of words. A LLM aims to predict the next word by considering the current context, namely the words that have come before.\n",
        "\n",
        "Here's the basic idea:\n",
        "\n",
        "To calculate the probability of a full sentence \"word1, word2, ..., last word\" appearing in a given context $c$, the procedure is to break down the sentence into individual words and consider the probability of each word given the words that precede it. These individual probabilities are then multiplied together:\n",
        "\n",
        "$$\\text{Probability of sentence} = \\text{Probability of word1} \\times \\text{Probability of word2} \\times \\ldots \\times \\text{Probability of last word}$$\n",
        "\n",
        "This method is akin to building up a narrative one piece at a time based on the preceding storyline.\n",
        "\n",
        "Mathematically, this is expressed as the likelihood (probability) of a sequence of words $y_1, y_2, ..., y_n$ in a given context $c$, which is achieved by multiplying the probabilities of each word $y_t$ calculated given the predecessors ($y_{<t}$) and the context $c$:\n",
        "\n",
        "$$\n",
        "P\\left(y_{1}, y_{2}, \\ldots, y_{n}, \\mid c\\right)=\\prod_{t=1}^{n} P\\left(y_{t} \\mid y_{<t}, c\\right)\n",
        "$$\n",
        "\n",
        "Here $y_{<t}$ stands for the sequence $y_1, y_2, ..., y_{t-1}$, while $c$ represents the context.\n",
        "\n",
        "This is analogous to solving a jigsaw puzzle where the next piece is predictively placed based on what's already in place.\n",
        "\n",
        "Remember just when training a transformer, we do not work in words, but in tokens. During the training process, the model's parameters are fine-tuned by computing the cross-entropy loss across the predicted token, and the correct token, and then performing backpropagation. The loss for time step \"t\" is computed as:\n",
        "\n",
        "$$ \\text{Loss}_t = - \\sum_{w \\in V} y_t\\log (\\hat{y}_t) $$\n",
        "\n",
        "Here $y_t$ is the actual token at time step $t$, and $\\hat{y}_t$ is the token predicted by the model at the same time step. The loss for the entire sentence is then computed as:\n",
        "\n",
        "$$ \\text{Sentence Loss} = \\frac{1}{n} \\sum^{n}_{t=1} \\text{Loss}_t $$\n",
        "\n",
        "where $n$ is the length of the sequence.\n",
        "\n",
        "This iterative process ultimately hones the model's predictive capabilities over time.\n",
        "\n",
        "**Code task**: Implement the cross-entropy loss function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXmjUYdDHseM"
      },
      "outputs": [],
      "source": [
        "def sequence_loss_fn(logits, targets):\n",
        "  '''\n",
        "  Compute the cross-entropy loss between predict token ID and true ID\n",
        "\n",
        "  Args:\n",
        "    logits: A array of shape [batch_size, sequence_length, vocab_size]\n",
        "    targets: The targets we are trying to predict\n",
        "\n",
        "  Returns:\n",
        "    loss: A scalar value representing the mean batch loss\n",
        "  '''\n",
        "\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  assert logits.shape == target_labels.shape\n",
        "\n",
        "  mask = jnp.greater(targets, 0)\n",
        "  loss = #FINISH ME\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4Cq5_4WN_RGA"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "VOCAB_SIZE = 25670\n",
        "targets = jnp.array([[0, 2, 0]])\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [1, 3, VOCAB_SIZE])\n",
        "loss = sequence_loss_fn(X, targets)\n",
        "real_loss = jnp.array(10.966118)\n",
        "assert jnp.allclose(real_loss, loss), \"Not returning the correct value\"\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cthfcbmC_RGA"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def sequence_loss_fn(logits, targets):\n",
        "  \"\"\"Compute the loss on data wrt params.\"\"\"\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  assert logits.shape == target_labels.shape\n",
        "  mask = jnp.greater(targets, 0)\n",
        "  loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "  loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CSfvGj__RGA"
      },
      "source": [
        "#### 2.5.2 Training models <font color='blue'>Advanced</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIQ_aJGW_RGA"
      },
      "source": [
        "In the next section, we define all the processes required to train the model using the objective described above. A lot of this is now the work required to do training using FLAX.\n",
        "\n",
        "Below we gather the dataset and we shall be training on, which is Karpathy's shakespeare dataset. Its not so important to understand this code, so either just run the cell to load the data, or view the code if you want to understand it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "guMHAaSo_RGB"
      },
      "outputs": [],
      "source": [
        "# @title Create Shakespeare dataset and iterator (optional, but run the cell)\n",
        "\n",
        "# Trick to avoid errors when downloading tinyshakespeare.\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "\n",
        "class WordBasedAsciiDatasetForLLM:\n",
        "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Tokenize by splitting the text into words\n",
        "        words = corpus.split()\n",
        "        self.vocab_size = len(set(words))  # Number of unique words\n",
        "\n",
        "        # Create a mapping from words to unique IDs\n",
        "        self.word_to_id = {word: i for i, word in enumerate(set(words))}\n",
        "\n",
        "        # Store the inverse mapping from IDs to words\n",
        "        self.id_to_word = {i: word for word, i in self.word_to_id.items()}\n",
        "\n",
        "        # Convert the words in the corpus to their corresponding IDs\n",
        "        corpus = np.array([self.word_to_id[word] for word in words]).astype(np.int32)\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Only {num_batches} batches; consider a shorter \"\n",
        "                \"sequence or a smaller batch.\"\n",
        "            )\n",
        "\n",
        "        self._ds = WordBasedAsciiDatasetForLLM._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Yield next mini-batch.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Create the language modeling observation/target pairs.\n",
        "        return dict(\n",
        "            input=batch[:, :-1], target=batch[:, 1:]\n",
        "        )\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        \"\"\"Convert a sequence of word IDs to words.\"\"\"\n",
        "        return [self.id_to_word[id] for id in ids]\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WBIFg51oQl0"
      },
      "source": [
        "Lets now look how our data is structured for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvH3XPM5_RGB"
      },
      "outputs": [],
      "source": [
        "# sample and look at the data\n",
        "batch_size = 2\n",
        "seq_length = 32\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
        "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(obs)))\n",
        "    print(\"ASCII:\", obs)\n",
        "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(target)))\n",
        "    print(\"ASCII:\", target)\n",
        "\n",
        "print(f\"\\n Total vocabulary size: {train_dataset.vocab_size}\")\n",
        "\n",
        "VOCAB_SIZE = train_dataset.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9vzee53_RGB"
      },
      "source": [
        "Next, let us train our LLM and see how it performs in producing Shakespearian text. First, we will define what happens for every training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGuYBCkekgDw"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(3, 4))\n",
        "def train_step(params, optimizer_state, batch, apply_fn, update_fn):\n",
        "  def loss_fn(params):\n",
        "    T = batch['input'].shape[1]\n",
        "    logits = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))))\n",
        "    loss = sequence_loss_fn(logits, batch['target'])\n",
        "    return loss\n",
        "\n",
        "  loss, gradients = jax.value_and_grad(loss_fn)(params)\n",
        "  updates, optimizer_state = update_fn(gradients, optimizer_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, optimizer_state, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtKWzKIAkfYU"
      },
      "source": [
        "Next we initialise our optimizer and model. Feel free to play with the hyperparameters during the practical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o3q-BZX_RGB"
      },
      "outputs": [],
      "source": [
        "# all hyperparameters\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "num_layers = 1\n",
        "widening_factor = 2\n",
        "LR = 2e-3\n",
        "batch_size = 32\n",
        "seq_length = 64\n",
        "\n",
        "# set up the data\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "batch = next(train_dataset)\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "# initialise model\n",
        "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
        "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "params = llm.init(key, batch['input'], mask)\n",
        "\n",
        "# set up the optimiser\n",
        "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
        "optimizer_state = optimizer.init(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bPEFakxmvsM"
      },
      "source": [
        "Now we train! This will take a few minutes.. While it trains, have you greeted your neighbour yet?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUAS6tie_RGB"
      },
      "outputs": [],
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 3500\n",
        "LOG_EVERY = 32\n",
        "losses = []\n",
        "VOCAB_SIZE = 25670\n",
        "\n",
        "# Training loop\n",
        "for step in range(MAX_STEPS):\n",
        "    batch = next(train_dataset)\n",
        "    params, optimizer_state, loss = train_step(\n",
        "        params, optimizer_state, batch, llm.apply, optimizer.update)\n",
        "    losses.append(loss)\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_ = jnp.array(losses).mean()\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"loss\": loss_,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()\n",
        "        losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGv9c2AFmF4V"
      },
      "source": [
        "#### 2.5.3 Inspecting the trained LLM <font color='orange'>Beginner</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfq61gim_RGB"
      },
      "source": [
        "**Reminder:** remember to run all code presented so far in this section before runnning the cells below!\n",
        "\n",
        "Lets generate some text now and see how our model did. DO NOT STOP THE CELL ONCE IT IS RUNNING, THIS WILL CHRASH THE SESSION."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lt8HTS__RGC"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(2, ))\n",
        "def generate_prediction(params, input, apply_fn):\n",
        "  logits = apply_fn(params, input)\n",
        "  argmax_out = jnp.argmax(logits, axis=-1)\n",
        "  return argmax_out[0][-1].astype(int)\n",
        "\n",
        "def generate_random_shakespeare(llm, params, id_2_word, word_2_id):\n",
        "    '''\n",
        "    Get the model output\n",
        "    '''\n",
        "\n",
        "    prompt = \"Love\"\n",
        "    print(prompt, end=\"\")\n",
        "    tokens = prompt.split()\n",
        "\n",
        "    # predict and append\n",
        "    for i in range(15):\n",
        "      input = jnp.array([[word_2_id[t] for t in tokens]]).astype(int)\n",
        "      prediction = generate_prediction(params, input, llm.apply)\n",
        "      prediction = id_2_word[int(prediction)]\n",
        "      tokens.append(prediction)\n",
        "      print(\" \"+prediction, end=\"\")\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "id_2_word = train_dataset.id_to_word\n",
        "word_2_id = train_dataset.word_to_id\n",
        "\n",
        "generated_shakespeare = generate_random_shakespeare(llm, params, id_2_word, word_2_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwNuMRf_RGC"
      },
      "source": [
        "Finally, we implemented everything above by taking the token ID with the maximum probability of being correct. This is greedy decoding, as we only took the most likely token. It worked well in this use case, but there are cases where we will see a degrading performance when taking this greedy approach, specifically when we are interested in generating realistic text.\n",
        "\n",
        "Other methods exist for sampling from the decoder, with a famous algorithm being beam search. We provide resources below for anyone interested in learning more about this.\n",
        "\n",
        "[Greedy Decoding](https://www.youtube.com/watch?v=DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=4)\n",
        "\n",
        "[Beam Search](https://www.youtube.com/watch?v=uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4hKnTFbHtdM"
      },
      "source": [
        "## 3. **Efficiently Finetuning LLMs with Hugging Face**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoqVNhCi_RGC"
      },
      "source": [
        "The availability of open source pretrained language models (LLMs), such as [LLAMA](https://github.com/facebookresearch/llama) and [FALCON](https://falconllm.tii.ae/) has been a game-changer in the field of natural language processing. These models, often comprising orders of billions of parameters, offer unprecedented language understanding capabilities. However, as their sizes have grown significantly, fine-tuning them for specific tasks has become more challenging than before.\n",
        "\n",
        "\n",
        "For our exploration in this tutorial, we will predominantly utilize open source code from Hugging Face, primarily drawing from the transformers, datasets, and PEFT libraries. The [transformers](https://github.com/huggingface/transformers) library grants us access to pretrained LLMs, the [datasets](https://github.com/huggingface/datasets) library provides convenient access to various datasets for training, and the [PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft) library encompasses implementations of the training and adaptation methods we'll be discussing below.\n",
        "\n",
        "In this section, we will explore the intricacies of custom adaptation techniques, understanding how to effectively fine-tune these large LLMs and make the most of their extraordinary potential in our research and applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoTvhvap_RGC"
      },
      "source": [
        "### 3.1 Adapter and Fine-Tuning methods  <font color='green'>Intermediate</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8ghCr8J_RGC"
      },
      "source": [
        "The world of open source LLMs brings forth an exciting range of possibilities, but their sheer size often poses a challenge for fine-tuning using consumer-grade hardware. Consequently, conventional adaptation methods fall short in such scenarios. To address this, innovative techniques have emerged to overcome these limitations.\n",
        "\n",
        "A significant proportion of these techniques involve either keeping the model parameters fixed, as seen in prompt engineering, where the input text acts as an agent to adapt the LLM's behavior, or altering only a tiny subset of model parameters. In this tutorial, our focus will be on the latter approach, presenting methods that modify a small portion of the model parameters, or bring additional parameters to a LLM.\n",
        "\n",
        "However, we encourage readers to explore [OpenAI's cookbook](https://github.com/openai/openai-cookbook), which hosts transferable recipes and links for prompting LLMs (as well as other models) for further insights and possibilities.\n",
        "\n",
        "A lot of content here is inspired by [Lightning AI blogs](https://lightning.ai/pages/community/article/understanding-llama-adapters/).\n",
        "\n",
        "**Discussion**: Before getting started into methods established in the literature let's first think why finetuning the whole model is so costly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znctvjrE_RGC"
      },
      "source": [
        "#### 3.1.1 Prefix tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8JWUHWf_RGC"
      },
      "source": [
        "Prefix tuning works by introducing a trainable token/tensor into each transformer block along with the input tokens, as opposed to solely modifying the input tokens (prompt engineering) or finetuning the entire transformer bloc. The contrast between a standard transformer block and a transformer block enhanced with a prefix is depicted in the following figure. This was first introduce in the [\"Prefix-Tuning: Optimizing Continuous Prompts for Generation\" paper](https://arxiv.org/abs/2101.00190) by Xiang Lisa Li and Percy Liang.\n",
        "\n",
        "By only training the \"Trainable tokens\" and the new introduced MLP layer, we are able to adapt a model to our domain by training close to 0.1% of the parameters of a full model and achieve performance comparable to fine tuning the entire model.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fSnk9MkoPN6KbmbP71iU9EViU9avvOHb\" alt=\"drawing\" width=\"230\"/>\n",
        "\n",
        "Below we show pseudo code for this method, as well as a normal block to showcase the differences. Note running the code will *not* work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgpupW9w_RGD"
      },
      "outputs": [],
      "source": [
        "def normal_transformer_block(tokens):\n",
        "  \"\"\"\n",
        "  Example of pseudo code for a normal transformer.\n",
        "  \"\"\"\n",
        "  original_tokens = tokens\n",
        "  x = MHA(tokens)\n",
        "  x = LayerNorm(x + original_tokens)\n",
        "  original_tokens = x\n",
        "  x = FF(x)\n",
        "  transformed_tokens = LayerNorm(x + original_tokens)\n",
        "  return transformed_tokens\n",
        "\n",
        "def transformer_block_with_prefix(tokens, trainable_tokens):\n",
        "  \"\"\"\n",
        "  Example of pseudo code of transformer block with prefix tuning.\n",
        "  \"\"\"\n",
        "  prefix = FF(trainable_tokens)  # Trainable FF and tokens.\n",
        "  tokens = concat([prefix, tokens])\n",
        "  original_tokens = tokens\n",
        "  x = MHA(tokens)\n",
        "  x = LayerNorm(x + original_tokens)\n",
        "  original_tokens = x\n",
        "  x = FF(x)\n",
        "  transformed_tokens = LayerNorm(x+original_tokens)\n",
        "  return transformed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4sLxSol_RGD"
      },
      "source": [
        "#### 3.2.1 Adapter Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiSHLMJV_RGD"
      },
      "source": [
        "Very similar, and introduced in the [\"Parameter-Efficient Transfer Learning for NLP\" paper](https://arxiv.org/abs/1902) by Houlsby etc, it consists of adding a new block of weights between the transformer blocks called \"Adapter\".\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1t521Q3_yAuUDsoakJmv7cgQyF5-VvjgX\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "During adapter tuning, the green layers are trained on the downstream data, this includes the adapter, the layer normalization parameters, and the final classification layer (not shown in the figure).\n",
        "\n",
        " It has been shown to achieve similar performance to updating an entire network while only training 3.6% of the total model parameters.\n",
        "\n",
        "Below again is pseudo code highlighting where and how this work. Note running the code will not work.\n",
        "\n",
        "**Code exercise**: In similar style implement a pseudo-code implementation of the `Adapter` block showed in the diagram above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA48peig_RGD"
      },
      "outputs": [],
      "source": [
        "def transformer_block_with_adapters(tokens):\n",
        "  \"\"\"\n",
        "  Example of psuedo code of transformer block with adapter layers.\n",
        "  \"\"\"\n",
        "\n",
        "  # finish me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n0ScoMIW_RGD"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "def transformer_block_with_adapters(tokens):\n",
        "  \"\"\"\n",
        "  Example of psuedo code of transformer block with adapter layers.\n",
        "  \"\"\"\n",
        "\n",
        "  original_tokens = tokens\n",
        "  adapted_tokens = AdapterLayer(tokens) # trainable\n",
        "  x = MHA(adapted_tokens)\n",
        "  x = LayerNorm(x + original_tokens)\n",
        "  original_tokens = x\n",
        "  x = AdapeterLayer(x) # trainable\n",
        "  x = FF(x)\n",
        "  transformed_tokens = LayerNorm(x+original_tokens)\n",
        "  return transformed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5tEZibv_RGD"
      },
      "source": [
        "To see how both worlds from adapters and prefix tuning are together refer to the [LLAMA-Adapter paper](https://arxiv.org/abs/2303.16199)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBc08xY_RGD"
      },
      "source": [
        "### 3.2 LoRA <font color='orange'>Beginner</font>, <font color='green'>Intermediate</font>, <font color='blue'>Advanced</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqz83Ge9ldeq"
      },
      "source": [
        "> This section is a summarized copy of the [LoRA paper]((https://arxiv.org/abs/2106.09685)), read the paper for more details! We also recommend beginner focus more on the finetuning and interacting with your trained model, than on the details presented here.\n",
        "\n",
        "Finally let's talk about one of the most widely used methods for Efficient FineTuning called LoRA introduced in the paper [\"LoRA: Low-Rank Adaptation of Large Language Models\"](https://arxiv.org/abs/2106.09685) from Edward J. Hu et al.\n",
        "\n",
        "The idea behind Parameter Efficient Tunning (PEFT) is how to **efficiently** (memory wise and speedwise ) finetune these large models while significantly improving quality of the outputs produced, or to make the model behave in a different manner.\n",
        "\n",
        "Also it's important not only to be efficient during the finetune process but also during **inference** (e.g. for text generation models we're usually most interested in sampling time).\n",
        "\n",
        "LoRA is a method that's not only efficient during finetung but also **can be applied during inference without any additional cost when compared to the original model!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTEdgeYnlTSG"
      },
      "source": [
        "**Task**: To build intuition for what is to come, prove that $y1$ equal, or not equal, to $y2$. Note, $X$ and $W$ are matrices.\n",
        "\n",
        "> $W = W_1 + W_2$\n",
        ">\n",
        "> $y_1 = WX$\n",
        ">\n",
        "> $y_2 = W_1X + W_2X$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UvM3jWr_lWkt"
      },
      "outputs": [],
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!')\n",
        "%%latex\n",
        "\\begin{aligned}\n",
        "y_1 &= WX \\\\\n",
        "y_1 &= (W_1+W_2)X \\\\\n",
        "y_1 &= W_1X+W_2X \\\\\n",
        "y_1 &= y_2\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4v9QucLlkG4"
      },
      "source": [
        "**LoRA details**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIYXt7awxCeX"
      },
      "source": [
        "In the LoRA paper, the authors implement an innovative strategy when using the GPT-3 model, which has 175 billion parameters. Instead of fine-tuning all those parameters, they opt to freeze the pre-trained weights.\n",
        "\n",
        "They rather decompose the delta weight matrix ($∇W$), which represents the changes applied to the original model weights ($W_0$) ,  into two smaller matrices, referred to as A and B, such that $BA=∇W$.\n",
        "\n",
        "The shapes of A and B are chosen as $(d,r)$ and $(r,d)$ respectively, where $r$ is a lower rank projection used for input projection, and $r$ plays a crucial role in determining the degree of reduction in number of parameters.\n",
        "\n",
        "This translates into a significant reduction in computational requirements, as illustrated in the figure blow when comparing the area of blue and orange areas in the paper.\n",
        "\n",
        "Thus the output of a finetuned layer, $y$,  is reformulated as, where $x$ is the input data:\n",
        "\n",
        "$y = W_0x + ∇Wx = W_0x + BAx$\n",
        "\n",
        "During the fine-tuning process, we then only adapt the weights of $A$ and $B$, which are substantially smaller compared to the initial model weights $W_0$. $A$ and $B$ are called: *trainable rank decomposition matrices* and can be added in between each/any layer of the Transformer architecture.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1046/format:webp/1*F7uWJePoMc6Qc1O2WxmQqQ.png\">\n",
        "\n",
        "Translated into simpler terms, the authors essentially create an \"encoder\" matrix $A$ that converts the original input data into a highly condensed hidden vector. They then use a \"decoder\" matrix $B$ to reconstruct this hidden vector back to its original dimensionality.  The final model output is then generated by adding together the original model output and the output created through this projection process. This way, the initially enormous model can be efficiently fine-tuned for specific tasks, saving computational resources and time, but only training these layer specific encoder-decoders.\n",
        "\n",
        "\n",
        "**Question**: How many trainable parameters are introduced by these matrices?\n",
        "\n",
        "**Question:** How could LoRA not add any extra costs during inference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pF6nC67_RGD"
      },
      "source": [
        "**Why this works?**\n",
        "\n",
        "LoRa takes inspiration from [Li et al. (2018)](https://arxiv.org/abs/1804.08838) and [Aghajanyan et al. (2020)](https://arxiv.org/abs/2012.13255), both of which demonstrate that learned over-parametrized models actually reside on a low intrinsic dimension. In other words, it should be possible to extract the main content of a large attention weight with dimension `D` into a vector with a much smaller dimension `d`, where `d` <<< `D`.\n",
        "\n",
        "The authors of LoRA then applied the following assumption: changes in weights during model adaptation/updates also exhibit this property of residing on lower instrinc dimensions. In theory then, it should be possible to learn this smaller dimension vector instead of directly updating the large matrix `D`.\n",
        "\n",
        "**Does this come for free?**\n",
        "\n",
        "Even though this method helps substantially to reduce the costs and speed during training, it does come with some additional costs during inference, if one keeps the learned matrices and original model separate. However, it is possible to merge the learned LoRA weights and original model together into one model again to actually have zero extra cost.\n",
        "\n",
        "Why would one consider keeping it separate? Well if one has many different tasks that one want to fine tune towards, having one large model optimised, with many multiple \"LoRA models\" for each task becomes much easier to maintain and run in production than various large models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri1FGEh6_RGE"
      },
      "source": [
        "#### 3.2.1 LoRA implementation <font color='blue'>Advanced</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EcRQF82_RGE"
      },
      "source": [
        "Now let's implement a simple LoRA module together!\n",
        "\n",
        "Before doing so, there're a couple of parameters / configs associated with LoRA:\n",
        "\n",
        "1. **target_modules**: Which layers or transformer matrices (Q, K, V) we should apply LoRA to?  \n",
        "2. **lora_rank**: What is the size of the rank to be used.  \n",
        "3. **lora_alpha**: This is used for scaling. This scaling helps to reduce the need to retune hyperparameters when we vary `lora_rank`. When optimizing with Adam, tuning `lora_alpha` is roughly the same as tuning the learning\n",
        "rate if we scale the initialization appropriately.\n",
        "4. **initialization**: We use a random Gaussian initialization for `A` and\n",
        "zero for `B`.   \n",
        "5. **dropout**: as usually applied in Deep Learning models.\n",
        "\n",
        "**Group task**: Can you think of why the initialization proposed in 4. is used?\n",
        "\n",
        "**Code task**: Finis the below implementation. Hint: If you unfamiliar with einsum, refer to the following [documentation](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfA2TGl3_RGE"
      },
      "outputs": [],
      "source": [
        "class Lora(nn.Module):\n",
        "  # Depend on Module we're applying LoRA to.\n",
        "  input_dims: int\n",
        "  output_dims: int\n",
        "\n",
        "  lora_rank: int\n",
        "  lora_alpha: float\n",
        "  lodra_dropout: float\n",
        "\n",
        "  a_init: nn.initializers.Initializer = nn.initializers.normal()\n",
        "  b_init: nn.initializers.Initializer =  nn.initializers.zeros_init()\n",
        "\n",
        "  def setup(self):\n",
        "    self.a_weights = # FINISH ME\n",
        "    self.b_weights = # FINISH ME\n",
        "\n",
        "  def __call__(self, input_array: chex.Array, attn_h: chex.Array, training: bool):\n",
        "    \"\"\"Implements LoRA technique.\n",
        "\n",
        "      Args:\n",
        "        input_array: Shaped[..., input_dims]\n",
        "        attn_h: Shaped[..., output_dims]\n",
        "      Returns:\n",
        "        output_array: Shaped[..., output_dims]\n",
        "    \"\"\"\n",
        "    low_rank = jnp.einsum('...i,ij->...j', input_array, self.a_weights)\n",
        "    output = # FINISH ME\n",
        "    return output + attn_h\n",
        "\n",
        "  def weights_for_inference(self, weights: chex.Array):\n",
        "    \"\"\"Return original weights + `LoRA` weights for no added costs during inference.\"\"\"\n",
        "    return jnp.einsum('ij,kj->ik', self.a_weights, self.b_weights) + # FINISH ME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "17HzrPnx_RGE"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "class Lora(nn.Module):\n",
        "  # Depend on Module we're applying LoRA to.\n",
        "  input_dims: int\n",
        "  output_dims: int\n",
        "\n",
        "  lora_rank: int\n",
        "  lora_alpha: float\n",
        "  lodra_dropout: float\n",
        "\n",
        "  a_init: nn.initializers.Initializer = nn.initializers.normal()\n",
        "  b_init: nn.initializers.Initializer = nn.initializers.zeros_init()\n",
        "\n",
        "  def setup(self):\n",
        "    self.a_weights = self.param('a_weights', self.a_init, (self.input_dims, self.lora_rank,))\n",
        "    self.b_weights = self.param('b_weights', self.b_init, (self.output_dims, self.lora_rank,))\n",
        "\n",
        "  def __call__(self, input_array: chex.Array, attn_h: chex.Array, training: bool):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        input_array: Shaped[..., input_dims]\n",
        "        attn_h: Shaped[..., output_dims]\n",
        "      Returns:\n",
        "        output_array: Shaped[..., output_dims]\n",
        "    \"\"\"\n",
        "    low_rank = jnp.einsum('...i,ij->...j', input_array, self.a_weights)\n",
        "    output = jnp.einsum('...j,...kj->...k', low_rank, self.b_weights)\n",
        "\n",
        "\n",
        "    scaling = self.lora_alpha / self.lora_rank\n",
        "    output = output * scaling\n",
        "    return output + attn_h\n",
        "\n",
        "  def weights_for_inference(self, weights: chex.Array):\n",
        "    return weights + jnp.einsum('ij,kj->ik', self.a_weights, self.b_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpCz5otl_RGE"
      },
      "source": [
        "#### 3.2.3 🤗 Deep dive into LoRA with Hugging Face! 🤗 <font color='orange'>Beginner</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi3RLBB8_RGE"
      },
      "source": [
        "While the implementation of LoRA is currently in place, there is still some additional work required to ensure it aligns perfectly with the outlined methods in the research paper. Fortunately, the open-source community is rapidly evolving. As a result, LoRA, including most of its variants, has now been fully developed and features a user-friendly interface for ease of use, rather than building it from scratch.\n",
        "\n",
        "We'll now finetune `gpt2-medium` to generate song lyrics from the artist of your choice! We will do this by:\n",
        "\n",
        "* Loading a pretrained model using Hugging Face transformers\n",
        "* Gathering the dataset using datasets\n",
        "* Fine tune using LoRA\n",
        "\n",
        "Code in this section is based on: https://github.com/22-hours/cabrita/blob/main/notebooks/train_lora.ipynb from [piEsposito](https://github.com/piEsposito) and\n",
        "[pedrogengo](https://github.com/pentrogengo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwZbrcFY_RGF"
      },
      "source": [
        "##### Gathering and processing data (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYRtUgsM_RGF"
      },
      "source": [
        "To train a model, we need to gather the dataset from Hugging Face to load some song lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it329o0D_RGF"
      },
      "outputs": [],
      "source": [
        "dataset_author = \"huggingartists\"\n",
        "artist_name = \"the-beatles\"\n",
        "\n",
        "# List all avalable datasets.\n",
        "all_datasets = {}\n",
        "for ds in huggingface_hub.list_datasets(author=dataset_author):\n",
        "  music_artist = ds.id.replace(f'{dataset_author}/', '')\n",
        "  all_datasets[music_artist] = ds.id\n",
        "\n",
        "dataset_name = all_datasets[artist_name]\n",
        "\n",
        "print(f'Choose an artist available in {dataset_author}/ (careful! Some lyrics might contain offensive language)')\n",
        "Dropdown_ = widgets.Dropdown(\n",
        "    options=all_datasets.keys(),\n",
        "    value=artist_name,\n",
        ")\n",
        "output = widgets.Output()\n",
        "\n",
        "\n",
        "def on_change(change):\n",
        "  global dataset_name\n",
        "  global artist_name\n",
        "  artist_name = change[\"new\"]\n",
        "  dataset_name = all_datasets[artist_name]\n",
        "  print(f'`dataset_name` is now {dataset_name}.')\n",
        "\n",
        "Dropdown_.observe(on_change, names='value')\n",
        "display(Dropdown_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S6xcI_R_RGF"
      },
      "outputs": [],
      "source": [
        "formatted_artist_name = artist_name.replace('-', ' ').title()\n",
        "prompt = f'This is a song by {formatted_artist_name}. It goes like this:\\n\\n' # @param\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1evNephNjPE9"
      },
      "outputs": [],
      "source": [
        "# Let's use only the first 500 examples so it doesn't take too long to finetune.\n",
        "train_dataset = datasets.load_dataset(dataset_name, split='train')\n",
        "# Let's use up to 500 examples so it doesn't take too long to finetune.\n",
        "train_dataset = train_dataset.filter(lambda example, idx: idx < 500, with_indices=True)\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "devAUlZF_RGF"
      },
      "source": [
        "To train the model we usually set a fixed `sequence_length` for all inputs. A way to achieve this is by:\n",
        "* Padding inputs shorter than `sequence_length`.\n",
        "* Truncating inputs longer than `sequence_length`.\n",
        "\n",
        "**Think about this**: How do we know which `sequence_length` to set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS-pk8JZ_RGF"
      },
      "outputs": [],
      "source": [
        "max_num_tokens = 256 # @param\n",
        "\n",
        "num_chars, num_tokens = [], []\n",
        "\n",
        "for i, text in enumerate(train_dataset['text']):\n",
        "  text = prompt + text\n",
        "  num_tokens.append(len(tokenizer.tokenize(text)))\n",
        "  num_chars.append(len(text))\n",
        "\n",
        "num_chars = np.array(num_chars)\n",
        "num_tokens = np.array(num_tokens)\n",
        "print('Median #chars', np.median(num_chars))\n",
        "print('Max #chars', np.max(num_chars))\n",
        "print('Median #tokens', np.median(num_tokens))\n",
        "print('Max #tokens', np.max(num_tokens))\n",
        "num_truncations = np.sum(num_tokens > max_num_tokens)\n",
        "num_truncated_tokens = num_tokens - max_num_tokens\n",
        "median_num_truncated_tokens = np.median(\n",
        "    np.where(num_truncated_tokens > 0, num_truncated_tokens, 0),\n",
        ")\n",
        "print(f'Number of examples that will be truncated: {num_truncations} ({num_truncations/len(num_tokens) * 100:.2f} %)')\n",
        "plt.boxplot(num_truncated_tokens)\n",
        "plt.title('#truncated tokens')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srz6WVLi_RGF"
      },
      "source": [
        "**Discussion:** By only truncating the data we're losing A LOT of relevant text. What we could do to avoid this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuiJEdX0_RGF"
      },
      "source": [
        "Now we need to start preprocessing the data for training, and create a function tokenize our data for the model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpyfuDzO_RGG"
      },
      "outputs": [],
      "source": [
        "# Drop empty lyrics.\n",
        "train_dataset = train_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
        "print('Here is a data example before tokenization')\n",
        "print_sample(prompt, train_dataset[0][\"text\"])\n",
        "# Add prompt\n",
        "train_dataset = train_dataset.map(lambda x: {\"text\": prompt + x[\"text\"]})\n",
        "\n",
        "def tokenize(prompt):\n",
        "  result = tokenizer(\n",
        "      prompt,\n",
        "      truncation=True,\n",
        "      max_length=max_num_tokens,\n",
        "      padding=\"max_length\",\n",
        "  )\n",
        "  return {\n",
        "      \"input_ids\": result[\"input_ids\"],\n",
        "      \"attention_mask\": result[\"attention_mask\"],\n",
        "  }\n",
        "\n",
        "train_dataset = train_dataset.shuffle().map(lambda x: tokenize(x[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUAjpRx3_RGG"
      },
      "source": [
        "##### Finetune a model with LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SUwsbIT_RGG"
      },
      "source": [
        "Below we fine tune our model with LoRA. The first thing to do now is to set our hyperparameters for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s5nPAOjy_RGG"
      },
      "outputs": [],
      "source": [
        "# @title Hyper-parameters\n",
        "MICRO_BATCH_SIZE = 4 # @param\n",
        "BATCH_SIZE = 32 # @param\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "EPOCHS = 40 # @param\n",
        "LEARNING_RATE = 3e-4 # @param\n",
        "CUTOFF_LEN = max_num_tokens\n",
        "LORA_R = 12 # @param\n",
        "LORA_ALPHA = 12 # @param\n",
        "LORA_DROPOUT = 0.2 # @param\n",
        "WARMUP_STEPS = 20 # @param\n",
        "QUERY_USED_DURING_TRAINING = \"Dreams of llamas\" # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USiriyKp_RGG"
      },
      "source": [
        "Now we load our model in using the PEFT library. Remember, the PEFT library is where all these optimisation and efficient training methods are created in.\n",
        "\n",
        "**Discussion:** Do you know what each of the hyper-parameters do above? Do you understand the intution behind each of them? Try thinking about this and discussing it with your colleagues and tutors about this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvOdyzJj_RGG"
      },
      "outputs": [],
      "source": [
        "peft_config = peft.LoraConfig(\n",
        "    task_type=peft.TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    # Default is to apply lora to Q and V projection matrices.\n",
        "    # These are based on results of Table 5 of the paper.\n",
        "    target_modules=None,\n",
        ")\n",
        "peft_model = peft.get_peft_model(copy.deepcopy(model), peft_config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRvf4Ygr_RGG"
      },
      "source": [
        "With all of the groundwork finally laid out, we can train our model! We are using a bit of hacking to better showcase what is happening. It is not too important to understand everything below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX7xt87v_RGG"
      },
      "outputs": [],
      "source": [
        "class PlotLossCalback(transformers.TrainerCallback):\n",
        "  def on_epoch_end(self, args, state, control, model=None, tokenizer=None, logs=None, **kwargs):\n",
        "    states_history = state.log_history\n",
        "    losses, learning_rates, steps = [], [], []\n",
        "    for curr_state in state.log_history:\n",
        "      if 'loss' not in curr_state:  # Evaluation from `HackyTrainerThatRunsSampleInTheLoop`.\n",
        "        continue\n",
        "      losses.append(curr_state['loss'])\n",
        "      learning_rates.append(curr_state['learning_rate'])\n",
        "      steps.append(curr_state['step'])\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
        "    ax1.plot(steps, losses, '-ob')\n",
        "    ax1.set_title('Steps vs Loss')\n",
        "\n",
        "    ax2.plot(steps, learning_rates, '-or')\n",
        "    ax2.set_title('Step vs Learning Rate')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class HackyTrainerThatRunsSampleInTheLoop(transformers.Trainer):\n",
        "  def prediction_step(\n",
        "      self,\n",
        "      model,\n",
        "      inputs,\n",
        "      prediction_loss_only: bool,\n",
        "      ignore_keys: list[str] | None = None,\n",
        "      # Return: loss, logits, labels\n",
        "  ) -> tuple[torch.Tensor | None, torch.Tensor | None, torch.Tensor | None]:\n",
        "    del inputs, prediction_loss_only, ignore_keys  # unused.\n",
        "    _ = run_sample(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        prompt=prompt + QUERY_USED_DURING_TRAINING,\n",
        "        seed=1,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    return (None, None, None)\n",
        "\n",
        "training_arguments = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,\n",
        "    logging_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=10,\n",
        "    # lr_scheduler_type=\"cosine\",\n",
        "    output_dir=\"tmp\",\n",
        "    save_strategy=\"no\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        ")\n",
        "trainer = HackyTrainerThatRunsSampleInTheLoop(\n",
        "    model=peft_model,\n",
        "    train_dataset=train_dataset,\n",
        "    # Unused. But needed to run hacky inference.\n",
        "    eval_dataset=[{'input_ids': [], 'attention_mask': []}],\n",
        "    args=training_arguments,\n",
        "    callbacks=[PlotLossCalback],\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "peft_model.config.use_cache = False\n",
        "trainer.train(resume_from_checkpoint=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkWj5bxd_RGG"
      },
      "source": [
        "### ⏰⚡ Demo Time with our trained model🚀😰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_9Qu2HW_RGG"
      },
      "outputs": [],
      "source": [
        "seed = 2\n",
        "query = \"You\"\n",
        "final_prompt = prompt + query\n",
        "temperature = 1.0\n",
        "top_p = 0.9\n",
        "print('LoRA model')\n",
        "_ = run_sample(\n",
        "    peft_model,\n",
        "    tokenizer,\n",
        "    prompt=final_prompt,\n",
        "    seed=seed,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        ")\n",
        "\n",
        "print('Original model')\n",
        "_ = run_sample(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt=final_prompt,\n",
        "    seed=seed,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_zlXaAs_RGG"
      },
      "source": [
        "That is pretty awesome is not it not?\n",
        "\n",
        "As a challenge, play with all the hyperparameters above, as well as model choices, and see if you can beat your friend to create the best lyric generator using your own custom LLM trained with the LoRA technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## **Conclusion**\n",
        "**Summary:**\n",
        "\n",
        "You have now learned all the basics of how a LLM works, all the way from the pure fundamentals to finetuning a GPT architecture with LoRA. These are powerful tools and very applicable for many tasks, but just like any other deep learning model, they are just models and should be used for the correct problem and data.\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "Follow all the links provided in this practical, as well as reading up on the llama2 and Falcon architectures to see how the latest techniques are utilised.\n",
        "\n",
        "\n",
        "**References:** for further references check the links referenced throughout\n",
        "specific sections of this colab.\n",
        "\n",
        "* Attention is all you need paper: https://arxiv.org/abs/1706.03762\n",
        "* LoRA paper: https://arxiv.org/abs/2106.09685\n",
        "* RLHF (how ChatGPT was trained): https://huggingface.co/blog/rlhf\n",
        "* Extending context lenght: https://kaiokendev.github.io/context\n",
        "\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2023)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "# Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/Cg9aoa7czoZCYqxF7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6EqhIg1odqg0"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
