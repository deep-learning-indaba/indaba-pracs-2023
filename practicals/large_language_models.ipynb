{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# LLMs for everyone\n",
        "\n",
        "<img src=\"https://www.marktechpost.com/wp-content/uploads/2023/05/Blog-Banner-3.jpg\" width=\"60%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/llm-draft/practicals/large_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "Â© Deep Learning Indaba 2023. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "\n",
        "**Reviewers:**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Welcome to \"LLMs for Everyone,\" a practical exploration into the captivating world of Language Models! This entire introductory block of text was crafted solely by ChatGPT, showcasing the remarkable capabilities of these models. Throughout this tutorial, we will delve into the underlying fundamentals of transformers, the powerful technology that drives models like GPT, and learn how to fine-tune and train our very own Language Models. Let's embark on this exciting journey of understanding and creating LLMs, and discover how such impressive AI text generation is made possible! ðŸš€ðŸ“š\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: [Attention Mechanism, Transformer Architecture, LoRA & QLoRA, RLHF; Multimodal Transformers; and other relevant developments related to LLMs]\n",
        "Level: [<font color='orange'>`Beginner`</font>, <font color='green'>`Intermediate`</font>, <font color='blue'>`Advanced`</font>, <font color='blue'>`Advanced`</font>]\n",
        "\n",
        "[Let's use the colours from notion here. E.g. <font color='grey'>`Beginner`</font> and <font color='blue'>`Generative Models`</font>.]\n",
        "\n",
        "\n",
        "*TODO*: Change to correct colour and format.\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "* Understanding attention mechanishms and why they are used\n",
        "* Understand and implement the fundamental building blocks of the Transformer Architecture\n",
        "* Finetuning a LLM (7 billion parameters) on a single GPU using PEFT techniques, speficially LoRA\n",
        "* Introduce references to trending research and applications of LLMs\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "* To populate still\n",
        "\n",
        "**Outline:**\n",
        "\n",
        ">[LLMs for everyone](#scrollTo=m2s4kN_QPQVe)\n",
        "\n",
        ">>[Installation and Imports](#scrollTo=6EqhIg1odqg0)\n",
        "\n",
        ">>[1. Attention (OPTIONAL)](#scrollTo=-ZUp8i37dFbU)\n",
        "\n",
        ">>>[Sequence to sequence attenion mechanisms - Intermediate](#scrollTo=aQfqM1EJyDXI)\n",
        "\n",
        ">>>[Self-attention to Multihead Attention- Intermediate](#scrollTo=J-MU6rrny8Nj)\n",
        "\n",
        ">>>>[Self-attention](#scrollTo=0AFUEFZGzCTv)\n",
        "\n",
        ">>>>>[Queries, keys and values](#scrollTo=pwOIMtdZzdTf)\n",
        "\n",
        ">>>>>[Scaled dot product attention](#scrollTo=OhGZHFsHz_Qp)\n",
        "\n",
        ">>>>>[Masked attention](#scrollTo=D7B-AgO80gIt)\n",
        "\n",
        ">>>>[Multihead Attention - Advanced](#scrollTo=hNHklaSV1Tej)\n",
        "\n",
        ">>[2. Building your own LLM](#scrollTo=e9NW58_3hAg2)\n",
        "\n",
        ">>>[2.1 Highlevel overvierw](#scrollTo=bA_2coZvhAg3)\n",
        "\n",
        ">>>[2.2 Tokenization + Positional encoding](#scrollTo=fbTsk0MdhAhC)\n",
        "\n",
        ">>>>[2.2.1 Tokenization](#scrollTo=DehUpfym_RF8)\n",
        "\n",
        ">>>>[2.2.2 Positional encodings](#scrollTo=639s7Zuk_RF9)\n",
        "\n",
        ">>>>>[Sine and cosine functions](#scrollTo=rklY-aL-_RF9)\n",
        "\n",
        ">>>[2.3 Transformer block](#scrollTo=SdNPg0pnhAhG)\n",
        "\n",
        ">>>>[2.3.1 FFN](#scrollTo=kTURbfr__RF-)\n",
        "\n",
        ">>>>[2.3.2 Add and Norm block](#scrollTo=Sts5Vr4i_RF-)\n",
        "\n",
        ">>>[2.4 Building the Transformer Decoder / LLM](#scrollTo=91dXd29b_RF_)\n",
        "\n",
        ">>>[2.5 Training your LLM](#scrollTo=wmt3tp38G90A)\n",
        "\n",
        ">>>>[2.5.1 Training objective](#scrollTo=agLIpsoh_RGA)\n",
        "\n",
        ">>>>[2.5.2 Training models](#scrollTo=4CSfvGj__RGA)\n",
        "\n",
        ">>>>[2.5.3 Inspecting the trained LLM](#scrollTo=pGv9c2AFmF4V)\n",
        "\n",
        ">>[Customising LLMs](#scrollTo=C4hKnTFbHtdM)\n",
        "\n",
        ">>>[3.1 Hugging Face](#scrollTo=8FZ-_nm3_RGC)\n",
        "\n",
        ">>>[3.2 Adapter and Fine-Tuning methods](#scrollTo=KoTvhvap_RGC)\n",
        "\n",
        ">>>>[3.2.1 Prefix tuning](#scrollTo=znctvjrE_RGC)\n",
        "\n",
        ">>>>[3.2.2 Adapter mMthods](#scrollTo=U4sLxSol_RGD)\n",
        "\n",
        ">>>[3.3 LoRA](#scrollTo=MoBc08xY_RGD)\n",
        "\n",
        ">>>>[3.3.1 LoRA implementation](#scrollTo=ri1FGEh6_RGE)\n",
        "\n",
        ">>>>[3.3.3 ðŸ¤— Deep dive into LoRA with Hugging Face! ðŸ¤—](#scrollTo=mpCz5otl_RGE)\n",
        "\n",
        ">>>>>[3.3.3.1 Load Hugging Face model and run sample](#scrollTo=_N-BSs9b_RGE)\n",
        "\n",
        ">>>>>[3.3.3.2 Gathering and processing data (optional)](#scrollTo=NwZbrcFY_RGF)\n",
        "\n",
        ">>>>>[3.3.3.3 Finetune a model with LoRA](#scrollTo=qUAjpRx3_RGG)\n",
        "\n",
        ">>>[â°âš¡ Demo Time with our trained modelðŸš€ðŸ˜°](#scrollTo=JkWj5bxd_RGG)\n",
        "\n",
        ">>[4.0  RLHF; Multimodal Transformers; and other relevant developments related to LLMs](#scrollTo=76Yc926IcbuL)\n",
        "\n",
        ">>[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
        "\n",
        ">[Feedback](#scrollTo=o1ndpYE50BpG)\n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n",
        "[Any other tasks just before starting.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952qogb79nnY"
      },
      "source": [
        "**Suggested experience level in this topic:**\n",
        "\n",
        "| Level         | Experience                            |\n",
        "| --- | --- |\n",
        "`Beginner`      | It is my first time being introduced to this work. |\n",
        "`Intermediate`  | I have done some basic courses/intros on this topic. |\n",
        "`Advanced`      | I work in this area/topic daily. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YBdDHcI_ArCR"
      },
      "outputs": [],
      "source": [
        "# @title **Paths to follow:** What is your level of experience in the topics presented in this notebook? (Run Cell)\n",
        "experience = \"advanced\" #@param [\"beginner\", \"intermediate\", \"advanced\"]\n",
        "\n",
        "sections_to_follow=\"\"\n",
        "\n",
        "if experience == \"beginner\":\n",
        "  sections_to_follow=\"Introduction -> 1.1 Subsection -> 2.1 Subsection -> Conclusion -> Feedback\"\n",
        "elif experience == \"intermediate\":\n",
        "  sections_to_follow=\"Introduction -> 1.2 Subsection -> 2.2 Subsection -> Conclusion -> Feedback\"\n",
        "elif experience == \"advanced\":\n",
        "  sections_to_follow=\"Introduction -> 1.3 Subsection -> 2.3 Subsection -> Conclusion -> Feedback\"\n",
        "\n",
        "print(f\"Based on your experience, it is advised you follow these -- {sections_to_follow} sections. Note this is just a guideline.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "!pip install transformers datasets\n",
        "!pip install seaborn umap-learn\n",
        "!pip install livelossplot\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install -q peft\n",
        "\n",
        "# Python utils\n",
        "!pip install -q ipdb      # debugging.\n",
        "!pip install -q colorama  # print colors :).\n",
        "\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if os.environ[\"COLAB_GPU\"] and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "\n",
        "import chex\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import optax\n",
        "\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "import peft\n",
        "\n",
        "from PIL import Image\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "# Utils.\n",
        "import colorama\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# download images used in notebook\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80\",\n",
        "    \"cat.png\",\n",
        ")\n",
        "\n",
        "import copy\n",
        "\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"word2vec_sample\")\n",
        "\n",
        "import huggingface_hub\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-9X10jhocGaS"
      },
      "outputs": [],
      "source": [
        "# @title Helper Plotting Functions. (Run Cell)\n",
        "def plot_position_encodings(P, max_tokens, d_model):\n",
        "    \"\"\"Function that takes in a position encoding matrix and plots it.\"\"\"\n",
        "\n",
        "    plt.figure(figsize=(20, np.min([8, max_tokens])))\n",
        "    im = plt.imshow(P, aspect=\"auto\", cmap=\"Blues_r\")\n",
        "    plt.colorbar(im, cmap=\"blue\")\n",
        "\n",
        "    if d_model <= 64:\n",
        "        plt.xticks(range(d_model))\n",
        "    if max_tokens <= 32:\n",
        "        plt.yticks(range(max_tokens))\n",
        "    plt.xlabel(\"Embedding index\")\n",
        "    plt.ylabel(\"Position index\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_image_patches(patches):\n",
        "    \"\"\"Function that takes in a list of patches and plots them.\"\"\"\n",
        "    axes = []\n",
        "    fig = plt.figure(figsize=(25, 25))\n",
        "    for a in range(patches.shape[1]):\n",
        "        axes.append(fig.add_subplot(1, patches.shape[1], a + 1))\n",
        "        plt.imshow(patches[0][a])\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_projected_embeddings(embeddings, labels):\n",
        "    \"\"\"Function that takes in a list of embeddings projects them onto a 2D space and plots them using UMAP.\"\"\"\n",
        "    import umap\n",
        "    import seaborn as sns\n",
        "\n",
        "    projected_embeddings = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.title(\"Projected text embeddings\")\n",
        "    sns.scatterplot(\n",
        "        x=projected_embeddings[:, 0], y=projected_embeddings[:, 1], hue=labels\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
        "    \"\"\"Function that takes in a weight matrix and plots it with custom axis ticks\"\"\"\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
        "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
        "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
        "    plt.title(\"Attention matrix\")\n",
        "    plt.xlabel(\"Attention score\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kMkaKekB_pR4"
      },
      "outputs": [],
      "source": [
        "# @title Helper Text Processing Functions. (Run Cell)\n",
        "\n",
        "def get_word2vec_embedding(words):\n",
        "    \"\"\"\n",
        "    Function that takes in a list of words and returns a list of their embeddings,\n",
        "    based on a pretrained word2vec encoder.\n",
        "    \"\"\"\n",
        "    word2vec_sample = str(find(\"models/word2vec_sample/pruned.word2vec.txt\"))\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "        word2vec_sample, binary=False\n",
        "    )\n",
        "\n",
        "    output = []\n",
        "    words_pass = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            output.append(jnp.array(model.word_vec(word)))\n",
        "            words_pass.append(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    embeddings = jnp.array(output)\n",
        "    del model  # free up space again\n",
        "    return embeddings, words_pass\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Function that takes in a string and removes all punctuation.\"\"\"\n",
        "    import re\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def print_sample(prompt: str, sample: str):\n",
        "  print(colorama.Fore.MAGENTA + prompt, end=\"\")\n",
        "  print(colorama.Fore.BLUE + sample)\n",
        "  print(colorama.Fore.RESET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **1. Attention (OPTIONAL)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acgW1ofF_RFz"
      },
      "source": [
        "\n",
        "In order to understand the transformer architecture, one must understand the concept of attention and how it is implemented in deep learning. The attention mechanism is inspired by how humans would look at an image or read a sentence.\n",
        "\n",
        "Let us take the image of the dog in human clothes below (image and example [source](https://lilianweng.github.io/posts/2018-06-24-attention/)). When paying *attention* to the red blocks of pixels, we will say that the yellow block of pointy ears is something we expected (correlated) but that the grey blocks of human clothes are unexpected for us (uncorrelated). This is *based on what we have seen in the past* when looking at pictures of dogs, specifically one of a Shiba Inu.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iEU7Cph2D2PCXp3YEHj30-EndhHAeB5T\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "Assume we want to identify the dog breed in this image. When we look at the red pixels, we tend to pay more *attention* to relevant pixels that are more similar or relevant to them, which could be the ones in the yellow box. We almost completely remove the snow in the background and theÂ human clothing for this task. However, when we begin looking at the background in anÂ attemptÂ to identify what is in it, we will fade out the dog pixels because they are irrelevant to the current task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usLBF2g0x5gH"
      },
      "source": [
        "The same thing happens when we read. In order to understand the entire sentence, we will learn to correlate and *attend to* certain words based on the context of the entire sentence.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j23kcfu_c3wINU6DUvxzMYNmp4alhHc9\" alt=\"drawing\" width=\"350\"/>\n",
        "\n",
        " For instance, in the first sentence in the image above, when looking at the word \"coding\", we pay more attention to the word \"Apple\" and \"computer\" because we know that when we speak about coding, \"Apple\" is actually referring to the company. However, in the second sentence, we realise we should not consider \" apple \" when looking at \"code\" because given the context of the rest of the sentence, we know that this apple is referring to an actual apple and not a computer.\n",
        "\n",
        "We can build better models by allowing by building mechanisms that mimic attention. It will enable our models to learn better representations of our input data by contextualising what it knows about some parts of the input based on other parts. In the following sections, we will delve deeper into the mechanisms that enable us to train our deep learning models to attend to input data in the context of other input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQfqM1EJyDXI"
      },
      "source": [
        "### Sequence to sequence attenion mechanisms - <font color='blue'>`Intermediate`</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68QBeG-4yDZ9"
      },
      "source": [
        "The first attention mechanisms were used in sequence-to-sequence models. These models were usually RNN encoder and decoder structures. The input sequence was processed sequentially by an RNN, encoding the sequence in a single context vector, which is then fed into another RNN that generates a new sequence. Below is an example of this ([source](https://lilianweng.github.io/posts/2018-06-24-attention/)).\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FKfaArN1rsLjzVWaJGpMLEcxEshSLXd6\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "Due to there only being one context vector, it was often found that for longer input sequences, information gets lost due to the inability of the encoders to remember longer sequences. The attention mechanism introduced in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) was proposed to solve this.\n",
        "\n",
        "Here, instead of relying on one static context vector, which is also only used once in the decoding process, let us provide information on the entire input sequence at every decoding step using a dynamic context vector. By doing this, the decoder can access a larger \"bank\" of memory and attend to the input's required information based on the current decoder RNN output state, $s_t$. This is shown below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fB5KObXcKo5x35xlIDIcjHTq1q75ejIB\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "In deep learning, attention can be interpreted as a vector of \"importance.\" To predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate how strongly it is correlated with, or \"attends to,\" other elements using the attention vector/weights. These attention weights are then used to generate a new weighted sum of the remaining elements, which represents the target [(source)](https://lilianweng.github.io/posts/2018-06-24-attention/).\n",
        "\n",
        "\n",
        "This, usually, consists of two steps for each decoding step $t$:\n",
        "\n",
        "1. Calculate the score (importance) for each $h_n$, given $s_{t-1}$ and generate an attention vector, $w_{n}$.\n",
        "  - $\\text{score} = a(s_{tâˆ’1}, h_{n})$, where $a$ can be any differentiable function\n",
        "  - $w_{n} = \\frac{\\exp \\left\\{a\\left(s_{t-1}, h_{n}\\right)\\right\\}}{\\sum_{j=1}^{N} \\exp \\left\\{a\\left(s_{t-1}, h_{j}\\right)\\right\\}}$, where we use the softmax function to generate relative attention weights\n",
        "2. Generate the final context vector, $c_t$\n",
        "  - $c_t=\\sum_{n=1}^{N} w_n h_{n}$\n",
        "\n",
        "The final state fed into the RNN to generate $s_{t+1}$, is given below, where $f$ can again be any combination method.\n",
        "\n",
        "$s_{t+1} = f\\left ( c_t, s_t \\right)$\n",
        "\n",
        "In Bahdanau et al., 2015, $f$ was a learned feedforward layer taking in the concatenated vector $[c_t; s_t]$, with $a(s_{tâˆ’1}, h_{n})$ being the dot product. Next, let us build up this attention schema.\n",
        "\n",
        "In dot product attention, the score is given by\n",
        "\n",
        "$a(s_{t-1}, h_n)=s_{t-1}^\\top h_n$\n",
        "\n",
        "**Code task**: Complete the dot product attention function below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn6DasDJyLS2"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(hidden_states, previous_state):\n",
        "  \"\"\"\n",
        "  Calculate the dot product between the hidden states and previous states.\n",
        "\n",
        "  Args:\n",
        "    hidden_states: A tensor with shape [T_hidden, dm]\n",
        "    previous_state: A tensor with shape [T_previous, dm]\n",
        "  \"\"\"\n",
        "\n",
        "  scores = # FINISH ME\n",
        "  w_n = # FINSIH ME\n",
        "  c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "  return w_n, c_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MlRlOStsyLap"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [2, 2])\n",
        "w_n, c_t = dot_product_attention(x, x)\n",
        "\n",
        "w_n_correct = jnp.array([[0.9567678, 0.04323225], [0.00121029, 0.99878967]])\n",
        "c_t_correct = jnp.array([[0.11144122, 0.95290256], [-1.5571996, -1.5321486]])\n",
        "\n",
        "assert jnp.allclose(w_n_correct, w_n), \"w_n is not calculated correctly\"\n",
        "assert jnp.allclose(c_t_correct, c_t), \"c_t is not calculated correctly\"\n",
        "\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LezLZVCoyVA_"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def dot_product_attention(hidden_states, previous_state):\n",
        "\n",
        "    # [T,d]*[d,N] -> [T,N]\n",
        "    scores = jnp.matmul(previous_state, hidden_states.T)\n",
        "    w_n = jax.nn.softmax(scores)\n",
        "\n",
        "    # [T,N]*[N,d] -> [T,d]\n",
        "    c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "    return w_n, c_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODURfDvYyev2"
      },
      "source": [
        "In order to show how the dot product can produce attention weights that make sense, let us use pretrained [word2vec](https://jalammar.github.io/illustrated-word2vec/) embeddings. These word2vec embeddings are generated by an encoder network that was trained to generate similar embeddings for words with similar meanings.\n",
        "\n",
        "Even though we are not processing something sequentially that contains context, the attention matrix should indicate which words are correlatedâ€”and would thus attend to each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-GErvdEynGc"
      },
      "outputs": [],
      "source": [
        "# when changing these words, note that if the word is not in the original\n",
        "# training corpus it will not be shown in the weight matrix plot.\n",
        "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
        "word_embeddings, words = get_word2vec_embedding(words)\n",
        "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
        "plot_attention_weight_matrix(weights, words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu9eXFwjysUl"
      },
      "source": [
        "Looking at the matrix, assuming the function was implemented correctly, we can see which words have similar meanings. The \"royal\" group of words have higher attention scores with each other than the \"food\" words, which all attend to one another. We also see that \"computers\" have very low attention scores for all of them, which shows that they are neither very related to \"royal\" or \"food\" words.  \n",
        "\n",
        "**Group task:**\n",
        "  - Play with the word selections above. See if you can find word combinations whose attention values seem counter-intuitive. Think of possible explanations. Which sense of a word did the attention scores capture?\n",
        "  - Ask your friend if they found examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFSfyP7Syxex"
      },
      "source": [
        "Dot product is only one of the ways to implement the scoring function for attention mechanisms, there is a more extensive list in this [blog](https://lilianweng.github.io/posts/2018-06-24-attention/#summary) post by Dr Lilian Weng.\n",
        "\n",
        "More resources:\n",
        "\n",
        "[A basic encoder-decoder model for machine translation](https://www.youtube.com/watch?v=gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=1)\n",
        "\n",
        "[Training and loss for encoder-decoder models](https://www.youtube.com/watch?v=aBZUTuT1Izs&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=2)\n",
        "\n",
        "[Basic attention](https://www.youtube.com/watch?v=BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-MU6rrny8Nj"
      },
      "source": [
        "### Self-attention to Multihead Attention- <font color='blue'>`Intermediate`</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRuLtxNey_EQ"
      },
      "source": [
        "Self-attention and multi-head attention (MHA) are the core building blocks for the transformer architecture. We will build up the intuition and implementation here in detail. Then in the **Transformers** section, you will see how this mechanism is utilised to build an attention only sequence-to-sequence model.\n",
        "\n",
        "\n",
        "Going forward in this section, we will represent a sentence by splitting it up into a list of words, then using the word2vec model from above to encode each word. In the transformers section, we will dive deeper into how we transform an input into a sequence of vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe1lR5_oynOR"
      },
      "outputs": [],
      "source": [
        "def embed_sentence(sentence):\n",
        "    # Embed a sentence using word2vec; for example use cases only.\n",
        "    sentence = remove_punctuation(sentence)\n",
        "    words = sentence.split()\n",
        "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
        "    return jnp.expand_dims(word_vector_sequence, axis=0), words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AFUEFZGzCTv"
      },
      "source": [
        "#### Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF2V3KI-za9l"
      },
      "source": [
        "Self-attention is an attention mechanism where each vector of a given input sequence attends to the entire sequence. To gain an intuition for why self-attention is important, let us think about the following sentence (example taken from [source](https://jalammar.github.io/illustrated-transformer/)):\n",
        "\n",
        "`\"The animal didn't cross the street because it was too tired.\"`\n",
        "\n",
        "A simple question about this sentence is what the word \"it\" refers to? Even though it might look simple, it can be tough for an algorithm to learn this. This is where self-attention comes in, as it can learn an attention matrix for the word \"it\" where a large weight is assigned to the word \"animal\".\n",
        "\n",
        "Self-attention also allows the model to learn how to interpret words with the same embeddings, such as apple, which can be a company or food, depending on the context. This is very similar to the hidden state found within an RNN, but this process, as you will see, allows the model to attend over the entire sequence in parallel, allowing longer sequences to be utilised.\n",
        "\n",
        "Self-attention consists of three concepts:\n",
        "\n",
        "- Queries, keys and values\n",
        "- Scaled dot product attention\n",
        "- Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwOIMtdZzdTf"
      },
      "source": [
        "##### **Queries, keys and values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEf7QWIWzdo1"
      },
      "source": [
        "Typically all attention mechanisms can be written in terms of `key-value` pairs and `queries` to calculate the attention matrix and new context vector.\n",
        "\n",
        "To gain intuition, one can interpret the `query` vector as containing the information we are interested in, which is used to determine the `values` we should attend to, based on the similarity between the `keys` (which are paired with the `values`) and the `query`. Thus the similarity between the `queries` and `keys` gives us our attention score, where that score then determines the attention put in conjunction with the `values`. Or as [Lena Voita](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) puts it:\n",
        "\n",
        "- Query: asking for information\n",
        "- Key: saying that it has some information\n",
        "- Value: giving the information\n",
        "\n",
        "In transformer architectures, we use learnable weights matrices, represented as $W_Q,W_K,W_V$, to project each sequence vector to unique $q$, $k$, and $v$ vectors.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-96YjPxhcqW6FczUYwErGXHp6YpoLltq\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "You will notice that the vectors $q,k,v$ are smaller in size than the input vectors. This will be covered at a later stage, but just know that it is a design choice for transformers and not required at all to work.\n",
        "\n",
        "This process can also be parallelised, as the input sequence can be represented as a matrix $X$, which can be transformed into query, key, and value matrices $Q$, $K$, and $V$ respectively:\n",
        "\n",
        "$Q=W_QX \\\\ K=W_KX \\\\ V=W_VX$\n",
        "\n",
        "Below we show the code that creates three linear layers, which projects the input data to the $Q,K,V$ matrices, where the output size can be adjusted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc8zjK6eziIV"
      },
      "outputs": [],
      "source": [
        "class SequenceToQKV(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, X):\n",
        "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "\n",
        "    # this can also be one layer, how do you think you would do it?\n",
        "    q_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    k_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    v_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "\n",
        "    Q = q_layer(X)\n",
        "    K = k_layer(X)\n",
        "    V = v_layer(X)\n",
        "\n",
        "    return Q, K, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhGZHFsHz_Qp"
      },
      "source": [
        "##### **Scaled dot product attention**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxycHDUW0BVE"
      },
      "source": [
        "Now that we have our `query`, `key` and `value` matrices, it is time to calculate the attention matrix. Remember, in attention mechanisms; we must first find a score for each sequence vector and then use these scores to create a new context vector. We do this in self-attention using scaled dot product attention with the formula below.\n",
        "\n",
        "$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$\n",
        "\n",
        "What happens here is similar to what we did in the dot product attention in the previous section, just applying the mechanism to the sequence itself. For each element in the sequence, we calculate the attention weight matrix between $q_i$ and $K$. We then multiply $V$ by each weight and finally sum all weighted vectors $v_{weighted}$ together to form a new representation for $q_i$. By doing this, we are essentially drowning out irrelevant vectors and bringing up important vectors in the sequence when our focus is on $q_1$.\n",
        "\n",
        "$QK^\\top$ is scaled by the square root of the dimension of the vectors, $\\sqrt{d_k}$, to ensure more stable gradients during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_UYNzrS0Hga"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    d_k = key.shape[-1]\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "    value = jnp.matmul(attention_weights, value)\n",
        "    return value, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuNaEjIm0PhV"
      },
      "source": [
        "Let's now see scaled dot product attention in action. We will take a sentence, embed each word using word2vec, and see what the final self-attention weights look like.\n",
        "\n",
        "We will not use the linear projection layers as they are not trained. Instead, we are going to make $X=Q=V=K$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oy2sWzR0Ok5"
      },
      "outputs": [],
      "source": [
        "sentence = \"I drink coke, but eat steak\"\n",
        "word_embeddings, words = embed_sentence(sentence)\n",
        "Q = K = V = word_embeddings\n",
        "\n",
        "# calculate weights and plot\n",
        "values, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "words = remove_punctuation(sentence).split()\n",
        "plot_attention_weight_matrix(attention_weights[0], words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG1Kxljr0Vzw"
      },
      "source": [
        "Keep in mind that we have not trained our attention matrix yet. However, we can see that by utilising the word2vec vectors as our sequence, we can see how scaled dot product attention already is capable of attending to \"eat\" when \"steak\" is our query and that the query \"drink\" attends more to \"coke\" and \"eat\".\n",
        "\n",
        "More resources:\n",
        "\n",
        "[Attention with Q,K,V](https://www.youtube.com/watch?v=k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7B-AgO80gIt"
      },
      "source": [
        "##### **Masked attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdRoKsu70gGW"
      },
      "source": [
        "There are cases where applying self-attention over the entire sequence is not practical. These can include:\n",
        "\n",
        "- Uneven length sequences batched together.\n",
        "  - When sending a batch of sequences through a network, the self-attention expects each sequence to be the same length. One handles this by padding the sequence. When calculating attention, ideally, these padding vectors should not be taken into consideration\n",
        "- Training a decoding model.\n",
        "  - When training decoder models, such as GPT-3, the decoder has access to the entire target sequence when training (as training is done in parallel). In order to prevent the method from cheating by looking at future tokens, we have to mask the future sequence data so that earlier data can not attend to it.\n",
        "\n",
        "By applying a mask to the final score calculated between queries and keys, we mitigate the influence of the unwanted sequence vectors. The vectors are masked by making the score between the query and their respective keys a VERY large negative value. This results in the softmax function pushing the attention weight very close to zero, and the resulting value will be summed out and not influence the final representation.\n",
        "\n",
        "\n",
        "Putting everything together, masked scaled dot product attention visually looks like this:\n",
        "\n",
        "<img src=\"https://windmissing.github.io/NLP-important-papers/AIAYN/assets/5.png\" alt=\"drawing\" width=\"200\"/>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Syx8_5E0eM9"
      },
      "outputs": [],
      "source": [
        "# example of building a mask for tokens of size 32\n",
        "mask = jnp.tril(jnp.ones((32, 32)))\n",
        "sns.heatmap(mask, cmap=\"Blues\")\n",
        "plt.title(\"Example of mask that can be applied\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwTJrQ20gDw"
      },
      "source": [
        "Lets now adapt our scaled dot product attention function to impliment masked attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVHpyNs_0ePh"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    d_k = key.shape[-1]\n",
        "    T_k = key.shape[-2]\n",
        "    T_q = query.shape[-2]\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
        "\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "    attention = jnp.matmul(attention_weights, value)\n",
        "    return attention, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNHklaSV1Tej"
      },
      "source": [
        "#### Multihead Attention - <font color='blue'>`Advanced`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1pk4C0F1Ta6"
      },
      "source": [
        "Rather than only computing the attention once, the multi-head attention (MHA) mechanism runs through the scaled dot-product attention multiple times in parallel. According to the paper, Attention is all you need, \"multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this\".\n",
        "\n",
        "Multi-head attention can be viewed as a similar strategy to stacking convolution kernels in a CNN layer. This allows the kernels to focus on and learn different features and rules, which is why multiple heads of attention also work. The process for MHA is given below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1q0Oq6IVEkkMfVSpY4LkHBP866mcoIFsh\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "As can be seen from the figure, the scaled dot product attention discussed earlier is just repeated $N$ times, with $3N$ learnable matrices for each head. The outputs from the different heads are then concatenated, whereafter it is fed through a linear projection, which produces the final representation.\n",
        "\n",
        "Due to these large amount of computations and memory requirements, a common design choice is to have the $W_{Qn}, W_{Kn}, W_{Vn}$ matrices produce embeddings of length $d_m/N$, where $d_m$ is the input sequence embedding size and $N$ is the number heads. By doing this, the MHA function is similar computation-wise to using a single head of attention.\n",
        "\n",
        "**Code Task:** Finish the implementation of MHA below. Hint, swapaxes and reshape is a good place to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jS0QesX0eR3"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "    self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "\n",
        "  def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "    if None in [Q, K, V]:\n",
        "      assert not X is None, \"X has to be provided if either Q,K,V not provided\"\n",
        "\n",
        "      # project all data to Q, K, V\n",
        "      Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "    # get the batch size, sequence length and embedding size\n",
        "    B, T, d_m = K.shape\n",
        "\n",
        "    # calculate heads embedding size (d_m/N)\n",
        "    head_size = d_m // self.num_heads\n",
        "\n",
        "    # B,T,d_m -> B, T, N, dm//N -> B, N, T, dm//N\n",
        "    q_heads = Q.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    k_heads = K.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    v_heads = V.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "    attention, attention_weights = scaled_dot_product_attention(\n",
        "        q_heads, k_heads, v_heads, mask\n",
        "    )\n",
        "\n",
        "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, d_m) - re-assemble all head outputs\n",
        "    attention = # FINISH ME\n",
        "\n",
        "    # apply Wo\n",
        "    X_new = self.Wo(attention)\n",
        "\n",
        "    if return_weights:\n",
        "      return X_new, attention_weights\n",
        "    else:\n",
        "      return X_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DQWnHB2jeEBD"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "\n",
        "mha = MultiHeadAttention(2, 8)\n",
        "# initialise model\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [1, 2, 8])\n",
        "params = mha.init(key, x)\n",
        "x_new = mha.apply(params, x)\n",
        "\n",
        "# TODO(ruan): verify if this is right.\n",
        "# Marianne: I had to update the `x_correct` here.\n",
        "x_correct = jnp.array(\n",
        "    [\n",
        "        [\n",
        "            [\n",
        "              -0.59349924, -0.79245573,  0.64649045, -0.52850205,\n",
        "              -0.4793459 , -0.34167248, -0.45467672,  0.8619362\n",
        "            ],\n",
        "            [\n",
        "              -0.7895622 , -0.9945788 ,  0.7638061 , -0.65239996,\n",
        "              -0.56319916, -0.2351217 , -0.39363512,  0.9993293\n",
        "            ],\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "assert jnp.allclose(x_correct, x_new), \"Not returning the correct value\"\n",
        "print(\n",
        "    \"It seems correct. Look at the answer below to compare methods then move to the transformers section.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kFRtzAFLd1UA"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "    self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "\n",
        "  def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "    if None in [Q, K, V]:\n",
        "      assert not X is None, \"X has to be provided if either Q,K,V not provided\"\n",
        "\n",
        "      # project all data to Q, K, V\n",
        "      Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "    # get the batch size, sequence length and embedding size\n",
        "    B, T, d_m = K.shape\n",
        "\n",
        "    # calculate heads embedding size (d_m/N)\n",
        "    head_size = d_m // self.num_heads\n",
        "\n",
        "    # B,T,d_m -> B, T, N, dm//N -> B, N, T, dm//N\n",
        "    q_heads = Q.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    k_heads = K.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "    v_heads = V.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "    attention, attention_weights = scaled_dot_product_attention(\n",
        "        q_heads, k_heads, v_heads, mask\n",
        "    )\n",
        "\n",
        "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, d_m) - re-assemble all head outputs\n",
        "    attention = attention.swapaxes(1, 2).reshape(B, -1, d_m)\n",
        "\n",
        "    # apply Wo\n",
        "    X_new = self.Wo(attention)\n",
        "\n",
        "    if return_weights:\n",
        "      return X_new, attention_weights\n",
        "    else:\n",
        "      return X_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxjK2bv-1iur"
      },
      "source": [
        "Until now, everything covered is not typically used on its own when constructing LLMs. However, they constitute the underlying mechanisms that enable these models to perform at such a high level. By comprehending these mechanisms, you can gain a better understanding of why LLMs may occasionally exhibit peculiar behavior and pinpoint potential starting points for debugging them.\n",
        "\n",
        "\n",
        "There has also been many optimisations to the MHA structure mentioned above, as the machine learning engineers find more and more ways to optimise this compute heavy step to scale the models. These include Multi-query attention (MQA) or grouped query attention (GQA).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **2. Building your own LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA_2coZvhAg3"
      },
      "source": [
        "### 2.1 Highlevel overvierw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BflycqAw_RF8"
      },
      "source": [
        "Originally the transformer was designed for machine translation, hence the encoder-decoder structure seen below.\n",
        "\n",
        "The encoder will receive an input sentence in one language and process it through multiple stacked `encoder blocks`. This creates a final representation, which contains helpful information necessary for the decoding task. This output is then fed into stacked `decoder blocks` that produce new outputs in an autoregressive manner.\n",
        "\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"350\" />\n",
        "\n",
        "\n",
        "The encoder consists of $N$ identical blocks, which process a sequence of token vectors sequentially. These blocks consist of 3 parts:\n",
        "\n",
        "1. A multi-head attention block. These are the transformer architecture's backbone. They process the data to generate representations for each token, ensuring that the necessary information for the task at hand is represented in the vectors. These are exactly the MHA we covered in the attention section previously.\n",
        "2. An MLP is applied to each input token separately and identically.\n",
        "3. Residual connection that adds the input tokens to the attended representations and a residual connection between the input to the MLP and its outputs. For both these connections, the result is normalized using layernorm. In certain implementations, these normalization steps are applied to the inputs rather than the outputs. Just like a Resnet, transformers are designed to be very deep models thus, these add and norm blocks are essential for a smooth gradient flow.  \n",
        "\n",
        "Similarly, the decoder block consists of $N$ identical blocks, however there is some variation within these block. Concretely, the different parts are:\n",
        "\n",
        "1. A masked multi-head attention block. This is an MHA block that performs _self-attention_ on the output sequence however this computation is restricted to the inputs that have already been seen. In other words, future tokens are blocked when making predictions.\n",
        "2. A multi-head attention block. This block receives the output of the final encoder block, the transformed tokens, and uses that as the key-value pairs, while using the output of the first MHA block as the query. In doing this, the model attends over the input required to perform the sequence task. This MHA block thus performs _cross-attention_ by looking at the encoder inputs.\n",
        "3. An MLP same as the encoder\n",
        "4. Residual connection same as the encoder.\n",
        "\n",
        "Given this original architecture, there have been several variation with others focusing on the encoder only and others the **decoder only**. Large language models(LLMs) such as GPT-2, GPT-3 and Turing-NLG were born out of decoder only architectures. These architecture look like:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
        "\n",
        "with the cross attention block missing as no encoder output is available. So to build a language model, we will focus on the decoder only architecture as seen above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbTsk0MdhAhC"
      },
      "source": [
        "### 2.2 Tokenization + Positional encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DehUpfym_RF8"
      },
      "source": [
        "#### 2.2.1 Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBiFpVBu_RF9"
      },
      "source": [
        "\n",
        "Transformers cannot handle raw strings of text. So to process text, the text is first split up into tokens. The tokens are then indexed and each token is assigned an embedding of size $d_{model}$. These embeddings can be learned during training or can come from a pretrained vocabulary of embeddings. This new sequence of token embeddings is then fed into the transformer architecture. This idea is visualised below.\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16euh4LADP_mcXywFwKKY3QQQkVplepiI\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "\n",
        "These token IDs are typically predicted when a model generates text, fills in missing words, etc.\n",
        "\n",
        "This process of splitting up text into tokens and assigning an ID to each token is called [tokenisation](https://huggingface.co/docs/transformers/tokenizer_summary). There are various ways to tokenise text, with some methods being trained directly from the data. When using pre-trained transformers, it is crucial to use the same tokeniser that was used to train the model. The previous link has in-depth descriptions of many widely known techniques.\n",
        "\n",
        "Below we show how the [BERT](https://arxiv.org/abs/1810.04805) model's tokeniser tokenises a sentence. We use [Hugging Face](https://huggingface.co/) for this part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJBMvlUA_RF9"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_input = bert_tokenizer(\"The practical is so much fun\")\n",
        "print(f\"Token IDs: {encoded_input['input_ids']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYbtZTVP_RF9"
      },
      "source": [
        "Here we can see that the tokeniser returns the IDs for each token, as shown in the figure. But counting the number of IDs, we see that it is larger than the number of words in the sentence. Let's print the tokens associated with each ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPZjiLis_RF9"
      },
      "outputs": [],
      "source": [
        "print(f\"Tokens: {bert_tokenizer.decode(encoded_input['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3K8UFlR_RF9"
      },
      "source": [
        "We can see the tokeniser attaches new tokens, `[CLS]` and `[SEP]`, to the start and end of the sequence. This is a BERT-specific requirement for training and inference. Adding special tokens is a very common thing to do. Using special tokens, we can tell a model when a sentence starts or ends or when a new part of the input starts. This can be helpful when performing different tasks.\n",
        "\n",
        "For instance, to pretrain specific transformers, they perform what is known as masked prediction. For this, random tokens in a sequence are replaced by the `[MASK]` token, and the model is trained to predict the correct token ID for the token replaced with that token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djMP4Ijz_RF9"
      },
      "source": [
        "**Drawback of using raw token**:\n",
        "\n",
        "One drawback of using raw tokens is that they lack any indication of the word's position in the sequence. This is evident when considering sentences like \"I am happy\" and \"Am I happy\" These two phrases have distinct meanings, and the model needs to grasp the word order to understand the intended message accurately.\n",
        "\n",
        "To address this, when converting the inputs into vectors, position vectors are introduced and added to these vectors to indicate the **position** of each word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "639s7Zuk_RF9"
      },
      "source": [
        "#### 2.2.2 Positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-hBFVYo_RF9"
      },
      "source": [
        "In most domains where a transformer can be utilised, there is an underlying order to the tokens produced, be it the order of words in a sentence, the location from which patches are taken in an image or even the steps taken in an RL environment. This order is very important in all cases; just imagine you interpret the sentence \"I have to read this book.\" as \"I have this book to read.\". Both sentences contain the exact same words, yet they have completely different meanings based on the order.\n",
        "\n",
        "As both the encoder and the decoder blocks process all tokens in parallel, the order of tokens is lost in these calculations. To cope with this, the sequence order has to be injected into the tokens directly. This can be done by adding *positional encodings* to the tokens at the start of the encoder and decoder blocks (though some of the latest techniques add positional information in the attention blocks). An example of how positional encodings alter the tokens is shown below.\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eSgnVN2hnEsrjdHygDGwk1kxEi8-dcFo\" alt=\"drawing\" width=\"650\"/>\n",
        "\n",
        "Ideally, these encodings should have these characteristics ([source](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)):\n",
        "* Each time-step should have a unique value\n",
        "* The distance between time steps should stay constant.\n",
        "* The encoding should be able to generalise to longer sequences than seen during training.\n",
        "* The encoding must be deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rklY-aL-_RF9"
      },
      "source": [
        "##### **Sine and cosine functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLcfkMku_RF9"
      },
      "source": [
        "\n",
        "In Attention is All you Need, the authors used a method that can satisfy all these requirements. This involves summing a combination of sine and cosine waves at different frequencies, with the formula for a position encoding at position $D$ shown below, where $i$ is the embedding index and $d_m$ is the token embedding size.\n",
        "\n",
        "\\\\\n",
        "\n",
        "$P_{D}= \\begin{cases}\\sin \\left(\\frac{D}{10000^{i/d_{m}}}\\right), & \\text { if } i \\bmod 2=0 \\\\ \\cos \\left(\\frac{D}{10000^{((i-1)/d_{m}}}\\right), & \\text { otherwise } \\end{cases}$\n",
        "\n",
        "\\\n",
        "\n",
        "Assuming our model as $d_m=8$, the position embedding will look like this:\n",
        "\n",
        "\\\n",
        "$P_{D}=\\left[\\begin{array}{c}\\sin \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{8/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{8/8}}\\right)\\end{array}\\right]$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Let's first create a function that can return these encodings to understand why this will work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5t5D30_RF9"
      },
      "outputs": [],
      "source": [
        "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
        "\n",
        "  assert token_embedding % 2 == 0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
        "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "  i = jnp.arange(0, token_embedding, 2)\n",
        "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
        "  frequencies = positions * frequency_steps\n",
        "\n",
        "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "\n",
        "  return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYW-VDOL_RF-"
      },
      "outputs": [],
      "source": [
        "token_sequence_length = 50  # Number of tokens the model will need to process\n",
        "token_embedding = 10000  # token embedding (and positional encoding) dimensions, ensure it is divisible by two\n",
        "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
        "plot_position_encodings(P, token_sequence_length, token_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mjHEDPO_RF-"
      },
      "source": [
        "Looking at the graph above, we can see that for each position index, there is a unique pattern forming, where each position index will always have the same encoding.\n",
        "\n",
        "**Group task**:\n",
        "\n",
        "- Discuss with your friend why we are seeing that specific pattern when `token_sequence_length` is 1000, and `token_embedding` is 768.\n",
        "- You can try playing around with smaller values for `token_sequence_length` and  `token_embedding` to get a better intuition for the above discussion.\n",
        "- Ask your friend why they think the 10000 constant is used in the functions above.\n",
        "- Make `token_sequence_length` to be 50 and `token_embedding` something large, like 10000. What do you notice? Is a large token embedding always needed?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdNPg0pnhAhG"
      },
      "source": [
        "### 2.3 Transformer block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4vSolF2_RF-"
      },
      "source": [
        "Just as a MLP or CNN is network is a stack of layers, transformers are also composed of a stack of transformer blocks. In this section we build out each one of these blocks that are required to form a transformer block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTURbfr__RF-"
      },
      "source": [
        "\n",
        "#### 2.3.1 FFN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTtFi9AZ_RF-"
      },
      "source": [
        "These blocks are just a single 2-layer MLP that uses ReLU activation in the original model. GeLU has also become very popular, and we will be using it throughout the practical.\n",
        "\n",
        "$$\n",
        "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
        "$$\n",
        "\n",
        "One can interpret this block as processing what the MHA block has produced and then projecting these new token representations to a space that the next block can use more optimally. Usually, the first layer is very wide, in the range of 2-8 times the size of the token representations. They do this as it is easier to parallelize computations for a single wider layer during training than to parallelize a feedforward block with multiple layers. Thus they can add in more complexity but keep training and inference optimized.\n",
        "\n",
        "**Code task:** Code up a Flax Module that implements the Feed forward block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsho1CnW_RF-"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  A 2-layer MLP which widens then narrows the input.\n",
        "\n",
        "  Args:\n",
        "    widening_factor [optional, default=4]: The size of the hidden layer will be d_model * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  widening_factor: int = 4\n",
        "  init_scale: float = 0.25\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      x: [B, T, d_m]\n",
        "    '''\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self.widening_factor * d_m\n",
        "\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "    )\n",
        "    layer1 = # FINISH ME\n",
        "    layer2 = # FINISH ME\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-qj0nfhH_RF-"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "  \"\"\"A 2-layer MLP which widens then narrows the input.\"\"\"\n",
        "  widening_factor: int = 4\n",
        "  init_scale: float = 0.25\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self.widening_factor * d_m\n",
        "\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "    )\n",
        "    layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
        "    layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sts5Vr4i_RF-"
      },
      "source": [
        "#### 2.3.2 Add and Norm block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWUpf8wt_RF-"
      },
      "source": [
        "In order to get transformers to go deeper, the residual connections are very important to allow an easier flow of gradients through the network. For normalisation, `layer norm` is used. This normalises each token vector independently in the batch. It is found that normalising the vectors improves the convergence and stability of transformers.\n",
        "\n",
        "There are two learnable parameters in layernorm, `scale` and `bias`, which rescales the normalised value. Thus, for each input token in a batch, we calculate the mean, $\\mu_{i}$ and variance $\\sigma_i^2$. We then normalise the token with:\n",
        "\n",
        "$\\hat{x}_i = \\frac{x_i-\\mu_{i}}{\\sigma_i^2 + Ïµ}$.\n",
        "\n",
        "Then $\\hat{x}$ is rescaled using the learned `scale`, $Î³$, and `bias` $Î²$, with:\n",
        "\n",
        "$y_i = Î³\\hat{x}_i + Î² = LN_{Î³,Î²}(x_i)$.\n",
        "\n",
        "So our add norm block can be represented as $LN(x+f(x))$, where $f(x)$ is either a MLP or MHA block.\n",
        "\n",
        "**Code task:** Code up a Flax Module that implements the add norm block. It should take as input the processed and unprocessed tokens. Hint: `hk.LayerNorm `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5bLb5Ly_RF_"
      },
      "outputs": [],
      "source": [
        "class AddNorm(nn.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, processed_x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: Sequence of tokens before feeding into MHA or FF blocks, with shape [B, T, d_m]\n",
        "      x: Sequence of after being processed by MHA or FF blocks, with shape [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      add_norm_x: Transformed tokens with shape [B, T, d_m]\n",
        "    '''\n",
        "\n",
        "    added = # FINISH ME\n",
        "    normalised = #FINISH ME\n",
        "    return normalised(added)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HXSi7BXZ_RF_"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "class AddNorm(nn.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, processed_x):\n",
        "\n",
        "    added = x + processed_x\n",
        "    normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
        "    return normalised(added)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91dXd29b_RF_"
      },
      "source": [
        "### 2.4 Building the Transformer Decoder / LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl0UAyvM_RF_"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
        "\n",
        "Most of the groundwork has happened. We have built the positional encoding block, the MHA block, the feed-forward block and the add&norm block.\n",
        "\n",
        "The only part needed is passing inputs to each decoder block and applying the masked MHA block found in the decoder blocks.\n",
        "\n",
        "**Code task:** Code up a FLAX Module that implements the (FFN(norm(MHA(norm(X))))) for the decoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVmSFKZK_RF_"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer decoder block.\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    d_m: Token embedding size\n",
        "    widening factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      encoder_output: Batch of tokens with was processed by the encoder, with shape [B, T_encoder, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    attention, attention_weights_1 = # FINISH ME\n",
        "\n",
        "    X = # FINISH ME\n",
        "\n",
        "    projection = # FINISH ME\n",
        "    X = # FINISH ME\n",
        "\n",
        "    return (X, attention_weights_1) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "stNZVVv3_RF_"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer decoder block.\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    d_m: Token embedding size\n",
        "    widening factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
        "\n",
        "    X = self.add_norm1(X, attention)\n",
        "\n",
        "    projection = self.MLP(X)\n",
        "    X = self.add_norm2(X, projection)\n",
        "\n",
        "    return (X, attention_weights_1) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXXVWd7_RF_"
      },
      "source": [
        "Next, we just put everything together, adding in the postiional encodings as well as stacking multiple transformer blocks and adding our prediction layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XBG24Qs_RF_"
      },
      "outputs": [],
      "source": [
        "class LLM(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer encoder consisting of several layers of decoder blocks.\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    num_layers: The number of decoder blocks to be used.\n",
        "    d_m: Token embedding size\n",
        "    vocab_size: The size of the vocabulary\n",
        "    widening_factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "  num_heads: int\n",
        "  num_layers: int\n",
        "  d_m: int\n",
        "  vocab_size: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.blocks = [\n",
        "        DecoderBlock(self.num_heads, self.d_m, self.widening_factor)\n",
        "        for _ in range(self.num_layers)\n",
        "    ]\n",
        "    self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m) # convert tokens to embedding\n",
        "    self.pred_layer = nn.Dense(self.vocab_size)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weights=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    # convert a token id to a d_m dimensioanl vector\n",
        "    X = self.embedding(X)\n",
        "    sequence_len = X.shape[-2]\n",
        "    positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "    X = X + positions\n",
        "\n",
        "    if return_att_weights:\n",
        "        att_weights = []\n",
        "    block_n = 0\n",
        "    for block in self.blocks:\n",
        "        out = block(X, mask, return_att_weights)\n",
        "        if return_att_weights:\n",
        "            X = out[0]\n",
        "            att_weights.append(out[1])\n",
        "        else:\n",
        "            X = out\n",
        "\n",
        "    # apply a linear layer and softmax to caclulate our logits over tokens\n",
        "    logits = nn.log_softmax(self.pred_layer(X))\n",
        "\n",
        "    return (\n",
        "        logits if not return_att_weights else (logits, jnp.array(att_weights).swapaxes(0, 1))\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sClFLLkU_RF_"
      },
      "source": [
        "If everything is correct, then if we run the code below, everything should run without any issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82CWEa5m_RGA"
      },
      "outputs": [],
      "source": [
        "B, T, d_m, N, vocab_size = 18, 32, 16, 8, 25670\n",
        "\n",
        "llm = LLM(num_heads=1, num_layers=1, d_m=d_m, vocab_size=vocab_size, widening_factor=4)\n",
        "mask = jnp.tril(np.ones((T, T)))\n",
        "\n",
        "# initialise module and get dummy output\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.randint(key, [B, T], 0, vocab_size)\n",
        "params = llm.init(key, X, mask=mask)\n",
        "\n",
        "# extract output from decoder\n",
        "logits, decoder_att_weights = llm.apply(\n",
        "    params,\n",
        "    X,\n",
        "    mask=mask,\n",
        "    return_att_weights=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gve7ssD__RGA"
      },
      "source": [
        "As a final sanity check, we can see that our attention weights behave as expected for now. The encoder weights can attend to all input sequences, and our decoder only attends to previous tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4NpywYv_RGA"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "plt.suptitle(\"LLM attention weights\")\n",
        "sns.heatmap(decoder_att_weights[0, 0, 0, ...], ax=ax, cmap=\"Blues\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmt3tp38G90A"
      },
      "source": [
        "### 2.5 Training your LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agLIpsoh_RGA"
      },
      "source": [
        "#### 2.5.1 Training objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOSv1-3B_RGA"
      },
      "source": [
        "Given a sequence of words, or prompt, an LLM learns to predict the next token/word by modeling a distribution over words available.\n",
        "\n",
        "Essentially:\n",
        "\n",
        "$$\n",
        "P\\left(y_{1}, y_{2}, \\ldots, y_{n}, \\mid c\\right)=\\prod_{t=1}^{n} p\\left(y_{t} \\mid y_{<t}, c\\right)\n",
        "$$\n",
        "\n",
        "This can be framed as an autoregressive problem where the decoder is given a token and predicts sentences based on the previous tokens.\n",
        "\n",
        "When training this decoder, the parameters are updated by computing the multi-class cross entropy\n",
        "loss and carrying out back propagation. The multi-class cross entropy loss is defined by:\n",
        "\n",
        "$$ \\text{Loss}_t = - \\sum y_t \\log \\hat{y}_t $$\n",
        "\n",
        "where $y_t$\n",
        "is the probability of target word at time step $t$ and $\\hat{y}_t$ is the probability of the predicted word at time step $t$. The loss for every sentence is computed by:\n",
        "\n",
        "$$ \\text{Sentence Loss} = \\frac{1}{sq} \\sum^{sq}_{t=1} \\text{Loss}_t $$\n",
        "\n",
        "**Code task**: Implement the cross-entropy loss function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXmjUYdDHseM"
      },
      "outputs": [],
      "source": [
        "def sequence_loss_fn(logits, targets):\n",
        "  '''\n",
        "  Compute the cross-entropy loss between predict token ID and true ID\n",
        "\n",
        "  Args:\n",
        "    logits: A array of shape [batch_size, sequence_length, vocab_size]\n",
        "    targets: The targets we are trying to predict\n",
        "\n",
        "  Returns:\n",
        "    loss: A scalar value representing the mean batch loss\n",
        "  '''\n",
        "\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  assert logits.shape == target_labels.shape\n",
        "\n",
        "  mask = jnp.greater(targets, 0)\n",
        "  loss = #FINISH ME\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4Cq5_4WN_RGA"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "VOCAB_SIZE = 25670\n",
        "targets = jnp.array([[0, 2, 0]])\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [1, 3, VOCAB_SIZE])\n",
        "loss = sequence_loss_fn(X, targets)\n",
        "real_loss = jnp.array(10.966118)\n",
        "assert jnp.allclose(real_loss, loss), \"Not returning the correct value\"\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cthfcbmC_RGA"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def sequence_loss_fn(logits, targets):\n",
        "  \"\"\"Compute the loss on data wrt params.\"\"\"\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  assert logits.shape == target_labels.shape\n",
        "  mask = jnp.greater(targets, 0)\n",
        "  loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "  loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CSfvGj__RGA"
      },
      "source": [
        "#### 2.5.2 Training models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIQ_aJGW_RGA"
      },
      "source": [
        "In the next section, we define all the processes required to train the model using the objective described above. A lot of this is now the work required to do training using FLAX.\n",
        "\n",
        "Below wegather the dataset and we shall be training on, which is Karpathy's shakespear dataset. Its not so important to understand this code, so either just run the cell to load the data, or view the code if you want to understand it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "guMHAaSo_RGB"
      },
      "outputs": [],
      "source": [
        "# @title Create Shakespeare dataset and iterator (optional, but run the cell)\n",
        "\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "\n",
        "class WordBasedAsciiDatasetForLLM:\n",
        "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Tokenize by splitting the text into words\n",
        "        words = corpus.split()\n",
        "        self.vocab_size = len(set(words))  # Number of unique words\n",
        "\n",
        "        # Create a mapping from words to unique IDs\n",
        "        self.word_to_id = {word: i for i, word in enumerate(set(words))}\n",
        "\n",
        "        # Store the inverse mapping from IDs to words\n",
        "        self.id_to_word = {i: word for word, i in self.word_to_id.items()}\n",
        "\n",
        "        # Convert the words in the corpus to their corresponding IDs\n",
        "        corpus = np.array([self.word_to_id[word] for word in words]).astype(np.int32)\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Only {num_batches} batches; consider a shorter \"\n",
        "                \"sequence or a smaller batch.\"\n",
        "            )\n",
        "\n",
        "        self._ds = WordBasedAsciiDatasetForLLM._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Yield next mini-batch.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Create the language modeling observation/target pairs.\n",
        "        return dict(\n",
        "            input=batch[:, :-1], target=batch[:, 1:]\n",
        "        )\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        \"\"\"Convert a sequence of word IDs to words.\"\"\"\n",
        "        return [self.id_to_word[id] for id in ids]\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WBIFg51oQl0"
      },
      "source": [
        "Lets now look how our data is structured for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvH3XPM5_RGB"
      },
      "outputs": [],
      "source": [
        "# sample and look at the data\n",
        "batch_size = 2\n",
        "seq_length = 32\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
        "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(obs)))\n",
        "    print(\"ASCII:\", obs)\n",
        "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(target)))\n",
        "    print(\"ASCII:\", target)\n",
        "\n",
        "print(f\"\\n Total vocabulary size: {train_dataset.vocab_size}\")\n",
        "\n",
        "VOCAB_SIZE = train_dataset.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9vzee53_RGB"
      },
      "source": [
        "Next, let us train our LLM and see how it performs in producing Shakespearian text. First, we will define what happens for every training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGuYBCkekgDw"
      },
      "outputs": [],
      "source": [
        "def train_step(model, params, optimizer, optimizer_state, batch):\n",
        "  def loss_fn(params):\n",
        "    T = batch['input'].shape[1]\n",
        "    logits = model.apply(params, batch['input'], jnp.tril(np.ones((T, T))))\n",
        "    loss = sequence_loss_fn(logits, batch['target'])\n",
        "    return loss\n",
        "\n",
        "  loss, gradients = jax.value_and_grad(loss_fn)(params)\n",
        "  updates, optimizer_state = optimizer.update(gradients, optimizer_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, optimizer_state, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtKWzKIAkfYU"
      },
      "source": [
        "Next we initialise our optimizer and model. Feel free to play with the hyperparameters during the practical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o3q-BZX_RGB"
      },
      "outputs": [],
      "source": [
        "# all hyperparameters\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "num_layers = 1\n",
        "widening_factor = 2\n",
        "LR = 2e-3\n",
        "batch_size = 32\n",
        "seq_length = 64\n",
        "\n",
        "# set up the data\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "batch = next(train_dataset)\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "# initialise model\n",
        "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
        "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "params = llm.init(key, batch['input'], mask)\n",
        "\n",
        "# set up the optimiser\n",
        "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
        "optimizer_state = optimizer.init(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bPEFakxmvsM"
      },
      "source": [
        "Now we train! This will take at least 15 minutes.. While it trains, have you greeted your neighbour yet?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUAS6tie_RGB"
      },
      "outputs": [],
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 7000\n",
        "LOG_EVERY = 32\n",
        "losses = []\n",
        "VOCAB_SIZE = 25670\n",
        "\n",
        "# Training loop\n",
        "for step in range(MAX_STEPS):\n",
        "    batch = next(train_dataset)\n",
        "    params, optimizer_state, loss = train_step(llm, params, optimizer, optimizer_state, batch)\n",
        "    losses.append(loss)\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_ = jnp.array(losses).mean()\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"loss\": loss_,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()\n",
        "        losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGv9c2AFmF4V"
      },
      "source": [
        "#### 2.5.3 Inspecting the trained LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfq61gim_RGB"
      },
      "source": [
        "Lets generate some text now and see how our model did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lt8HTS__RGC"
      },
      "outputs": [],
      "source": [
        "def generate_random_shakespeare(llm, params, id_2_word, word_2_id):\n",
        "    '''\n",
        "    Get the model output\n",
        "    '''\n",
        "\n",
        "    prompt = \"Love\"\n",
        "    print(prompt)\n",
        "    tokens = prompt.split()\n",
        "\n",
        "    # predict and append\n",
        "    for i in range(15):\n",
        "      input = jnp.array([[word_2_id[t] for t in tokens]])\n",
        "      logits = llm.apply(params, input)\n",
        "      argmax_out = jnp.argmax(logits, axis=-1)\n",
        "      prediction = id_2_word[int(argmax_out[0][-1])]\n",
        "      tokens.append(prediction)\n",
        "      print(\" \"+prediction, end=\"\")\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "id_2_word = train_dataset.id_to_word\n",
        "word_2_id = train_dataset.word_to_id\n",
        "\n",
        "generated_shakespeare = generate_random_shakespeare(llm, params, id_2_word, word_2_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwNuMRf_RGC"
      },
      "source": [
        "Finally, we implemented everything above by taking the token ID with the maximum probability of being correct. This is greedy decoding, as we only took the most likely token. It worked well in this use case, but there are cases where we will see a degrading performance when taking this greedy approach, specifically when we are interested in generating realistic text.\n",
        "\n",
        "Other methods exist for sampling from the decoder, with a famous algorithm being beam search. We provide resources below for anyone interested in learning more about this.\n",
        "\n",
        "[Greedy Decoding](https://www.youtube.com/watch?v=DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=4)\n",
        "\n",
        "[Beam Search](https://www.youtube.com/watch?v=uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4hKnTFbHtdM"
      },
      "source": [
        "## 3. **Customising LLMs**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoqVNhCi_RGC"
      },
      "source": [
        "The availability of open source pretrained language models (LLMs), such as [LLAMA](https://github.com/facebookresearch/llama) and [FALCON](https://falconllm.tii.ae/) has been a game-changer in the field of natural language processing. These models, often comprising orders of billions of parameters, offer unprecedented language understanding capabilities. However, as their sizes have grown significantly, fine-tuning them for specific tasks has become more challenging than before.\n",
        "\n",
        "In this section, we will explore the intricacies of custom adaptation techniques, understanding how to effectively fine-tune these large LLMs and make the most of their extraordinary potential in our research and applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FZ-_nm3_RGC"
      },
      "source": [
        "### 3.1 Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgD88nz9_RGC"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png\" width=\"10%\">\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) is a startup founded in 2016 and, in their own words: \"are on a mission to democratize good machine learning, one commit at a time.\"\n",
        "\n",
        "They have developed various open-source packages and allow users to easily interact with a large corpus of pretrained transformer models (across all modalities) and datasets to train or fine-tune pre-trained transformers. Their software is used widely in industry and research. For more details on them and usage, refer to [last years transformer practical](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/practicals/attention_and_transformers.ipynb#scrollTo=qFBw8kRx-4Mk).\n",
        "\n",
        "For our exploration in this tutorial, we will predominantly utilize open source code from Hugging Face, primarily drawing from the transformers, datasets, and PEFT libraries. The [transformers](https://github.com/huggingface/transformers) library grants us access to pretrained LLMs, the [datasets](https://github.com/huggingface/datasets) library provides convenient access to various datasets for training, and the [PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft) library encompasses implementations of the training and adaptation methods we'll be discussing below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoTvhvap_RGC"
      },
      "source": [
        "### 3.2 Adapter and Fine-Tuning methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8ghCr8J_RGC"
      },
      "source": [
        "The world of open source LLMs brings forth an exciting range of possibilities, but their sheer size often poses a challenge for fine-tuning using consumer-grade hardware. Consequently, conventional adaptation methods fall short in such scenarios. To address this, innovative techniques have emerged to overcome these limitations.\n",
        "\n",
        "A significant proportion of these techniques involve either keeping the model parameters fixed, as seen in prompt engineering, where the input text acts as an agent to adapt the LLM's behavior, or altering only a tiny subset of model parameters. In this tutorial, our focus will be on the latter approach, presenting methods that modify a small portion of the model parameters, or bring additional parameters to a LLM.\n",
        "\n",
        "However, we encourage readers to explore [OpenAI's cookbook](https://github.com/openai/openai-cookbook), which hosts transferable recipes and links for prompting LLMs (as well as other models) for further insights and possibilities.\n",
        "\n",
        "A lot of content here is inspired by [Lightning AI blogs](https://lightning.ai/pages/community/article/understanding-llama-adapters/).\n",
        "\n",
        "**Discussion**: Before gettting started into methods stablished in the literature let's first think about (1) how would you go about doing more efficient finetuning? (2) Why finetuning the whole model is so costly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znctvjrE_RGC"
      },
      "source": [
        "#### 3.2.1 Prefix tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8JWUHWf_RGC"
      },
      "source": [
        "Prefix tuning works by introducing a trainable token/tensor into each transformer block along with the input tokens, as opposed to solely modifying the input tokens (prompt engineering) or finetuning the entire transformer bloc. The contrast between a standard transformer block and a transformer block enhanced with a prefix is depicted in the following figure. This was first introduce in the [\"Prefix-Tuning: Optimizing Continuous Prompts for Generation\" paper](https://arxiv.org/abs/2101.00190) by Xiang Lisa Li and Percy Liang.\n",
        "\n",
        "By only training the \"Trainable tokens\" and the new introduced MLP layer, we are able to adapt a model to our domain by training close to 0.1% of the parameters of a full model and achieve performance comparible to fine tuning the entire model.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fSnk9MkoPN6KbmbP71iU9EViU9avvOHb\" alt=\"drawing\" width=\"230\"/>\n",
        "\n",
        "Below we show pseudo code for this method, as well as a normal block to showcase the differences. Note running the code will *not* work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgpupW9w_RGD"
      },
      "outputs": [],
      "source": [
        "def normal_transformer_block(tokens):\n",
        "  \"\"\"\n",
        "  Example of pseudo code for a normal transformer.\n",
        "  \"\"\"\n",
        "  original_tokens = tokens\n",
        "  x = MHA(tokens)\n",
        "  x = LayerNorm(x + original_tokens)\n",
        "  original_tokens = x\n",
        "  x = FF(x)\n",
        "  transformed_tokens = LayerNorm(x + original_tokens)\n",
        "  return transformed_tokens\n",
        "\n",
        "def transformer_block_with_prefix(tokens, trainable_tokens):\n",
        "  \"\"\"\n",
        "  Example of pseudo code of transformer block with prefix tuning.\n",
        "  \"\"\"\n",
        "  prefix = FF(trainable_tokens)  # Trainable FF and tokens.\n",
        "  tokens = concat([prefix, tokens])\n",
        "  original_tokens = tokens\n",
        "  x = MHA(tokens)\n",
        "  x = LayerNorm(x + original_tokens)\n",
        "  original_tokens = x\n",
        "  x = FF(x)\n",
        "  transformed_tokens = LayerNorm(x+original_tokens)\n",
        "  return transformed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4sLxSol_RGD"
      },
      "source": [
        "#### 3.2.2 Adapter mMthods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiSHLMJV_RGD"
      },
      "source": [
        "Very similar, and introduced in the [\"Parameter-Efficient Transfer Learning for NLP\" paper](https://arxiv.org/abs/1902) by Houlsby etc, it consists of adding a new block of weights between the transformer blocks called \"Adapter\".\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1t521Q3_yAuUDsoakJmv7cgQyF5-VvjgX\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "During adapter tuning, the green layers are trained on the downstream data, this includes the adapter, the layer normalization parameters, and the final classification layer (not shown in the figure).\n",
        "\n",
        " It has been shown to achieve similar performance to updating an entire network while only training 3.6% of the total model parameters.\n",
        "\n",
        "Below again is pseudo code highlighting where and how this work. Note running the code will not work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78tDlQ8j_RGD"
      },
      "source": [
        "**Code exercise**: In similar style implement a pseudo-code implementation of the `Adapter` block showed in the diagram above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA48peig_RGD"
      },
      "outputs": [],
      "source": [
        "def transformer_block_with_adapeters(tokens):\n",
        "  \"\"\"\n",
        "  Example of psuedo code of transformer block with adapter layers.\n",
        "  \"\"\"\n",
        "\n",
        "  # finish me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n0ScoMIW_RGD"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "def transformer_block_with_adapeters(tokens):\n",
        "  \"\"\"\n",
        "  Example of psuedo code of transformer block with adapter layers.\n",
        "  \"\"\"\n",
        "\n",
        "  original_tokens = tokens\n",
        "  adapted_tokens = AdapterLayer(tokens) # trainable\n",
        "  x = MHA(adapted_tokens)\n",
        "  x = LayerNorm(x + original_tokens)\n",
        "  original_tokens = x\n",
        "  x = AdapeterLayer(x) # trainable\n",
        "  x = FF(x)\n",
        "  transformed_tokens = LayerNorm(x+original_tokens)\n",
        "  return transformed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5tEZibv_RGD"
      },
      "source": [
        "To see how both worlds from adapters and prefix tuning are together refer to the [LLAMA-Adapter paper](https://arxiv.org/abs/2303.16199)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBc08xY_RGD"
      },
      "source": [
        "### 3.3 LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdj_-UTc_RGD"
      },
      "source": [
        "> This section is a summarized copy of the [LoRA paper]((https://arxiv.org/abs/2106.09685)), read the paper for more details!\n",
        "\n",
        "Finally let's talk about one of the most widely used methods for Efficient FineTuning called LoRA introduced in the paper [\"LoRA: Low-Rank Adaptation of Large Language Models\"](https://arxiv.org/abs/2106.09685) from Edward J. Hu et al.\n",
        "\n",
        "In the LoRA paper using GPT-3 175B they propose to freeze all pre-trained model weights and inject trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.\n",
        "\n",
        "This brings many advantages:\n",
        "-  Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times\n",
        "- Performs on-par or better than fine-tuning in model quality\n",
        "- No inference cost.\n",
        "- Even though LoRA was proposed as a finetunig technique for large-language models it can be applied to any dense layers in deep learning models e.g. [Stable diffusion](https://huggingface.co/blog/lora).\n",
        "\n",
        "\n",
        "**Task**: Prove that $y1$ equal, or not equal, to $y2$. Note, $X$ and $W$ are matrices.\n",
        "\n",
        "> $W = W_1 + W_2$\n",
        ">\n",
        "> $y_1 = WX$\n",
        ">\n",
        "> $y_2 = W_1X + W_2X$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZH6jwc1OdyvU"
      },
      "outputs": [],
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!')\n",
        "%%latex\n",
        "\\begin{aligned}\n",
        "y_1 &= WX \\\\\n",
        "y_1 &= (W_1+W_2)X \\\\\n",
        "y_1 &= W_1X+W_2X \\\\\n",
        "y_1 &= y_2\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CXoBxu7diah"
      },
      "source": [
        "Spoiler alert, yes they are excactly same, and this important characteristic is what the author of the [LoRA](https://arxiv.org/abs/2106.09685), Low Rank Adaption, utilises this to finetune and adapt a LLM to specific downstream tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pF6nC67_RGD"
      },
      "source": [
        "\n",
        "**What are rank decomposition matrices?**\n",
        "\n",
        "<img src=\"https://i.imgur.com/f4TFqMi.png\" width=\"25%\" height=\"25%\">\n",
        "\n",
        "In the diagram above it shows the trainable weights `A` and `B` (in orange) added by the LoRA technique.\n",
        "\n",
        "It consists of two matrics `A` with shape (`d`, `r`) and `B` with shape (`r`, `d`).\n",
        "\n",
        "> Note: in the image above the \"input\" dimension is the same as the \"output\" dimension but in practice these are usually different and set respectively based on the embedding dimension and the attention block dimension.\n",
        "\n",
        "**Question**: How many trainable parameters are introduced by these matrices?\n",
        "\n",
        "**Why this works?**\n",
        "\n",
        "LoRA take inspiration from [Li et al. (2018)](https://arxiv.org/abs/1804.08838) and [Aghajanyan et al. (2020)](https://arxiv.org/abs/2012.13255) which show that the learned\n",
        "over-parametrized models in fact reside on a low intrinsic dimension. I.e. should be possible to extract the main content of a large attention weight with dimension `D` into into a vector with small dimension `d` where `d` <<< `D`.\n",
        "\n",
        " The LoRA authors test the hypothesis that change in weights during model adaptation also has this property, so in theory once could learn this small dimension vector instead of directly updating the large matrix `D`.\n",
        "\n",
        "**Does this come for free?**\n",
        "\n",
        "Even though this method helps substantially to reduc the costs and speed during training, it does come with some additional costs during inference, if one keeps the learned matrices and oringial model seperate. However, it is possible to merge the learned LoRA weights and original model together into one model again to actually have zero extra cost.\n",
        "\n",
        "Why would one consider keeping it seperate? Well if one has many different tasks that one want to fine tune towards, having one large model optimised, with many multiple \"LoRA models\" for each task becomes much easier to maintain and run in production than various large models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri1FGEh6_RGE"
      },
      "source": [
        "#### 3.3.1 LoRA implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EcRQF82_RGE"
      },
      "source": [
        "Now let's implement a simple LoRA module together!\n",
        "\n",
        "Before doing so, there're a couple of parameters / configs associated with LoRA:\n",
        "\n",
        "1. **layers**: Which layers / transformer matrices (Q, K, V) we should apply LoRA to?  \n",
        "2. **lora_rank**: What is the size of the rank to be used.  \n",
        "3. **lora_alpha**: This is used for scaling. This scaling helps to reduce the need to retune hyperparameters when we vary `lora_rank`. When optimizing with Adam, tuning `lora_alpha` is roughly the same as tuning the learning\n",
        "rate if we scale the initialization appropriately.\n",
        "4. **initialization**: As shown in the diagram above we use a random Gaussian initialization for `A` and\n",
        "zero for `B`.  \n",
        "5. **dropout**: as usually applied in Deep Learning models.\n",
        "\n",
        "**Group task**: Can you think of why the initialization proposed in 4. is used?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfA2TGl3_RGE"
      },
      "outputs": [],
      "source": [
        "class Lora(nn.Module):\n",
        "  # Depend on Module we're applying LoRA to.\n",
        "  input_dims: int\n",
        "  output_dims: int\n",
        "\n",
        "  lora_rank: int\n",
        "  lora_alpha: float\n",
        "  lodra_dropout: float\n",
        "\n",
        "  a_init: nn.initializers.Initializer = nn.initializers.normal()\n",
        "  b_init: nn.initializers.Initializer =  nn.initializers.zeros_init()\n",
        "\n",
        "  def setup(self):\n",
        "    self.a_weights = # FINISH ME\n",
        "    self.b_weights = # FINISH ME\n",
        "\n",
        "  def __call__(self, input_array: chex.Array, attn_h: chex.Array, training: bool):\n",
        "    \"\"\"Implements LoRA technique.\n",
        "\n",
        "      Args:\n",
        "        input_array: Shaped[..., input_dims]\n",
        "        attn_h: Shaped[..., output_dims]\n",
        "      Returns:\n",
        "        output_array: Shaped[..., output_dims]\n",
        "    \"\"\"\n",
        "    if training:\n",
        "      # TODO: check if dropout is applied like this in Flax.\n",
        "      self.a_weights = jax.lax.dropout(self.a_weights, self.lodra_dropout)\n",
        "      self.b_weights = jax.lax.dropout(self.b_weights, self.lodra_dropout)\n",
        "\n",
        "    low_rank = jnp.einsum('...i,ij->...j', input_array, self.a_weights)\n",
        "    output = # FINISH ME\n",
        "    return output + attn_h\n",
        "\n",
        "  def weights_for_inference(self, weights: chex.Array):\n",
        "    \"\"\"Return original weights + `LoRA` weights for no added costs during inference.\"\"\"\n",
        "    return jnp.einsum('ij,kj->ik', self.a_weights, self.b_weights) + # FINISH ME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ow3u_-WbhZzq"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "17HzrPnx_RGE"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "class Lora(nn.Module):\n",
        "  # Depend on Module we're applying LoRA to.\n",
        "  input_dims: int\n",
        "  output_dims: int\n",
        "\n",
        "  lora_rank: int\n",
        "  lora_alpha: float\n",
        "  lodra_dropout: float\n",
        "\n",
        "  a_init: nn.initializers.Initializer = nn.initializers.normal()\n",
        "  b_init: nn.initializers.Initializer = nn.initializers.zeros_init()\n",
        "\n",
        "  def setup(self):\n",
        "    self.a_weights = self.param('a_weights', self.a_init, (self.input_dims, self.lora_rank,))\n",
        "    self.b_weights = self.param('b_weights', self.b_init, (self.output_dims, self.lora_rank,))\n",
        "\n",
        "  def __call__(self, input_array: chex.Array, attn_h: chex.Array, training: bool):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        input_array: Shaped[..., input_dims]\n",
        "        attn_h: Shaped[..., output_dims]\n",
        "      Returns:\n",
        "        output_array: Shaped[..., output_dims]\n",
        "    \"\"\"\n",
        "    if training:\n",
        "      # TODO: check if dropout is applied like this in Flax.\n",
        "      self.a_weights = jax.lax.dropout(self.a_weights, self.lodra_dropout)\n",
        "      self.b_weights = jax.lax.dropout(self.b_weights, self.lodra_dropout)\n",
        "\n",
        "    low_rank = jnp.einsum('...i,ij->...j', input_array, self.a_weights)\n",
        "    output = jnp.einsum('...j,...kj->...k', low_rank, self.b_weights)\n",
        "\n",
        "    scaling = self.lora_alpha / self.lora_rank\n",
        "    output = output * scaling\n",
        "    return output + attn_h\n",
        "\n",
        "  def weights_for_inference(self, weights: chex.Array):\n",
        "    return weights + jnp.einsum('ij,kj->ik', self.a_weights, self.b_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpCz5otl_RGE"
      },
      "source": [
        "#### 3.3.3 ðŸ¤— Deep dive into LoRA with Hugging Face! ðŸ¤—"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi3RLBB8_RGE"
      },
      "source": [
        "Even though we have implimented LoRA now, there is a bit more work to do than just what we have to make it match the work in the paper. But luckilly the open source community is moving very fast and LoRA (and most of its variants) have been implimented fully an easy to use interface.\n",
        "\n",
        "To showcase the strenght of this method, we'll finetune `gpt2-medium` to generate song lyrics from the artist of your choice! We will do this by:\n",
        "\n",
        "* Loading a pretrained model using Hugging Face transformers\n",
        "* Gathering the dataset using datasets\n",
        "* Fine tune using LoRA\n",
        "\n",
        "This is based on: https://github.com/22-hours/cabrita/blob/main/notebooks/train_lora.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4GB7Bq5_RGE"
      },
      "source": [
        "In the cells below we print prompts in pink and sample responses in purple like in the example below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wdYPJAO_RGE"
      },
      "outputs": [],
      "source": [
        "print_sample(prompt='My fake prompt', sample=' is awesome!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N-BSs9b_RGE"
      },
      "source": [
        "##### 3.3.3.1 Load Hugging Face model and run sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qRJ86ib_RGE"
      },
      "source": [
        "Below we setup two model options:\n",
        "* gpt2-medium: 355M parameters\n",
        "* gpt-neo-125M: 125M parameters (faster and uses less memory! Try it out if downloading the `gpt-medium` weights take too long).\n",
        "\n",
        "Note: What we do below can even work on models such the Llama 13 billion parameter model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SsGZ-_O_RGE"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2-medium\" # @param [\"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
        "\n",
        "test_prompt = 'What is love?' # @param {type: \"string\"}\n",
        "generator = transformers.pipeline('text-generation', model=model_name)\n",
        "generator(test_prompt, do_sample=True, min_length=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D37K0pPR_RGE"
      },
      "outputs": [],
      "source": [
        "if 'gpt2' in model_name:\n",
        "  tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n",
        "  model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
        "elif model_name == \"EleutherAI/gpt-neo-125M\":\n",
        "  tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "  model = transformers.AutoModelForCausalLM(model_name)\n",
        "else:\n",
        "  raise NotImplementedError\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.to(\"cuda\")\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOwiJhhN_RGF"
      },
      "source": [
        "To make everthin easier, lets generate our own `generator` like function to easier load and sample data as well as display results for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TdMMFCV_RGF"
      },
      "outputs": [],
      "source": [
        "def run_sample(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    seed: int | None = None,\n",
        "    temperature: float = 0.6,\n",
        "    top_p: float = 0.9,\n",
        "    max_new_tokens: int = 64,\n",
        ") -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    input_ids = input_ids.to(model.device)\n",
        "    attention_mask = attention_mask.to(model.device)\n",
        "\n",
        "    generation_config = transformers.GenerationConfig(\n",
        "      do_sample=True,\n",
        "      temperature=temperature,\n",
        "      top_p=top_p,\n",
        "      pad_token_id=tokenizer.pad_token_id,\n",
        "      top_k=0,\n",
        "    )\n",
        "\n",
        "    if seed is not None:\n",
        "      torch.manual_seed(seed)\n",
        "\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "    # We assume a single sample is returned to make things simpler.\n",
        "    assert len(generation_output.sequences) == 1\n",
        "    output_sequence = generation_output.sequences[0]\n",
        "    output_string = tokenizer.decode(output_sequence)\n",
        "    response = output_string.split(prompt)[1].rstrip()\n",
        "    print_sample(prompt, response)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UScpIX4D_RGF"
      },
      "outputs": [],
      "source": [
        "_ = run_sample(model, tokenizer, prompt=\"What is love?\", seed=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK1TvcGr_RGF"
      },
      "source": [
        "Note how this does not follow or resemble a song lyric? This is excactly what we want to change with the model and fine tune it to respond with text in the style of lyrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwZbrcFY_RGF"
      },
      "source": [
        "##### 3.3.3.2 Gathering and processing data (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYRtUgsM_RGF"
      },
      "source": [
        "To train a model, we need to gather the dataset from Hugging Face to load some song lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it329o0D_RGF"
      },
      "outputs": [],
      "source": [
        "dataset_author = \"huggingartists\"\n",
        "artist_name = \"red-hot-chili-peppers\"\n",
        "\n",
        "# List all avalable datasets.\n",
        "all_datasets = {}\n",
        "for ds in huggingface_hub.list_datasets(author=dataset_author):\n",
        "  music_artist = ds.id.replace(f'{dataset_author}/', '')\n",
        "  all_datasets[music_artist] = ds.id\n",
        "\n",
        "dataset_name = all_datasets[artist_name]\n",
        "\n",
        "print(f'choose an artist available in {dataset_author}/ (careful! Some lyrics might contain offensive language)')\n",
        "Dropdown_ = widgets.Dropdown(\n",
        "    options=all_datasets.keys(),\n",
        "    value=artist_name,\n",
        ")\n",
        "output = widgets.Output()\n",
        "\n",
        "\n",
        "def on_change(change):\n",
        "  global dataset_name\n",
        "  global artist_name\n",
        "  artist_name = change[\"new\"]\n",
        "  dataset_name = all_datasets[artist_name]\n",
        "  print(f'`dataset_name` is now {dataset_name}.')\n",
        "\n",
        "Dropdown_.observe(on_change, names='value')\n",
        "display(Dropdown_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S6xcI_R_RGF"
      },
      "outputs": [],
      "source": [
        "formatted_artist_name = artist_name.replace('-', ' ').title()\n",
        "prompt = f'This is a song by {formatted_artist_name}. It goes like this: ' # @param\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1evNephNjPE9"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.load_dataset(dataset_name, split='train')\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "devAUlZF_RGF"
      },
      "source": [
        "To train the model we usually set a fixed `sequence_length` for all inputs. A way to achieve this is by:\n",
        "* Padding inputs shorter than `sequence_length`.\n",
        "* Truncating inputs longer than `sequence_length`.\n",
        "\n",
        "**Think about this**: How do we know which `sequence_length` to set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS-pk8JZ_RGF"
      },
      "outputs": [],
      "source": [
        "max_num_tokens = 256 # @param\n",
        "\n",
        "num_chars, num_tokens = [], []\n",
        "\n",
        "for i, text in enumerate(train_dataset['text']):\n",
        "  text = prompt + text\n",
        "  num_tokens.append(len(tokenizer.tokenize(text)))\n",
        "  num_chars.append(len(text))\n",
        "\n",
        "num_chars = np.array(num_chars)\n",
        "num_tokens = np.array(num_tokens)\n",
        "print('Median #chars', np.median(num_chars))\n",
        "print('Max #chars', np.max(num_chars))\n",
        "print('Median #tokens', np.median(num_tokens))\n",
        "print('Max #tokens', np.max(num_tokens))\n",
        "num_truncations = np.sum(num_tokens > max_num_tokens)\n",
        "num_truncated_tokens = num_tokens - max_num_tokens\n",
        "median_num_truncated_tokens = np.median(\n",
        "    np.where(num_truncated_tokens > 0, num_truncated_tokens, 0),\n",
        ")\n",
        "print(f'Number of examples that will be truncated: {num_truncations} ({num_truncations/len(num_tokens) * 100:.2f} %)')\n",
        "plt.hist(num_truncated_tokens, bins=20)\n",
        "plt.title('#truncated tokens')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srz6WVLi_RGF"
      },
      "source": [
        "**Discussion:** By only truncating the data we're loosing A LOT of relevant text. What we could do to avoid this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuiJEdX0_RGF"
      },
      "source": [
        "Now we need to start preprocessing the data for training, and create a function tokenize our data for the model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpyfuDzO_RGG"
      },
      "outputs": [],
      "source": [
        "# Drop empty lyrics.\n",
        "train_dataset = train_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
        "print('Here is a data example before tokenization')\n",
        "print_sample(prompt, train_dataset[0][\"text\"])\n",
        "# Add prompt\n",
        "train_dataset = train_dataset.map(lambda x: {\"text\": prompt + x[\"text\"]})\n",
        "\n",
        "def tokenize(prompt):\n",
        "  result = tokenizer(\n",
        "      prompt,\n",
        "      truncation=True,\n",
        "      max_length=max_num_tokens,\n",
        "      padding=\"max_length\",\n",
        "  )\n",
        "  return {\n",
        "      \"input_ids\": result[\"input_ids\"],\n",
        "      \"attention_mask\": result[\"attention_mask\"],\n",
        "  }\n",
        "\n",
        "train_dataset = train_dataset.shuffle().map(lambda x: tokenize(x[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUAjpRx3_RGG"
      },
      "source": [
        "##### 3.3.3.3 Finetune a model with LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SUwsbIT_RGG"
      },
      "source": [
        "Below we fine tune our model with LoRA. The first thing to do now is to set our hyperparameters for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s5nPAOjy_RGG"
      },
      "outputs": [],
      "source": [
        "# @title Hyper-parameters\n",
        "MICRO_BATCH_SIZE = 4 # @param\n",
        "BATCH_SIZE = 32 # @param\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "EPOCHS = 20 # @param\n",
        "LEARNING_RATE = 3e-4 # @param\n",
        "CUTOFF_LEN = max_num_tokens\n",
        "LORA_R = 12 # @param\n",
        "LORA_ALPHA = 12 # @param\n",
        "LORA_DROPOUT = 0.2 # @param\n",
        "WARMUP_STEPS = 20 # @param\n",
        "QUERY_USED_DURING_TRAINING = \"Dreams of llamas\" # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USiriyKp_RGG"
      },
      "source": [
        "Now we load our model in using the PEFT library. Remeber, the PEFT library is where all these optimisation and effiecient training methods are created in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvOdyzJj_RGG"
      },
      "outputs": [],
      "source": [
        "peft_config = peft.LoraConfig(\n",
        "    task_type=peft.TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        ")\n",
        "peft_model = peft.get_peft_model(copy.deepcopy(model), peft_config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRvf4Ygr_RGG"
      },
      "source": [
        "With all of the groundwork finally laid out, we can train our model! We are using a bit of hacking to better showcase what is happening. It is not to important to understand everything below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX7xt87v_RGG"
      },
      "outputs": [],
      "source": [
        "class PlotLossCalback(transformers.TrainerCallback):\n",
        "  def on_epoch_end(self, args, state, control, model=None, tokenizer=None, logs=None, **kwargs):\n",
        "    states_history = state.log_history\n",
        "    losses, learning_rates, steps = [], [], []\n",
        "    for curr_state in state.log_history:\n",
        "      if 'loss' not in curr_state:  # Evaluation from `HackyTrainerThatRunsSampleInTheLoop`.\n",
        "        continue\n",
        "      losses.append(curr_state['loss'])\n",
        "      learning_rates.append(curr_state['learning_rate'])\n",
        "      steps.append(curr_state['step'])\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
        "    ax1.plot(steps, losses, '-ob')\n",
        "    ax1.set_title('Steps vs Loss')\n",
        "\n",
        "    ax2.plot(steps, learning_rates, '-or')\n",
        "    ax2.set_title('Step vs Learning Rate')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class HackyTrainerThatRunsSampleInTheLoop(transformers.Trainer):\n",
        "  def prediction_step(\n",
        "      self,\n",
        "      model,\n",
        "      inputs,\n",
        "      prediction_loss_only: bool,\n",
        "      ignore_keys: list[str] | None = None,\n",
        "      # Return: loss, logits, labels\n",
        "  ) -> tuple[torch.Tensor | None, torch.Tensor | None, torch.Tensor | None]:\n",
        "    del inputs, prediction_loss_only, ignore_keys  # unused.\n",
        "    _ = run_sample(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        prompt=prompt + QUERY_USED_DURING_TRAINING,\n",
        "        seed=1,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    return (None, None, None)\n",
        "\n",
        "training_arguments = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,\n",
        "    logging_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=10,\n",
        "    # lr_scheduler_type=\"cosine\",\n",
        "    output_dir=\"tmp\",\n",
        "    # since this is a toy example, let's keep only the last checkpoint to\n",
        "    # speed things up.\n",
        "    save_strategy=\"no\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        ")\n",
        "trainer = HackyTrainerThatRunsSampleInTheLoop(\n",
        "    model=peft_model,\n",
        "    train_dataset=train_dataset,\n",
        "    # Unused. But needed to run hacky inference.\n",
        "    eval_dataset=[{'input_ids': [], 'attention_mask': []}],\n",
        "    args=training_arguments,\n",
        "    callbacks=[PlotLossCalback],\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "peft_model.config.use_cache = False\n",
        "trainer.train(resume_from_checkpoint=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkWj5bxd_RGG"
      },
      "source": [
        "### â°âš¡ Demo Time with our trained modelðŸš€ðŸ˜°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_9Qu2HW_RGG"
      },
      "outputs": [],
      "source": [
        "seed = 3\n",
        "query = \"In the midle of the \"\n",
        "final_prompt = prompt + query\n",
        "temperature = 1.0\n",
        "top_p = 0.9\n",
        "print('LoRA model')\n",
        "_ = run_sample(\n",
        "    peft_model,\n",
        "    tokenizer,\n",
        "    prompt=final_prompt,\n",
        "    seed=seed,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        ")\n",
        "\n",
        "print('Original model')\n",
        "_ = run_sample(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt=final_prompt,\n",
        "    seed=seed,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_zlXaAs_RGG"
      },
      "source": [
        "That is pretty awesome is not it not?\n",
        "\n",
        "As a challenge, play with all the hyperparameters above, as well as model choices, and see if you can beat your friend to create the best lyric generator using your own custom LLM trained with the LoRA technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## **Conclusion**\n",
        "**Summary:**\n",
        "\n",
        "You have now learned all the basics of how a LLM works, all the way from the pure fundemantals to finetuning a GPT architecture with LoRA. These are powerfull tools and very applicable for many tasks, but just like any other deep learning model, they are just models and should be used for the correct problem and data.\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "Follow all the links provided in this practical, as well as reading up on the llama2 and Falcon architectures to see how the latest techniques are utilised.\n",
        "\n",
        "\n",
        "**References:**\n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2023)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "# Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/Cg9aoa7czoZCYqxF7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "o1ndpYE50BpG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
