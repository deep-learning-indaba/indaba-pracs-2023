{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QHyB2I-Bv_LX",
        "_8bykYPvwTVC",
        "K-vaEcA8wlsd",
        "qWanFZBdxKs_",
        "iwrtmw9axZjf",
        "KQJVuMi7yCQd",
        "ey3thWXeyMXy",
        "IxCtwO5uzytJ",
        "kuz3qNbf04EV",
        "wCmcjX721OGf",
        "_1d-Tk1E29Ea"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Les bases mathématiques de l'apprentissage automatique (ou Machine Learning): L'aventure d'un débutant**\n",
        "\n",
        "<!-- I've hosted the image on my own google drive, this embed link is possibly a bit brittle. The image is here: https://drive.google.com/file/d/15S_hS_3Hil_zuJQwWqquOa0RwfXNSBDM/ and the embedding code comes from here\n",
        "https://www.labnol.org/embed/google/drive/\n",
        "-->\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-x2E9J_9vDgyiiiVWaqbF5eVDc2ULDTdHSfx9ggmhnHwBokyFI6M4H5H3wfoOogWtmOPKvB0LfFP_mapLkDFRVPltg5=s2560\"\n",
        "width=\"60%\" />\n",
        "\n",
        "### *Avant de commencer*\n",
        "Utilisez ce lien pour accéder au guide pratique, puis sauvegardez une copie personnelle avant de commencer à travailler.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1SSOmG-qnnuVwn8Mv0gUB_pHR7H-MsCOV?usp=sharing\" target=\"_parent\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "© Deep Learning Indaba 2023. Apache License 2.0.\n",
        "\n",
        "**Auteurs:** Abdel Mfougouon Njupoun, Joseph Romaric Cheuteu Tazopap, Geraud Nangue Tasse\n",
        "\n",
        "**Examinateurs:** Ulrich Mbou Sob, Espoir Murhabazi"
      ],
      "metadata": {
        "id": "24378-ZJMZHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Lorsque l'on se lance dans le monde de l'apprentissage automatique, il est essentiel de comprendre les bases mathématiques fondamentales qui constituent le fondement de ce domaine passionnant. Imaginez ces concepts mathématiques comme les blocs de construction au cœur de l'image ci-dessus. Bien que cette liste ne soit pas exhaustive, elle donne un bon aperçu des concepts essentiels qui sous-tendent les efforts d'apprentissage automatique. Dans ce tutoriel, nous nous concentrerons sur plusieurs fondements essentiels : `algèbre linéaire`, `géométrie analytique`, `décomposition matricielle` et `calcul vectoriel`. Nous n'aborderons pas les probabilités, les distributions et l'optimisation, mais ces notions jouent un rôle essentiel dans les applications les plus avancées des exemples que nous explorerons.\n",
        "\n",
        "Il est important de reconnaître l'interconnexion de ces domaines mathématiques dans le cadre de l'apprentissage automatique. Par exemple, le processus d'optimisation s'entrelace parfaitement avec le calcul vectoriel, une relation évidente lors de l'entraînement des réseaux neuronaux par des techniques telles que la descente de gradient. Le diagramme présente les quatre piliers de l'apprentissage automatique, qui représentent les principales catégories de modèles : `regression`, `classification`, `la réduction de la dimensionnalité` et `l'estimation de la densité`. Bien que vous soyez peut-être familier avec <font color='green'>`Régression`</font> et <font color='orange'>`Réduction de la dimensionnalité`</font>, ces quatre piliers font partie intégrante du paysage plus large de la science des données. Rejoignez-nous pour découvrir l'essence mathématique qui fait avancer l'apprentissage automatique."
      ],
      "metadata": {
        "id": "OQIZq5AsP-7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivation\n",
        "\n",
        "**Pourquoi les mathématiques sous-jacentes sont-elles importantes pour vous ?**\n",
        "Il existe actuellement une pléthore d'outils et de bibliothèques qui font le gros du travail à votre place. Cependant,\n",
        "\n",
        "- Un outil en soi ne vous dit pas\n",
        " - Pourquoi une technique a fonctionné ou non ;\n",
        " - ce que la technique fait réellement ;\n",
        " - Si une technique est susceptible d'être efficace pour votre problème ;\n",
        " - Les hypothèses sous-jacentes que la technique utilise ;\n",
        "- La plupart des approches prêtes à l'emploi ne sont pas à la pointe de la technologie.\n",
        " - Si vous voulez vraiment repousser les limites, vous devez innover, mais sans une réelle compréhension des composants fondamentaux, c'est presque impossible.\n",
        " - L'absence de connaissances fondamentales implique de toujours travailler à partir des premiers principes."
      ],
      "metadata": {
        "id": "lC9bHCJYRwqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organisation de ce tutoriel\n",
        "\n",
        "## Algèbre linéaire I <font color='green'>`Debutant`</font>\n",
        "\n",
        "### Overview\n",
        "Ce cours donne une introduction très basique à l'algèbre linéaire. Nous couvrons les bases des vecteurs et des matrices (ce qu'ils sont et comment les calculer) ainsi que les produits scalaires et les normes. Nous soulignons l'utilisation de ces sujets dans l'apprentissage automatique dans l'exemple de la régression ridge. [Conduisez-moi là-bas!!]\n",
        "(#scrollTo=dTbUZzMpogjx)\n",
        "\n",
        "#### Sections\n",
        "Dans ce parcours, vous couvrirez les sujets suivants:\n",
        "- [ ] Vecteurs\n",
        "- [ ] Notions de base sur les matrices\n",
        "- [ ] Produits scalaires\n",
        "- [ ] Nores\n",
        "- [ ] Exemple: Régression linéaire\n",
        "\n",
        "#### Prérequis\n",
        "Cette séance pratique n'exige pratiquement aucune condition préalable, si ce n'est des connaissances de base en mathématiques (niveau secondaire). Pour l'exemple de la régression linéaire, il sera utile de connaître un peu de calcul vectoriel, mais ce n'est pas nécessaire.\n",
        "\n",
        "> <font color='red'>`Astuce`</font> pour suivre vos progrès, assurez-vous de cocher les cases au fur et à mesure que vous terminez chaque sujet en éditant cette cellule et en changeant la case\n",
        "- [ ] ...\n",
        "- [x] ...\n",
        "\n",
        "## Algèbre linéaire II <font color='orange'>`Intermédiaire`</font>\n",
        "\n",
        "### Overview\n",
        "Cette séance pratique part du principe que vous avez couvert les bases et vous propose des sujets plus avancés. [Conduisez-moi là-bas!!](#scrollTo=dTbUZzMpogjx)\n",
        "\n",
        "#### Sections\n",
        " - [ ] Bases orthonormées et projections orthogonales\n",
        " - [ ] Déterminant, trace\n",
        " - [ ] Vecteurs propres et valeurs propres\n",
        " - [ ] Décomposition en valeurs propres et diagonalisation\n",
        " - [ ] Analyse en composantes principales (ACP)\n",
        "\n",
        "#### Prérequis\n",
        "Soyez confortable avec l'algèbre linéaire I. <font color='green'>`Debutant`</font>.\n",
        "\n",
        "<!-- We will cover the minimal mathematics for\n",
        "- <font color='green'>`Regression`</font>:\n",
        "<font color='green'>`Beginner`</font> `Section 1`\n",
        " - What is a vector, basic vector operations `Linear Algebra`\n",
        " - What is a matrix, basic matrix operations `Linear Algebra`\n",
        " - Norms and Inner products `Analytic Geometry`\n",
        " - Univariate differentiation `vector calculus`\n",
        " - Differentiating a multivariate function `vector calculus`\n",
        " - Taking a gradient and some basic gradient operations `vector calculus`\n",
        " - Linear regression `Example`\n",
        "\n",
        "- <font color='orange'>`Dimensionality Reduction`</font>: <font color='orange'>`Intermediate`</font> `Section 2`\n",
        " - Orthonormal bases and orthogonal projections `Analytic Geometry`\n",
        " - Determinant, trace `Matrix Decomposition`\n",
        " - Eigenvectors and eigenvalues `Matrix Decomposition`\n",
        " - Eigendecomposition and diagonalisation `Matrix Decomposition`\n",
        " - Principal component analysis (PCA) `Example` -->"
      ],
      "metadata": {
        "id": "rBf6SuZbTcEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ressources Additionnelles\n",
        "\n",
        "Le matériel pour cette séance pratique est basé sur le livre \"[Mathematics for Machine Learning](https://mml-book.github.io)\".\n",
        "Nous vous recommandons de consulter le livre pour plus de détails et une exploration approfondie de chaque sujet.\n",
        "Pour les parcours d'Algèbre Linéaire I et Algèbre Linéaire II, consultez les chapitres 2, 3 et 4.\n",
        "Pour les notions de calcul vectoriel, référez-vous au chapitre 5.\n",
        "\n",
        "Tout au long des sections, nous mettrons en évidence des ressources supplémentaires.\n",
        "Souvent, nous recommanderons des vidéos ou des leçons de [3Blue1Brown](https://www.3blue1brown.com), car elles peuvent être utiles pour développer votre intuition.\n",
        "Ces vidéos suivent un ordre différent de nos parcours, il se peut donc que vous rencontriez des concepts que nous n'avons pas encore introduits.\n",
        "Lorsque cela se produit, lisez soit la suite de cette séance pratique, soit consultez la leçon pertinente de 3Blue1Brown [ici](https://www.3blue1brown.com/topics/linear-algebra) (par exemple, les vecteurs de base sont mentionnés dans le Chapitre 2 mais n'apparaissent pas dans l'Algèbre Linéaire I).\n",
        "\n",
        "De manière plus générale, il existe de nombreuses bonnes ressources pour l'algèbre linéaire :\n",
        "\n",
        "- [Cours 3Blue1Brown](https://www.3blue1brown.com/topics/linear-algebra)\n",
        "- Gilbert Strang (célèbre pour l'enseignement de l'Algèbre Linéaire)\n",
        "  - [Vidéos des conférences de Gilbert Strang](https://www.youtube.com/playlist?list=PL49CF3715CB9EF31D)\n",
        "  - [Introduction to Linear Algebra (livre), Gilbert Strang](https://math.mit.edu/~gs/linearalgebra/)\n",
        "- Pour tout ce que vous pourriez vouloir savoir sur les matrices, essayez le livre \"Matrix Analysis\" de Johnson et Horn. (Une excellente référence).\n",
        "- Pour une perspective plus mathématique, vous pouvez utiliser des notes de cours,\n",
        "comme [celles du cursus de mathématiques de l'Université de Cambridge](https://dec41.user.srcf.net/notes). Les cours pertinents sont _Vectors and Matrices_ et _Linear Algebra_.\n",
        "\n",
        "Les vidéos peuvent être utiles pour développer l'intuition, mais si vous souhaitez vraiment comprendre en profondeur, vous devrez également utiliser les cours magistraux ou les livres. En particulier, vous devrez travailler sur des exemples et faire les exercices associés.\n",
        "Comparées aux vidéos, les notes de cours et les livres seront plus autonomes et plus utiles à long terme, mais probablement plus exigeants et offrant moins d'intuition.\n",
        "Comme toujours, vous obtenez ce que vous investissez."
      ],
      "metadata": {
        "id": "P2ZZcPnVXWSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installer et importer les packages requis. (Exécutez-moi)"
      ],
      "metadata": {
        "id": "_JlqHRHKXWYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import anything required. Capture hides the output from the cell.\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import timeit\n",
        "import matplotlib as mpl\n",
        "mpl.use('Agg')\n",
        "plt.style.use('fivethirtyeight')\n",
        "from ipywidgets import interact\n",
        "import sklearn.datasets\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "2TqeEiiUMaIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 1 <font color='green'>Débutant</font>\n",
        "\n",
        "Ce que vous apprendrez dans cette section : Vecteurs, Bases des matrices, Produits internes, Normes, Bases du calcul, Exemple de régression Ridge.\n",
        "\n",
        "<font color='red'>Astuce</font> : Vous pouvez masquer les sections que vous n'étudiez pas actuellement en basculant le bouton de réduction de $\\blacktriangledown$ à $\\blacktriangleright$."
      ],
      "metadata": {
        "id": "DKRPH1EGYrg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vecteurs"
      ],
      "metadata": {
        "id": "MVO7-6WBYpka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les tableaux numériques sont la structure de données fondamentale dans la plupart des codes d'apprentissage automatique.\n",
        "De manière intuitive, un vecteur est un tableau de nombres en colonne\n",
        "qui est accompagné d'opérations algébriques\n",
        "telles que l'addition et la soustraction (avec d'autres tableaux de la même forme)\n",
        "et la multiplication par un scalaire (en multipliant tous les éléments du tableau\n",
        "par le même nombre).\n",
        "\n",
        "Les vecteurs apparaissent très régulièrement à la fois dans l'apprentissage automatique théorique et appliqué.\n",
        "Le sujet de l'algèbre linéaire, qui concerne des collections particulières de vecteurs appelées espaces vectoriels et les transformations entre eux, est fondamental non seulement\n",
        "pour l'apprentissage automatique, mais aussi pour les mathématiques et les sciences physiques.\n",
        "\n",
        "Les vecteurs sont utilisés en apprentissage automatique pour représenter et manipuler des données.\n",
        "Les vecteurs sont couramment utilisés comme ensemble de caractéristiques d'entrée pour un modèle, telles que\n",
        "`(superficie, nombre de salles de bains, âge de la propriété, ...)`\n",
        "dans une tâche de régression des prix des maisons.\n",
        "Les vecteurs sont également utilisés pour contenir des représentations de données, comme dans les modèles de langage modernes ou le classique `Word2Vec`."
      ],
      "metadata": {
        "id": "83fdIu6cZZa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vecteurs en tant que tableaux numériques"
      ],
      "metadata": {
        "id": "JDGWhAI6aUCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La définition la plus simple d'un vecteur est un tableau (1D) de nombres en colonne comme\n",
        "$$\\begin{pmatrix} 1\\\\4\\\\2\\end{pmatrix}\n",
        "\\quad \\text{ ou } \\quad\n",
        "\\begin{pmatrix} 0.5\\\\6\\\\7\\\\4.9\\\\2.1\\end{pmatrix}$$\n",
        "\n",
        "La _longueur_ du vecteur correspond simplement à la longueur du tableau.\n",
        "Ainsi, dans les exemples ci-dessus, les vecteurs ont des longueurs respectives de 3 et 5.\n",
        "(La _dimension_ du vecteur est parfois utilisée pour signifier sa longueur.)\n",
        "Nous écrirons les vecteurs en police grasse, donc\n",
        "$\\mathbf{a}, \\mathbf{b}, \\mathbf{c},$ etc.\n",
        "\n",
        "Les éléments du tableau sont appelés les _composantes_ du vecteur.\n",
        "Nous écrivons $\\mathbf{a}_i$ pour la i<sup>ème</sup> composante du vecteur\n",
        "$\\mathbf{a}$. Ainsi, $\\mathbf{a}_2$ est le nombre en position 2\n",
        "(en partant du haut) dans le tableau. Il s'agit de 4 (dans le premier exemple) et de 6 (dans le deuxième exemple ci-dessus).\n",
        "Par convention, l'indexation commence à partir de 1 en mathématiques, mais à partir de 0 en Python (désolé !)."
      ],
      "metadata": {
        "id": "fNwyl6Ajaj_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Dans cette séance pratique, nous considérerons spécifiquement des vecteurs réels, qui sont des tableaux dont les éléments sont des nombres réels.\n",
        "Deux vecteurs réels de même longueur peuvent être ajoutés ou soustraits l'un de l'autre en ajoutant ou en soustrayant les éléments correspondants, tout comme les tableaux.\n",
        "Par exemple,\n",
        "$$\n",
        "\\begin{pmatrix} 1\\\\4\\\\2\\end{pmatrix} +\n",
        "\\begin{pmatrix} 3\\\\1\\\\6\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix} 4\\\\5\\\\8\\end{pmatrix}\n",
        "$$\n",
        "et\n",
        "$$\n",
        "\\begin{pmatrix} 3\\\\1\\\\6\\end{pmatrix}\n",
        "-\n",
        "\\begin{pmatrix} 1\\\\4\\\\2\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix} 2\\\\-3\\\\4\\end{pmatrix}\n",
        "$$\n",
        "Cependant, les longueurs doivent correspondre. L'expression suivante n'a pas de sens mathématique\n",
        "$$\n",
        "\\begin{pmatrix} 1\\\\3\\\\2\\end{pmatrix} +\n",
        "\\begin{pmatrix} 3\\\\1\\\\6\\\\4\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "De plus, tout vecteur réel peut être multiplié par un nombre réel pour obtenir un autre vecteur réel, où la multiplication se fait élément par élément. Par exemple\n",
        "$$\n",
        "2 \\times \\begin{pmatrix} 1\\\\4\\\\2\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix} 2\\\\8\\\\4\\end{pmatrix}\n",
        "$$\n",
        ">Dans le contexte de l'algèbre linéaire, les nombres sont souvent appelés _scalaires_ et le calcul ci-dessus est appelé _multiplication scalaire_.\n",
        "\n",
        "L'ensemble de tous les vecteurs réels de longueur $n$ est écrit $\\mathbb{R}^{n}$, c'est-à-dire l'ensemble de tous les $n$-uplets de nombres réels.\n",
        "\n",
        "**Note** :\n",
        "À partir de maintenant, sauf indication contraire, le terme _vecteur_ se référera spécifiquement aux vecteurs réels."
      ],
      "metadata": {
        "id": "xwr9qk5dax7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exemples en Code\n",
        "En utilisant JAX, nous pouvons représenter des vecteurs à l'aide de `jnp.array`. Ce qui suit représente le vecteur\n",
        "$$\\mathbf{a} = \\begin{pmatrix} 1\\\\4\\\\2\\end{pmatrix}$$"
      ],
      "metadata": {
        "id": "5w51rVhmbT1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.array([1, 4, 2])\n",
        "a"
      ],
      "metadata": {
        "id": "RR20A-Y-amXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les éléments des vecteurs peuvent être accédés à l'aide de crochets carrés.\n",
        "Rappelez-vous que l'indexation des éléments de vecteurs en mathématiques commence généralement à partir de 1, mais l'indexation des tableaux\n",
        "en Python commence à partir de 0, alors soyez prudent !"
      ],
      "metadata": {
        "id": "QmL_gNhDbpQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first component of a. In math this component would be written a_1\n",
        "print(a[0])"
      ],
      "metadata": {
        "id": "ioVcDDqQbp8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leur dimension peut être calculée à l'aide de `len` ou de `.shape`"
      ],
      "metadata": {
        "id": "BLlsiGixb2yC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'addition, la soustraction et la multiplication par un scalaire peuvent toutes être effectuées à l'aide des opérations standard de Python."
      ],
      "metadata": {
        "id": "XuyN6n_ycUlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.array([1, 4, 2])\n",
        "b = jnp.array([3, 1, 6])\n",
        "a + b"
      ],
      "metadata": {
        "id": "pZG2Xje9b3mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2 * a"
      ],
      "metadata": {
        "id": "rx4_xvA_ccRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b - a"
      ],
      "metadata": {
        "id": "oO-WXTkqciK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Si les longueurs ne correspondent pas, JAX générera une erreur, mais cela touche un sujet informatique appelé _broadcasting_ qui dépasse le cadre de cette séance pratique."
      ],
      "metadata": {
        "id": "iUQjJiMJcZat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercice\n",
        "\n",
        "Soit\n",
        "$$\n",
        "\\mathbf{a} = \\begin{pmatrix} 1\\\\1\\\\0.9\\\\0.6\\end{pmatrix}\n",
        "\\quad\\text{ et }\\quad\n",
        "\\mathbf{b}\n",
        "=\n",
        "\\begin{pmatrix} 0\\\\1\\\\3\\\\ 2.718\\end{pmatrix}\n",
        "$$\n",
        "Utilisez JAX pour calculer $3\\mathbf{a} - \\mathbf{b}$"
      ],
      "metadata": {
        "id": "CTu7-nSJc8vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placez votre réponse ici"
      ],
      "metadata": {
        "id": "QklSUvyzcwEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "mznekPKbzAxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.array([1., 1., 0.9, 0.6])\n",
        "b = jnp.array([0., 1., 3., 2.718])\n",
        "\n",
        "resultat = 3 * a - b\n",
        "resultat"
      ],
      "metadata": {
        "id": "3AF5Tbm9zJ74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Définition Mathématique: Espace vectoriel"
      ],
      "metadata": {
        "id": "Bw7x9c_Pdylp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathématiquement, un vecteur est défini comme tout élément d'un _espace vectoriel_.\n",
        "Un espace vectoriel (réel) $V$ est un ensemble d'objets avec deux opérations : l'addition vectorielle et la multiplication par un scalaire.\n",
        "\n",
        "L'addition vectorielle doit satisfaire, pour tous $\\mathbf{a}, \\mathbf{b}, \\mathbf{c} \\in V$ :\n",
        "- Loi de composition interne : $\\mathbf{a} + \\mathbf{b} \\in V$\n",
        "- commutativité : $\\mathbf{a} + \\mathbf{b} = \\mathbf{b} + \\mathbf{a}$\n",
        "- associativité : $\\mathbf{a} + (\\mathbf{b} + \\mathbf{c}) = (\\mathbf{a} + \\mathbf{b}) + \\mathbf{c}$\n",
        "- élément neutre : Il existe un vecteur nul $\\mathbf{0}$ tel que pour tout $\\mathbf{a}\\in V$,\n",
        "$\\mathbf{a} + \\mathbf{0} = \\mathbf{a}$.\n",
        "- inverse : Pour tout $\\mathbf{a}\\in V$, il existe un vecteur $\\mathbf{-a} \\in V$\n",
        "tel que $\\mathbf{a} + \\mathbf{-a} = \\mathbf{0}$.\n",
        "\n",
        "La multiplication par un scalaire doit satisfaire, pour tous $\\mathbf{a}, \\mathbf{b} \\in V$\n",
        "et pour tous $\\lambda, \\mu \\in \\mathbb{R}$ :\n",
        "- $\\lambda(\\mathbf{a}+\\mathbf{b}) = \\lambda \\mathbf{a} + \\lambda \\mathbf{b}$\n",
        "- $(\\lambda+ \\mu)\\mathbf{a} = \\lambda \\mathbf{a} + \\mu \\mathbf{a}$\n",
        "- $\\lambda(\\mu \\mathbf{a}) = (\\lambda \\mu)\\mathbf{a}$\n",
        "- $1 \\mathbf{a} = \\mathbf{a}$"
      ],
      "metadata": {
        "id": "8HtBDOxFd0fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemples d'Espaces Vectoriels"
      ],
      "metadata": {
        "id": "iiJisJNteq3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- L'ensemble de tous les $n$-uplets de nombres réels $\\mathbb{R}^n$ avec une addition élément par élément et une multiplication par des scalaires réels.\n",
        "- L'ensemble de tous les polynômes de degré $n$ avec des coefficients réels\n",
        "$$f(x) = a_0 + a_1 x + a_2 x^2 + \\cdots + a_n x^n$$\n",
        "est un espace vectoriel. Le polynôme $f$ peut être représenté par\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "a_0\\\\\n",
        "a_1\\\\\n",
        "a_2\\\\\n",
        "\\vdots\\\\\n",
        "a_n\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "- L'ensemble de toutes les fonctions $f : \\mathcal{X} \\rightarrow \\mathbb{R}$ pour n'importe quel ensemble fini $\\mathcal{X}$."
      ],
      "metadata": {
        "id": "GdIysjvNet8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations de l'Intuition des Tableaux\n",
        "_Cette section est facultative. Si elle est confuse, ignorez-la.\n",
        "Elle est présente uniquement pour vous informer que,\n",
        "bien que la compréhension intuitive des vecteurs que nous utilisons soit utile,\n",
        "elle a ses limites._"
      ],
      "metadata": {
        "id": "XLy6D30ze_SJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La définition d'un vecteur comme un tableau de nombres en colonne est suffisante pour la plupart de l'apprentissage automatique moderne.\n",
        "L'intuition géométrique en tant que flèche depuis l'origine est également utile.\n",
        "En effet, ces notions sont probablement ce que beaucoup de scientifiques ont à l'esprit lorsqu'ils pensent au mot « vecteur ».\n",
        "\n",
        ">Cependant, la définition mathématique d'un vecteur permet d'avoir\n",
        "des vecteurs qui _ne peuvent pas_ être écrits comme des tableaux de nombres en colonne.\n",
        "Un espace vectoriel dans lequel chaque vecteur peut être représenté comme un tableau de nombres en colonne est appelé un _espace vectoriel de dimension finie_.\n",
        "Un espace vectoriel où cela n'est pas possible est appelé un\n",
        "_espace vectoriel de dimension infinie_.\n",
        "\n",
        "Nous n'aborderons pas les espaces vectoriels de dimension infinie dans cette séance pratique car ils nécessitent\n",
        "des mathématiques plus avancées, mais ils sont d'une importance fondamentale en mathématiques, en physique et même en apprentissage automatique.\n",
        "En particulier, une compréhension adéquate des méthodes de noyau nécessite\n",
        "de rencontrer ces vecteurs. Un exemple d'un espace vectoriel de dimension infinie\n",
        "est l'ensemble de toutes les fonctions\n",
        "$f : \\mathcal{X} \\rightarrow \\mathbb{R}$ pour un ensemble infini $\\mathcal{X}$.\n",
        "\n",
        "Mis à part l'exemple que nous venons de donner, tous les espaces vectoriels\n",
        "dans cette séance pratique seront de dimension finie."
      ],
      "metadata": {
        "id": "-zXXOiVCfCLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercices\n",
        "\n",
        "_Ces exercices se concentrent sur la compréhension de la définition mathématique d'un espace vectoriel. Ils vous aideront à mieux comprendre à long terme, mais n'hésitez pas à les ignorer pour le moment si vous le souhaitez._"
      ],
      "metadata": {
        "id": "k_JgXruwfc6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Convainquez-vous que l'intuition computationnelle des vecteurs en tant qu'ensembles de nombres réels est un espace vectoriel en utilisant la définition ci-dessus.\n",
        "   - Indication : vérifiez que les règles que nous avons données pour l'addition, la soustraction et la multiplication scalaire des ensembles satisfont toutes les règles de la définition d'un espace vectoriel.\n",
        "\n",
        "2. Vérifiez que l'ensemble de tous les polynômes de degré $n$ avec des coefficients réels est un espace vectoriel.\n",
        "   - Indication : Montrez que la représentation d'un polynôme sous forme d'ensemble donnée dans les exemples ci-dessus est une correspondance bijective, c'est-à-dire que chaque polynôme de degré $n$ a exactement une représentation sous forme d'ensemble et que chaque représentation sous forme d'ensemble correspond à un polynôme. Ensuite, appliquez l'exercice 1.\n",
        "\n",
        "3. Démontrez que $0 \\mathbf{a} = \\mathbf{0}$ pour n'importe quel vecteur $\\mathbf{a}$.\n",
        "\n",
        "4. Utilisez les règles de l'espace vectoriel pour prouver que $(-1) \\mathbf{a} = \\mathbf{-a}$.\n",
        "   - Indication : Prouvez que la multiplication de $\\mathbf{a}$ par le scalaire $-1$ donne le vecteur $\\mathbf{b}$ tel que $\\mathbf{a} + \\mathbf{b} = 0$ et montrez qu'il n'existe qu'un tel $\\mathbf{b}$.\n",
        "\n",
        "5. Utilisez les axiomes ci-dessus pour prouver que $\\lambda \\mathbf{0} = \\mathbf{0}$ pour tous les\n",
        "$\\lambda \\in \\mathbf{R}$."
      ],
      "metadata": {
        "id": "qGyED7RXfeXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "sjvkvln70IwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Nous avons\n",
        "$$\n",
        "0\\mathbf{a} + 0\\mathbf{a} = (0+0) \\mathbf{a} = 0\\mathbf{a}\n",
        "$$\n",
        "donc en soustrayant $0\\mathbf{a}$ des deux côtés, nous obtenons\n",
        "$$\n",
        "0\\mathbf{a} = \\mathbf{0}\n",
        "$$\n",
        "\n",
        "4. Aux fins de cet exercice, nous dirons que tout vecteur $\\mathbf{b}$ qui satisfait\n",
        "$$\n",
        "\\mathbf{a} + \\mathbf{b} = 0\n",
        "$$\n",
        "est un _inverse de $\\mathbf{a}$_. Nous montrerons d'abord que les inverses sont uniques. Supposons que $\\mathbf{c}$ soit un autre inverse de $\\mathbf{a}$, de sorte que\n",
        "$$\n",
        "\\mathbf{a} + \\mathbf{c} = 0\n",
        "$$\n",
        "En soustrayant les équations, nous obtenons\n",
        "$$\n",
        "\\mathbf{b} - \\mathbf{c} = 0\n",
        "$$\n",
        "donc $\\mathbf{b} = \\mathbf{c}$ et les inverses sont uniques. Maintenant, nous montrerons que $(-1)\\mathbf{a}$ est un inverse de $\\mathbf{a}$, et l'unicité des inverses impliquera que $(-1) \\mathbf{a}= \\mathbf{-a}$.\n",
        "Cela se fait en:\n",
        "$$\n",
        "\\mathbf{a} + (-1) \\times \\mathbf{a} = (1 + (- 1))\\mathbf{a} = \\mathbf{0}\n",
        "$$\n",
        "et la preuve est terminée.\n",
        "\n",
        "5. Cela ressemble à l'exercice 3. Nous avons\n",
        "$$\n",
        "\\lambda \\mathbf{0}\n",
        "=\\lambda (\\mathbf{0} + \\mathbf{0}) = \\lambda \\mathbf{0} + \\lambda \\mathbf{0}\n",
        "$$\n",
        "puis soustrayez $\\lambda \\mathbf{0}$ des deux côtés."
      ],
      "metadata": {
        "id": "n0ALF_ur0Lmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ressources Recommandées"
      ],
      "metadata": {
        "id": "xszw31oIf6E7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Livre Mathematics for Machine Learning](https://mml-book.github.io/book/mml-book.pdf) Le chapitre 1 offre une excellente introduction à ces perspectives également.\n",
        "- [Leçon 3Blue1Brown sur les Vecteurs](https://www.3blue1brown.com/lessons/vectors) est utile pour développer l'intuition.\n",
        "- [Vidéo 3Blue1Brown sur les Espaces Vectoriels Abstraits](https://www.3blue1brown.com/lessons/abstract-vector-spaces) pour une perspective plus mathématique de l'intuition.\n",
        "- Chapitre 2 du cours de Cambridge [Vectors and Matrices](https://dec41.user.srcf.net/notes/IA_M/vectors_and_matrices.pdf) pour des lectures plus avancées sur les bases.\n",
        "- Chapitre 1 du cours de Cambridge [Linear Algebra](https://dec41.user.srcf.net/notes/IB_M/linear_algebra.pdf) pour une perspective plus mathématique."
      ],
      "metadata": {
        "id": "E_FaybwBf8vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fondamentaux des Matrices"
      ],
      "metadata": {
        "id": "3vKdLDKqgJB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Les Matrices en tant que Tableaux Informatiques"
      ],
      "metadata": {
        "id": "Upnm9XYRgUju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une matrice réelle $m\\times n$ est simplement un tableau $(m\\times n)$-dimensionnel de nombres réels\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "  a_{11} & a_{12} & \\dots \\\\\n",
        "  \\vdots & \\ddots & \\\\\n",
        "  a_{m1} &        & a_{mn}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "L'ensemble de toutes les matrices réelles $m\\times n$ est souvent écrit $\\mathbb{R}^{m\\times n}$ ou $\\text{Mat}_{m\\times n}(\\mathbb{R})$. Pour le reste de cette séance pratique, nous dirons simplement \"matrice\" au lieu de \"matrice réelle\".\n",
        "Elle est appelée une matrice $m\\times n$ car le tableau a $m$ lignes et $n$ colonnes.\n",
        "\n",
        "> <font color='grey'>`Note`</font> Dans certains manuels, cela est appelé une matrice $n\\times m$ ce qui signifie qu'elle a $n$ lignes et $m$ colonnes. Cela peut parfois être déroutant, en particulier pour les <font color='green'>`débutants`</font>. Il est donc important de comprendre ce que le symbole représente exactement dans le contexte donné en consultant la définition fournie par l'auteur. Bien, revenons au tutoriel.\n",
        "\n",
        "Nous appelons le nombre $a_{ij}$ _le composant $(i, j)$ de la matrice_. La matrice ci-dessus a des _composants_ $a_{ij}$ pour $i=1, \\dots, m$ et $j=1, \\dots, n$.\n",
        "Nous écrivons les matrices en majuscules en gras,\n",
        "par exemple\n",
        "$$\n",
        "\\mathbf{A}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "  1 & 3 \\\\\n",
        "  2 & 4 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Les composants de n'importe quelle matrice $\\mathbf{A}$ sont souvent écrits $\\mathbf{A}_{ij}$.\n",
        "\n",
        "Une tranche horizontale complète (-) d'entrées d'une matrice est appelée une _ligne_, tandis qu'une tranche verticale complète (|) est appelée une _colonne_.\n",
        "Nous appelons une matrice de dimension $(1\\times n)$ un _vecteur ligne_ et\n",
        "une matrice de dimension $(n\\times 1)$ un _vecteur colonne_.\n",
        "Une matrice de dimension $(n\\times n)$ est une _matrice carrée_ (car le tableau de nombres forme un carré). Par exemple, la matrice ($\\mathbf{A}$) ci-dessus a 2 lignes et 2 colonnes. Sa première ligne a des composants $[1, 3]$, et sa deuxième colonne a des composants $[3, 4]$.\n",
        "\n",
        "Les matrices sont une méthode très courante de représentation des données en apprentissage automatique et en statistiques.\n",
        "Très souvent, on doit représenter les caractéristiques de chaque exemple d'entraînement\n",
        "par un vecteur.\n",
        "Supposons que nous ayons $m$ exemples, chacun ayant $n$ caractéristiques, donc pour chaque exemple, nous avons un vecteur de longueur $n$.\n",
        "Ces vecteurs peuvent être empilés en lignes dans une matrice $m\\times n$ appelée\n",
        "_matrice de conception_. Nous verrons cela plus tard dans l'exemple de la régression linéaire.\n",
        "\n",
        "Les matrices peuvent être additionnées et soustraites élément par élément, tout comme les vecteurs, tant que les matrices ont les mêmes dimensions\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "  1 & 3 \\\\\n",
        "  2 & 4 \\\\\n",
        "  6 & 8 \\\\\n",
        "\\end{pmatrix}\n",
        "+\n",
        "\\begin{pmatrix}\n",
        "  8 & -2 \\\\\n",
        "  2 & 0.5 \\\\\n",
        "  -3 & 0 \\\\\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "  9 & 1 \\\\\n",
        "  4 & 4.5 \\\\\n",
        "  3 & 8 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "et elles peuvent également être multipliées par des scalaires réels\n",
        "$$\n",
        "3 \\times \\begin{pmatrix}\n",
        "  1 & 3 \\\\\n",
        "  2 & 4 \\\\\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "  3 & 9 \\\\\n",
        "  6 & 12 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "En composantes, si $\\mathbf{A}$ et $\\mathbf{B}$ sont toutes deux des matrices $m\\times n$\n",
        "alors $\\mathbf{A}+\\mathbf{B}$ est une matrice $m\\times n$ avec\n",
        "$$\n",
        "(\\mathbf{A}+\\mathbf{B})_{ij}\n",
        "=\n",
        "\\mathbf{A}_{ij} +\\mathbf{B}_{ij}\n",
        "$$\n",
        "et pour tout scalaire $\\lambda \\in \\mathbb{R}$, $\\lambda \\mathbf{A}$ est une matrice\n",
        "$m\\times n$ avec des composants\n",
        "$$\n",
        "(\\lambda \\mathbf{A})_{ij}\n",
        "= \\lambda \\times \\mathbf{A}_{ij}\n",
        "$$"
      ],
      "metadata": {
        "id": "F28l_QWegWhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Les Matrices dans Jax"
      ],
      "metadata": {
        "id": "nH9JUixBg1jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les matrices peuvent être représentées à l'aide d'arrays `jax`, tout comme les vecteurs."
      ],
      "metadata": {
        "id": "T1GngrOSrsGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.array([[1, 3], [2, 4], [6, 8]])"
      ],
      "metadata": {
        "id": "qQmJ-nSgddAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice"
      ],
      "metadata": {
        "id": "QZx-89ULr6qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Soit\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "  1 & 3 \\\\\n",
        "  2 & 4 \\\\\n",
        "\\end{pmatrix}\n",
        "\\quad\\text{ et }\\quad\n",
        "\\mathbf{B} =\n",
        "\\begin{pmatrix}\n",
        "  8 & -2 \\\\\n",
        "  2 & 0.5 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Utilisez `jax` pour calculer $2\\mathbf{A} - \\mathbf{B}$."
      ],
      "metadata": {
        "id": "JhVy0PIYsLO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Place your answer here"
      ],
      "metadata": {
        "id": "S6Y9cTjfsAQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- (Optionnel, plus mathématique) : Montrez que l'ensemble de toutes les matrices réelles $m\\times n$ est un espace vectoriel selon la définition mathématique.\n",
        "\n",
        "> Bloc indenté"
      ],
      "metadata": {
        "id": "hsArx9SzsSCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplication des matrices"
      ],
      "metadata": {
        "id": "7EHNYaf0shx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Multiplication Matrice-Matrice"
      ],
      "metadata": {
        "id": "QOkNukz1uX6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La multiplication des matrices est définie comme suit.\n",
        "Soit $\\mathbf{A}$ une matrice $m\\times k$ et $\\mathbf{B}$ une matrice $k\\times n$, la multiplication de $\\mathbf{B}$ par $\\mathbf{A}$ par la gauche produit\n",
        "une matrice $\\mathbf{C}$ de dimensions $m\\times n$\n",
        "$$\n",
        "\\mathbf{C} = \\mathbf{A}\\mathbf{B}\n",
        "$$\n",
        "qui a des composants\n",
        "$$\n",
        "\\mathbf{C}_{ij} = \\sum_{l=1}^k \\mathbf{A}_{il}\\mathbf{B}_{lj}\n",
        "$$\n",
        "\n",
        "Voici un exemple\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "0 & 1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 & 7 \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "1 & 1 & 1  \\\\\n",
        "0 & 1 & 1  \\\\\n",
        "0 & 0 & 1  \\\\\n",
        "1 & 0 & 1  \\\\\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "(0\\times1) + (1\\times0) + (2\\times0) + (3\\times1) & ... & ... \\\\\n",
        "(4\\times1) + (5\\times0) + (6\\times0) + (7\\times1) & ... & ...\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "3 & 1 & 6 \\\\\n",
        "11 & 9 & 22\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "La multiplication n'est définie que lorsque les dimensions adjacentes des deux matrices correspondent.\n",
        "Le cas ci-dessus est valide car la deuxième dimension de $\\mathbf{A}$ est $k$, tout comme la première dimension de $\\mathbf{B}$.\n",
        "\n",
        ">Même si nous pouvons multiplier les matrices $\\mathbf{A}\\mathbf{B}$, le produit $\\mathbf{B}\\mathbf{A}$ _n'est pas défini (c'est-à-dire impossible) sauf si $m=n$_. Plus de détails à ce sujet dans la sous-section \"Matrices Commutatives\" ci-dessous.\n",
        "\n",
        "Notez également que la dimension de la matrice résultante n'est pas nécessairement la même que la dimension de l'un des arguments.\n",
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-zPXr2BVQaeIMGXkD9QURdyLS8BCDsP6Yx4L0KgiRh05tKsiOTJ6DmDmDhZiZnDJS0KlCHKTQ0mfUM7Mn88RdILeFIMDw=s2560\"\n",
        "width=\"30%\"  class=\"center\"/>\n",
        "\n",
        "Les images suivantes peuvent vous être utiles pour la visualisation.\n",
        "Si vous n'avez jamais vu la multiplication de matrices auparavant, cela deviendra rapidement\n",
        "facile une fois que vous aurez pratiqué quelques exemples.\n",
        "\n",
        "<img src=\"\n",
        "https://lh3.googleusercontent.com/drive-viewer/AITFw-ykpJ8ShB0EfJeuzB9xqZsmzSEAZUbPbAFfTvcctojVcRgssi1wYHMvjLL2qyMBU_l2qPtxs3XMQEEf6C312kVgX2Hd=s2560\"\n",
        "width=\"50%\"  class=\"center\"/>\n",
        "\n",
        "<img src=\n",
        "\"https://assets.tivadardanka.com/2022_matrix_multiplication_def_01_1b4c6d7211.png\n",
        "\"\n",
        "width=\"75%\"  class=\"center\"/>"
      ],
      "metadata": {
        "id": "EmoaXNABsmBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Multiplication de matrices dans `jax`\n",
        "\n",
        "La multiplication de matrices peut être calculée en utilisant `jnp.matmul` ou l'opérateur binaire `@`."
      ],
      "metadata": {
        "id": "JMJ7pJmztAzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[1, 2], [3, 4]])\n",
        "B = jnp.array([[5, 6], [7, 8]])\n",
        "jnp.matmul(A, B)"
      ],
      "metadata": {
        "id": "lyb9U9vpsbmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94864051-1b79-45ba-c7ae-f70ba3b75c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[19, 22],\n",
              "       [43, 50]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A @ B"
      ],
      "metadata": {
        "id": "vDAbxb_4tTFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8180a95e-2e65-4be4-f4f5-1aaf00893e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[19, 22],\n",
              "       [43, 50]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Propriétés Algébriques"
      ],
      "metadata": {
        "id": "mExN_Us0tPOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "\n",
        "Le symbole `@` a une autre utilisation (complètement différente !) en Python, qui concerne les _décorateurs_.\n",
        "Ne vous en préoccupez pas dans cette pratique, mais il est bon de le savoir pour éviter toute confusion."
      ],
      "metadata": {
        "id": "cUx5xwOVteGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La multiplication de matrices est toujours _associative_. Cela signifie que, tant que les dimensions sont appropriées,\n",
        "$$\n",
        "(\\mathbf{AB})\\mathbf{C}\n",
        "=\n",
        "\\mathbf{A}(\\mathbf{BC})\n",
        "$$\n",
        "Elle est également _distributive_, ce qui signifie que (encore une fois, en supposant que les dimensions sont appropriées)\n",
        "$$\n",
        "\\mathbf{A}(\\mathbf{B} +\\mathbf{C})\n",
        "=\n",
        "\\mathbf{A}\\mathbf{B}\n",
        "+\n",
        "\\mathbf{A}\\mathbf{C}\n",
        "$$\n",
        "et\n",
        "$$\n",
        "(\\mathbf{A} + \\mathbf{B})\\mathbf{C}\n",
        "=\n",
        "\\mathbf{A}\\mathbf{C}\n",
        "+\n",
        "\\mathbf{B}\\mathbf{C}\n",
        "$$\n",
        "\n",
        "Cependant, elle **n'est pas** toujours _commutative_, ce qui signifie que $\\mathbf{AB}\\ne\\mathbf{BA}$ en général, même si les deux expressions sont bien définies. Plus à ce sujet plus tard."
      ],
      "metadata": {
        "id": "5OmnaS7vtpWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercices"
      ],
      "metadata": {
        "id": "UOy95TB0t0EE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Soit\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "1 & 2 & 3\\\\\n",
        "3 & 2 & 1\n",
        "\\end{pmatrix}\n",
        "\\quad \\text{ et } \\quad\n",
        "\\mathbf{B} =\n",
        "\\begin{pmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 2 \\\\\n",
        "4 & 5\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "  - $\\mathbf{AB}$ est-il défini ? Si oui, calculez-le. Sinon, expliquez pourquoi.\n",
        "  - $\\mathbf{BA}$ est-il défini ? Si oui, calculez-le. Sinon, expliquez pourquoi.\n",
        "\n",
        "- Soit\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "1 & 2\\\\\n",
        "3 & 2\n",
        "\\end{pmatrix}\n",
        "\\quad \\text{ et } \\quad\n",
        "\\mathbf{B} =\n",
        "\\begin{pmatrix}\n",
        "0 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "  - $\\mathbf{AB}$ est-il défini ? Si oui, calculez-le. Sinon, expliquez pourquoi.\n",
        "  - $\\mathbf{BA}$ est-il défini ? Si oui, calculez-le. Sinon, expliquez pourquoi.\n",
        "\n",
        "- Utilisez `jax` pour vérifier les calculs que vous avez effectués."
      ],
      "metadata": {
        "id": "tdqPS9BDt7b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placez votre code ici"
      ],
      "metadata": {
        "id": "Oy-UFX5ctdTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiplication Matrice-Vecteur\n",
        "\n",
        "Vous avez peut-être remarqué que les vecteurs que nous avons définis précédemment comme des tableaux de nombres verticaux sont simplement un cas particulier des matrices. En particulier, un vecteur de longueur $n$ est simplement une matrice $n\\times 1$.\n",
        "\n",
        "Cela signifie que nous avons automatiquement une formule pour multiplier des vecteurs par des matrices.\n",
        "Encore une fois, il est important que les dimensions voisines correspondent.\n",
        "\n",
        "Soit $\\mathbf{v}$ un vecteur de longueur $n$ et $\\mathbf{A}$ une matrice $m\\times n$.\n",
        "En multipliant $\\mathbf{v}$ par $\\mathbf{A}$, on obtient un vecteur de longueur $m$\n",
        "$$\n",
        "\\mathbf{u} = \\mathbf{Av}\n",
        "$$\n",
        "où $\\mathbf{u}$ a des composantes\n",
        "$$\n",
        "\\mathbf{u}_i = \\sum_{j=1}^n\\mathbf{A}_{ij}\\mathbf{v}_j\n",
        "$$\n",
        "Cette opération n'est valide que lorsque la deuxième dimension de $\\mathbf{A}$ (dans ce cas $n$) est égale à la longueur du vecteur.\n",
        "En d'autres termes, le nombre de colonnes dans la matrice $\\mathbf{A}$ doit correspondre au nombre de lignes dans le vecteur $\\mathbf{v}$."
      ],
      "metadata": {
        "id": "LO0AoyeduM1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Matrice-Vecteur en Jax\n",
        "\n",
        "La multiplication matrice-vecteur est réalisée de la même manière en utilisant `jax` que la multiplication matrice-matrice."
      ],
      "metadata": {
        "id": "iPABYV_0u7rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
        "b = jnp.array([1, 2, 3])\n",
        "jnp.matmul(A, b)"
      ],
      "metadata": {
        "id": "P0muJwRGuiGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caab4e2c-8aa4-4aa4-963b-1e6a33775d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([14, 32], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A @ b"
      ],
      "metadata": {
        "id": "dbdBVWYHvXx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a94ba1-d08c-4756-dcea-2e22d9b7e100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([14, 32], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercice"
      ],
      "metadata": {
        "id": "MgO4gz6EvVEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour quelles combinaisons de matrice-vecteur parmi les suivantes l'expression\n",
        "$\n",
        "\\mathbf{Ab}\n",
        "$\n",
        "est-elle valide ? Si ce n'est pas le cas, pourquoi ? Si c'est le cas, calculez le résultat et utilisez `jax` pour vérifier votre réponse.\n",
        "- $$\n",
        "\\mathbf{A}\n",
        "= \\begin{pmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4 \\\\\n",
        "5 & 6\n",
        "\\end{pmatrix}\n",
        "\\quad\\text{ et }\\quad\n",
        "\\mathbf{b}\n",
        "= \\begin{pmatrix}\n",
        "9\\\\8\\\\7\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "- $$\n",
        "\\mathbf{A}\n",
        "= \\begin{pmatrix}\n",
        "1 & 0 & 1\\\\\n",
        "0 & 1 & 0 \\\\\n",
        "1 & 0 & 1\\\\\n",
        "\\end{pmatrix}\n",
        "\\quad\\text{ et }\\quad\n",
        "\\mathbf{b}\n",
        "= \\begin{pmatrix}\n",
        "3\\\\ 50 \\\\6\n",
        "\\end{pmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "OVjtC__Uveuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placez votre code ici"
      ],
      "metadata": {
        "id": "tDa_B_Xo1gQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiplication Élément par Élément de Matrices\n",
        "\n",
        "Une opération alternative de multiplication entre matrices\n",
        "(ou tableaux en général)\n",
        "est la _multiplication élément par élément_. Ne la confondez pas avec la multiplication de matrices !\n",
        "\n",
        "La multiplication élément par élément de matrices est très simple. Prenez deux matrices de mêmes dimensions et créez une nouvelle matrice en multipliant les éléments correspondants.\n",
        "Si $\\mathbf{A}$ et $\\mathbf{B}$ sont deux matrices $m\\times n$\n",
        "alors le produit élément par élément est défini par\n",
        "$$\n",
        "\\mathbf{C} = \\mathbf{A} \\odot \\mathbf{B}\n",
        "$$\n",
        "où $\\mathbf{C}$ est une matrice $m\\times n$ avec des composantes\n",
        "$$\n",
        "\\mathbf{C}_{ij} = \\mathbf{A}_{ij}\\mathbf{B}_{ij}\n",
        "$$\n",
        "\n",
        "Notez que, contrairement à la multiplication de matrices, la multiplication élément par élément\n",
        "nécessite que _les deux_ dimensions des matrices correspondent, pas seulement les dimensions voisines.\n",
        "\n",
        "La multiplication élément par élément est utilisée en apprentissage automatique,\n",
        "comme lors de l'application d'un masque sur les poids d'un réseau neuronal\n",
        "dans [l'article sur l'hypothèse du ticket de loterie](https://arxiv.org/abs/1803.03635),\n",
        "mais elle est généralement moins courante dans les sciences que la multiplication de matrices.\n",
        "Nous ne la mentionnons ici que pour souligner la différence par rapport à la multiplication de matrices."
      ],
      "metadata": {
        "id": "7upo9ulyv0yO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### La multiplication élément par élément de tableaux peut être réalisée en python en utilisant l'opérateur `*`.\n",
        "\n",
        "Par exemple,"
      ],
      "metadata": {
        "id": "QHyB2I-Bv_LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[1,  2], [3, 4]])\n",
        "B = 2 * jnp.ones((2, 2)) # look this function up if you don't know it!\n",
        "B"
      ],
      "metadata": {
        "id": "c_0uVg8xvdt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4c8791-ad55-47ba-e8fb-f18a18e073e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[2., 2.],\n",
              "       [2., 2.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A * B"
      ],
      "metadata": {
        "id": "kOB7zyuGwGcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bddb773-a429-4acd-94ee-10df7c2796d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[2., 4.],\n",
              "       [6., 8.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notez que ceci est différent de la multiplication matricielle."
      ],
      "metadata": {
        "id": "sO-86C-zwD8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.matmul(A, B)"
      ],
      "metadata": {
        "id": "kR9hFG1XwOpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992e0240-61ca-45e6-b2a7-299ca40315c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 6.,  6.],\n",
              "       [14., 14.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercice"
      ],
      "metadata": {
        "id": "_8bykYPvwTVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- La multiplication élément-par-élément des matrices est-elle commutative ? Autrement dit, est-il vrai que\n",
        "$$\n",
        "\\mathbf{A}\\odot\\mathbf{B} = \\mathbf{B}\\odot\\mathbf{A}\n",
        "$$\n",
        "pour toutes les matrices $\\mathbf{A}$ et $\\mathbf{B}$ de dimensions $m\\times n$ ? Justifiez votre réponse."
      ],
      "metadata": {
        "id": "KQS-WN94wZYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Commutativité des matrices"
      ],
      "metadata": {
        "id": "K-vaEcA8wlsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les matrices carrées sont celles dont les dimensions sont de la forme $n\\times n$ (elles forment donc un tableau carré).\n",
        "Si $\\mathbf{A}$ et $\\mathbf{B}$ sont toutes deux des matrices carrées de dimensions $n\\times n$, alors les produits $\\mathbf{AB}$ et $\\mathbf{BA}$ sont toujours définis.\n",
        "\n",
        "Cependant, et c'est un point important, ils ne sont pas toujours égaux !\n",
        "Cela est très différent de la multiplication de nombres réels !\n",
        "\n",
        "Si $\\mathbf{A}$ et $\\mathbf{B}$ sont toutes deux des matrices carrées de dimensions $n\\times n$ telles que\n",
        "$$\n",
        "\\mathbf{AB} = \\mathbf{BA}\n",
        "$$\n",
        "alors nous disons que $\\mathbf{A}$ et $\\mathbf{B}$ _commutent_.\n",
        "(Cela ne se produit pas toujours !)\n",
        "\n",
        "Les matrices qui commutent ont des propriétés importantes les unes par rapport aux autres, mais cela dépasse le cadre de ce didacticiel.\n",
        "Si vous êtes intéressé : consultez [la page Wikipédia sur les matrices commutatives](https://en.wikipedia.org/wiki/Commuting_matrices), demandez à un enseignant ou consultez l'une des ressources de lecture complémentaires."
      ],
      "metadata": {
        "id": "aOoPEtGsw8ZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice"
      ],
      "metadata": {
        "id": "qWanFZBdxKs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Soit\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{pmatrix}\n",
        "\\quad \\text{ et } \\quad\n",
        "\\mathbf{B} =\n",
        "\\begin{pmatrix}\n",
        "0 & 1 \\\\\n",
        "1 & 0 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "  - Est-ce que $\\mathbf{A}$ et $\\mathbf{B}$ commutent ?\n",
        "  - Vérifiez vos calculs à l'aide de `jax`.\n",
        "\n",
        "- Soient $a, b, c, d \\in \\mathbb{R}$ des nombres réels arbitraires et définissons\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "a & b \\\\\n",
        "c & d\n",
        "\\end{pmatrix}\n",
        "\\quad \\text{ et } \\quad\n",
        "\\mathbf{I} =\n",
        "\\begin{pmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Prouvez que $\\mathbf{A}$ et $\\mathbf{B}$ commutent."
      ],
      "metadata": {
        "id": "B0PAfCzExTA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Place any code here"
      ],
      "metadata": {
        "id": "Dr7-_LbtwYjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### La Matrice Identité"
      ],
      "metadata": {
        "id": "iwrtmw9axZjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La _matrice identité_ est la matrice $n \\times n$ $\\mathbf{I}$ avec des composantes\n",
        "\n",
        "$$\n",
        "I_{ij}\n",
        "= \\begin{cases}\n",
        "  0 \\quad \\text{si } \\; i \\ne j \\\\\n",
        "  1 \\quad \\text{si } \\; i = j\n",
        "  \\end{cases}\n",
        "$$\n",
        "\n",
        "Sous forme de tableau, cela ressemble à\n",
        "\n",
        "$$\n",
        "\\mathbf{I}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "1 & 0 & 0 & \\cdots & 0 \\\\\n",
        "0 & 1 & 0 & \\cdots & 0 \\\\\n",
        "0 & 0 & 1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & 0 & \\cdots & 1 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Parfois, vous verrez $\\mathbf{I}_n$ lorsque l'auteur souhaite souligner que c'est la matrice identité de taille $n \\times n$. Par exemple, $\\mathbf{I}_2$ serait\n",
        "\n",
        "$$\n",
        "\\mathbf{I}_2 =\n",
        "\\begin{pmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Cependant, la dimension est souvent claire dans le contexte, donc nous ne l'écrirons pas sauf si nécessaire.\n",
        "\n",
        "Qu'est-ce qui est spécial à propos de la matrice identité ? C'est dans le nom : lorsque vous multipliez par l'identité, rien ne se passe. En particulier, si $\\mathbf{A}$ est une matrice carrée $n \\times n$ et $\\mathbf{I}$ est la matrice identité $n \\times n$, alors\n",
        "\n",
        "$$\n",
        "\\mathbf{IA} = \\mathbf{AI} = \\mathbf{A}\n",
        "$$\n",
        "\n",
        "et $\\mathbf{I}$ est la _seule_ matrice ayant cette propriété. Notez que cela signifie également que la matrice identité commute avec toutes les matrices ayant les mêmes dimensions.\n",
        "\n",
        "La matrice identité $n \\times n$ peut être obtenue dans `jax` en utilisant `jnp.eye(n)`. Par exemple, avec $n = 3$."
      ],
      "metadata": {
        "id": "TPGjVQdqx1Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.eye(3)"
      ],
      "metadata": {
        "id": "UShw20f0xpZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercice\n",
        "- Choisissez une matrice carrée $\\mathbf{A}$ et utilisez `jax` pour vérifier que la matrice identité (de la dimension appropriée) satisfait\n",
        "$$\n",
        "\\mathbf{IA} = \\mathbf{AI} = \\mathbf{A}\n",
        "$$"
      ],
      "metadata": {
        "id": "KQJVuMi7yCQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Place code here"
      ],
      "metadata": {
        "id": "FvCQuAWTyJr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inverse d'une Matrice"
      ],
      "metadata": {
        "id": "ey3thWXeyMXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Qu'est-ce que l'Inverse d'une Matrice ?\n",
        "Nous avons couvert l'addition et la soustraction de matrices ainsi que la multiplication de matrices. Existe-t-il une opération de division de matrices ?\n",
        "La réponse est oui, mais ce n'est pas toujours possible.\n",
        "\n",
        "Étant donné une matrice carrée $\\mathbf{A}$ de dimension $n \\times n$, nous disons que $\\mathbf{A}$ est _inversible_ ou _non singulière_ s'il existe une matrice $\\mathbf{A}^{-1}$ de même dimension $n \\times n$ telle que la multiplication par $\\mathbf{A}$ donne la matrice identité $n \\times n$. Autrement dit,\n",
        "$$\n",
        "\\mathbf{A}^{-1}\\mathbf{A} =\n",
        "\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n",
        "$$\n",
        "Si une telle matrice $\\mathbf{A}^{-1}$ existe, alors nous l'appelons l'inverse de la matrice $\\mathbf{A}$. Toutes les matrices n'ont pas un inverse, mais lorsqu'un inverse existe, il est unique."
      ],
      "metadata": {
        "id": "c6uPDe1JyaUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quand l'Inverse Existe-t-il ?\n",
        "Il est important de noter que _toutes les matrices n'ont pas d'inverse_.\n",
        "Par exemple, il n'existe pas de matrice $2 \\times 2$ $\\mathbf{B}$ telle que\n",
        "$$\n",
        "\\mathbf{B}\n",
        "\\begin{pmatrix}\n",
        "0 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "(Pouvez-vous voir pourquoi ?)\n",
        "Lorsqu'une matrice n'a pas d'inverse, on dit qu'elle est _non inversible_ ou _singulière_.\n",
        "\n",
        "Lorsque l'inverse d'une matrice existe, cela nous permet de résoudre certaines équations matricielles. Par exemple, si $\\mathbf{A}$ est inversible, alors\n",
        "$$\n",
        "\\mathbf{C} = \\mathbf{AB}\n",
        "\\quad\\implies\\quad\n",
        "\\mathbf{B} = \\mathbf{A}^{-1}\\mathbf{C}\n",
        "$$\n",
        "De même, si $\\mathbf{b}$ et $\\mathbf{c}$ sont des vecteurs de longueur $n$ et que $\\mathbf{A}$ est inversible, alors\n",
        "$$\n",
        "\\mathbf{c} = \\mathbf{Ab}\n",
        "\\quad\\implies\\quad\n",
        "\\mathbf{b} = \\mathbf{A}^{-1}\\mathbf{c}\n",
        "$$\n",
        "\n",
        "Il existe de nombreuses façons équivalentes de vérifier si une matrice est inversible. Par exemple, consultez [la page wiki sur les matrices inversibles](https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem). Parfois, vous pouvez même le voir par simple inspection, comme avec la matrice de tous les zéros ci-dessus.\n",
        "Une méthode importante consiste à vérifier le _déterminant_ de la matrice, qui est une quantité scalaire associée à une matrice carrée. Une matrice est inversible si et seulement si son déterminant est différent de zéro. Nous aborderons le déterminant plus tard dans ce tutoriel."
      ],
      "metadata": {
        "id": "fYNi-OHay11J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Propriétés Algébriques\n",
        "Soient $\\mathbf{A}$ et $\\mathbf{B}$ des matrices carrées inversibles $n \\times n$. Alors, la matrice $\\mathbf{AB}$ est inversible avec pour inverse\n",
        "$$\n",
        "(\\mathbf{AB})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\n",
        "$$\n",
        "En général, même si les deux côtés existent,\n",
        "$$\n",
        "(\\mathbf{A} + \\mathbf{B})^{-1} \\ne \\mathbf{A}^{-1} + \\mathbf{B}^{-1}\n",
        "$$\n",
        "Cela peut sembler un peu décevant, mais nous ne sommes pas complètement démunis. Un cas particulier de l'identité matricielle de Woodbury est\n",
        "$$\n",
        "(\\mathbf{A} + \\mathbf{B})^{-1}\n",
        "=\n",
        "\\mathbf{A}^{-1} - \\mathbf{A}^{-1}(\\mathbf{A}^{-1}\n",
        "+ \\mathbf{B}^{-1})^{-1}\\mathbf{A}^{-1}\n",
        "=\n",
        "\\mathbf{A}^{-1} - (\\mathbf{A} + \\mathbf{A}\\mathbf{B}^{-1}\\mathbf{A})^{-1}\n",
        "$$\n",
        "qui est valable tant que les inverses pertinents existent.\n",
        "\n",
        "#### Exercices\n",
        "- La matrice suivante est-elle inversible ?\n",
        "  $$\n",
        "\\begin{pmatrix}\n",
        "0 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "- La matrice identité $n \\times n$ est-elle inversible ? Si oui, quel est son inverse ?\n",
        "- (Optionnel) Démontrez que les inverses de matrices sont uniques. Autrement dit, prouvez que pour toute matrice $\\mathbf{A}$ de taille $n \\times n$, il existe au plus une matrice $\\mathbf{A}^{-1}$ telle que\n",
        "$$\n",
        "\\mathbf{A}^{-1}\\mathbf{A} =\n",
        "\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n",
        "$$\n",
        "- (Optionnel) Trouvez un contre-exemple (où les deux côtés de l'équation existent) à\n",
        "$(\\mathbf{A} + \\mathbf{B})^{-1}\n",
        "=\n",
        "\\mathbf{A}^{-1} + \\mathbf{B}^{-1}$."
      ],
      "metadata": {
        "id": "O0NlGZpyzICC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calcul de l'Inverse\n",
        "\n",
        "Calculer l'inverse d'une matrice est intensif en termes de calcul (voir [ici](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra)).\n",
        "Il existe des formules pour calculer les inverses à la main, mais le faire est fastidieux même pour les matrices de $3 \\times 3$, et cela devient bien pire lorsque les dimensions augmentent.\n",
        "La formule pour l'inverse d'une matrice $2 \\times 2$ est relativement simple, mais elle ne vous mènera pas loin en apprentissage automatique. Nous la donnons ici au cas où vous ne l'auriez jamais vue.\n",
        "\n",
        "Si\n",
        "$$\n",
        "\\mathbf{A}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "a & b\\\\\n",
        "c & d\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "avec $ad - bc \\neq 0$, alors\n",
        "$$\n",
        "\\mathbf{A}^{-1}\n",
        "=\n",
        "\\frac{1}{ad - bc}\n",
        "\\begin{pmatrix}\n",
        "d & -b\\\\\n",
        "-c & a\n",
        "\\end{pmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "cf1HpJY4zeBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercice (Optionnel)\n",
        "- Vérifiez que la formule ci-dessus donne $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$.\n",
        "- Calculez l'inverse de\n",
        "$$\n",
        "\\mathbf{A} = \\begin{pmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{pmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "Ko03dSOZzoJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calcul de l'inverse à l'aide de `jax`\n",
        "Comme la plupart des bibliothèques d'algèbre linéaire, `jax` offre une manière de calculer les inverses de matrices."
      ],
      "metadata": {
        "id": "IxCtwO5uzytJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[1, 3], [2, 4]])\n",
        "jnp.linalg.inv(A)"
      ],
      "metadata": {
        "id": "gH4hWR1cyZUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cependant, cela peut être numériquement instable pour les matrices de grande taille. De plus, ce que l'on veut souvent à la fin n'est pas $\\mathbf{A}^{-1}$, mais en réalité $\\mathbf{A}^{-1} \\mathbf{b}$ pour un vecteur $\\mathbf{b}$. Par exemple, nous pourrions vouloir résoudre l'équation\n",
        "\n",
        "$$\n",
        "\\mathbf{b} = \\mathbf{A}\\mathbf{c}\n",
        "$$\n",
        "\n",
        "pour $\\mathbf{c}$. Dans ces cas, il est bien meilleur (à la fois plus stable et plus rapide) d'utiliser `jnp.linalg.solve` plutôt que de calculer l'inverse et effectuer la multiplication. Notez que cette fonction suppose que $\\mathbf{A}$ est inversible. Pour une application simple à la résolution de systèmes d'équations linéaires, consultez la section 2.1 du [Livre de mathématiques pour l'apprentissage automatique](https://mml-book.github.io/book/mml-book.pdf)."
      ],
      "metadata": {
        "id": "6ab7ZFnIz3a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[1, 3], [2, 4]])\n",
        "b = jnp.array([1, 1])\n",
        "c = jnp.linalg.inv(A) @ b # bad way\n",
        "c = jnp.linalg.solve(A, b) # good way"
      ],
      "metadata": {
        "id": "VyJbLifZ0E7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons faire une comparaison sommaire pour évaluer la différence de vitesse."
      ],
      "metadata": {
        "id": "wCWbxUfJ0BHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# générer des données aléatoires\n",
        "rng = np.random.RandomState(seed=0)\n",
        "A = jnp.eye(100) + 0.01 * jnp.array(rng.randn(100, 100))\n",
        "b = jnp.array(rng.randn(100))\n",
        "\n",
        "# vérifier si la matrice générée A est inversible (c'est-à-dire que son déterminant n'est pas nul)\n",
        "# il est également intéressant de s'assurer qu'il n'est pas trop proche de 0 lors de ce test pour des raisons de stabilité\n",
        "print(jnp.linalg.det(A))"
      ],
      "metadata": {
        "id": "6jZInzlg0AYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "c1 = jnp.linalg.inv(A) @ b"
      ],
      "metadata": {
        "id": "IIgEQ0SO0WRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "c2 = jnp.linalg.solve(A, b)"
      ],
      "metadata": {
        "id": "Km7QgDGd0gOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifiez si rien n'a été cassé et si les résultats sont les mêmes (dans une tolérance numérique) :\n",
        "c1 = jnp.linalg.inv(A) @ b\n",
        "c2 = jnp.linalg.solve(A, b)\n",
        "assert all(np.isfinite(c1))\n",
        "assert np.allclose(c1, c2)"
      ],
      "metadata": {
        "id": "ytHgZ1fr0rWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercice\n",
        "\n",
        "Soit\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "1 & 2 & 3 & 4 \\\\\n",
        "4 & 3 & 2 & 1 \\\\\n",
        "5 & 0 & 0 & 8 \\\\\n",
        "8 & 1 & 1 & 5\n",
        "\\end{pmatrix}\n",
        "\\quad\\text{ et }\\quad\n",
        "\\mathbf{b} =\n",
        "\\begin{pmatrix}\n",
        "1 \\\\ 0 \\\\ 0 \\\\ 1\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Utilisez `jax` pour résoudre\n",
        "$$\n",
        "\\mathbf{Ac} = \\mathbf{b}\n",
        "$$\n",
        "pour $\\mathbf{c}$."
      ],
      "metadata": {
        "id": "kuz3qNbf04EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([\n",
        "    [1, 2, 3, 4],\n",
        "    [4, 3, 2, 1],\n",
        "    [5, 0, 0, 8],\n",
        "    [8, 1, 1, 5],\n",
        "  ])\n",
        "b = jnp.array([1, 0, 0, 1])\n",
        "\n",
        "# Put your code here"
      ],
      "metadata": {
        "id": "gK0x8k2N0xBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### La Transposée"
      ],
      "metadata": {
        "id": "wCmcjX721OGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une opération importante sur les matrices est la _transposée_.\n",
        "Si vous ne l'avez pas encore rencontrée, la transposée d'une matrice vous semblera probablement une opération inhabituelle et arbitraire, sans beaucoup d'intuition derrière elle. Cependant, elle a une signification profonde en mathématiques (liée aux espaces duaux, que nous n'aborderons pas ici) et aussi une interprétation géométrique (liée aux produits intérieurs, que nous aborderons plus tard). Indépendamment de sa signification plus profonde, la transposée se produit régulièrement, il est donc important de savoir comment la calculer."
      ],
      "metadata": {
        "id": "qgNtSMse1XHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calcul de la transposée"
      ],
      "metadata": {
        "id": "vvQIjizF1nEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soit $\\mathbf{A}$ une matrice de dimensions $m \\times n$. Alors, la transposée de $\\mathbf{A}$, notée $\\mathbf{A}^\\top$, est la matrice de dimensions $n \\times m$ obtenue en utilisant les colonnes de $\\mathbf{A}$ comme lignes à la place. La première colonne de $\\mathbf{A}$ devient la première ligne de $\\mathbf{A}^\\top$, la deuxième colonne de $\\mathbf{A}$ devient la deuxième ligne de $\\mathbf{A}^\\top$, et ainsi de suite.\n",
        "\n",
        "Quelques exemples :\n",
        "- $$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4 \\\\\n",
        "5 & 6\n",
        "\\end{pmatrix}\n",
        "\\quad\\implies\\quad\n",
        "\\mathbf{A}^\\top =\n",
        "\\begin{pmatrix}\n",
        "1 & 3 & 5 \\\\\n",
        "2 & 4 & 6\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "- $$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{pmatrix}\n",
        "\\quad\\implies\\quad\n",
        "\\mathbf{A}^\\top =\n",
        "\\begin{pmatrix}\n",
        "1 & 3  \\\\\n",
        "2 & 4\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "- $$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "1 & -1 & 0 \\\\\n",
        "-1 & 1 & -1 \\\\\n",
        "0 & -1 & 1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\quad\\implies\\quad\n",
        "\\mathbf{A}^\\top =\n",
        "\\begin{pmatrix}\n",
        "1 & -1 & 0 \\\\\n",
        "-1 & 1 & -1 \\\\\n",
        "0 & -1 & 1 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Remarquez que la transposée d'une matrice non carrée est une matrice de forme différente, mais qu'une matrice carrée a autant de lignes que de colonnes, donc sa forme reste la même que celle de sa transposée.\n",
        "\n",
        "La formule générale pour calculer la transposée d'une matrice est la suivante. Si $\\mathbf{A}$ est une matrice $m \\times n$ avec des composantes $\\mathbf{A}_{ij}$, alors la transposée de $\\mathbf{A}$ est la matrice $n \\times m$ $\\mathbf{B} = \\mathbf{A}^\\top$ avec des composantes $\\mathbf{B}_{ij} = \\mathbf{A}_{ji}$. Le changement d'ordre des indices représente l'échange des lignes avec les colonnes.\n",
        "\n",
        "Si une matrice est égale à sa transposée, c'est-à-dire $\\mathbf{A} = \\mathbf{A}^\\top$, alors on dit qu'elle est _symétrique_.\n",
        "\n",
        "Une classe importante de matrices sont les _matrices orthogonales_, ce sont des matrices pour lesquelles $\\mathbf{A}^\\top = \\mathbf{A}^{-1}$. Autrement dit, des matrices pour lesquelles la transposée est égale à l'inverse. Encore une fois, cela peut sembler arbitraire, mais cela a une signification algébrique et géométrique importante. Nous aborderons certains de ces aspects plus tard.\n",
        "\n",
        "La transposée d'une matrice peut être calculée facilement dans `jax` en utilisant `A.T` ou `A.transpose()`."
      ],
      "metadata": {
        "id": "qLf9MuVF1pB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[1, 2], [3, 4], [5, 6]])\n",
        "A"
      ],
      "metadata": {
        "id": "9ySNVnZV1WZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.T"
      ],
      "metadata": {
        "id": "eSkQIGyj1_M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.transpose()"
      ],
      "metadata": {
        "id": "hy3HHlzs2CA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculez la transposée de la matrice\n",
        "$$\n",
        "\\mathbf{A}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "-1 & -2\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "et vérifiez le résultat avec `jax` ci-dessous."
      ],
      "metadata": {
        "id": "xH78GHFO2N3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A =  jnp.array([[1, 0], [0, 1], [-1, -2]])\n",
        "# Place your code here"
      ],
      "metadata": {
        "id": "JPjkyafH2Ewj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Propriétés algébriques"
      ],
      "metadata": {
        "id": "5sfszw6a2WMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tout comme pour l'inverse de la matrice, nous avons des règles pour manipuler les transposées de matrices qui sont utiles et faciles à prouver (essayez-le !).\n",
        "\n",
        "Pour toutes matrices $\\mathbf{A}$ et $\\mathbf{B}$ de dimensions $m\\times n$ :\n",
        "- $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$\n",
        "- $(\\mathbf{A} + \\mathbf{B})^\\top = \\mathbf{A}^\\top + \\mathbf{B}^\\top$\n",
        "\n",
        "Si $\\mathbf{A}$ est une matrice $m\\times n$ et $\\mathbf{B}$ est une matrice $n\\times k$, alors\n",
        "$$\n",
        "\\mathbf{AB}^{\\top} = \\mathbf{B}^\\top\\mathbf{A}^\\top\n",
        "$$\n",
        "\n",
        "Si $\\mathbf{A}$ est une matrice carrée inversible, alors $\\mathbf{A}^\\top$ l'est également, et\n",
        "$$\n",
        "{(\\mathbf{A}^\\top)}^{-1}\n",
        "=\n",
        "{(\\mathbf{A}^{-1})}^{\\top}\n",
        "$$"
      ],
      "metadata": {
        "id": "n5gZV97H2mdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrices en tant qu'applications linéaires"
      ],
      "metadata": {
        "id": "_1d-Tk1E29Ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jusqu'à présent, nous avons considéré les matrices comme des tableaux avec certaines opérations telles que les multiplications, les additions, les inversions et les transpositions. Cette vue computationnelle offre une littératie suffisante pour les utilisations de base en apprentissage automatique, et elle est même solide d'un point de vue mathématique. Cependant, elle manque d'intuition.\n",
        "\n",
        "Une perspective alternative, plus géométrique, sur les matrices est qu'elles représentent des _transformations linéaires de vecteurs_. Soient $U$ et $V$ des espaces vectoriels, alors une fonction $f : U \\to V$ est une _application linéaire_ si elle satisfait\n",
        "$$\n",
        "f(\\lambda \\mathbf{a} + \\mu \\mathbf{b})\n",
        "= \\lambda f(\\mathbf{a}) + \\mu f(\\mathbf{b})\n",
        "$$\n",
        "pour tous les $\\mathbf{a}, \\mathbf{b} \\in V$ et tous les $\\lambda, \\mu \\in \\mathbb{R}$.\n",
        "\n",
        "Vous pouvez vérifier à partir de la formule que la multiplication matricielle est linéaire. En particulier, si $\\mathbf{A}$ est une matrice $m \\times k$ et $\\mathbf{B}$ et $\\mathbf{C}$ sont des matrices $k \\times n$, alors\n",
        "$$\n",
        "\\mathbf{A}(\\lambda \\mathbf{B} + \\mu \\mathbf{C})\n",
        "= \\lambda \\mathbf{AB} + \\mu \\mathbf{AC}\n",
        "$$\n",
        "Cela vaut également pour les vecteurs $\\mathbf{b}, \\mathbf{c} \\in \\mathbb{R}^n$ (qui correspondent au cas $n=1$) :\n",
        "$$\n",
        "\\mathbf{A}(\\lambda \\mathbf{b} + \\mu \\mathbf{c})\n",
        "= \\lambda \\mathbf{Ab} + \\mu \\mathbf{Ac}\n",
        "$$\n",
        "\n",
        "L'ensemble des transformations linéaires est important car elles préservent la structure de l'espace vectoriel. Elles mappent les vecteurs en vecteurs (éventuellement dans un espace différent) sans perturber l'addition et la multiplication scalaire. Par exemple, une transformation linéaire appliquée à une somme est simplement la somme des transformations linéaires appliquées aux termes individuels.\n",
        "\n",
        "Toute application linéaire $f : \\mathbb{R}^n \\to \\mathbb{R}^n$ peut être représentée par une matrice $n \\times n$. Ce fait peut être vérifié en considérant une _base_ de l'espace vectoriel $\\mathbb{R}^n$, qui sera introduite plus tard. Par \"représentée\", nous entendons qu'il existe une matrice $\\mathbf{A}$ telle que $f(\\mathbf{v}) = \\mathbf{A}\\mathbf{v}$ pour tous les vecteurs $\\mathbf{v} \\in \\mathbb{R}^n$. Pour être clair, c'est _une seule_ matrice $\\mathbf{A}$ qui fonctionne pour _tous_ les vecteurs $\\mathbf{v} \\in \\mathbb{R}^n$ simultanément. La fonction identité $f(\\mathbf{v}) = \\mathbf{v}$ est représentée par la matrice identité $\\mathbf{I}$.\n",
        "\n",
        "Suppose que la transformation linéaire $f$ est représentée par la multiplication par la matrice $\\mathbf{A}$. Alors, $f$ est inversible (en tant que fonction) si et seulement si l'inverse matriciel $\\mathbf{A}^{-1}$ existe. Lorsque $f$ est inversible, $f^{-1}$ est représentée par la matrice $\\mathbf{A}^{-1}$.\n",
        "\n",
        "Les transformations linéaires, par exemple de $\\mathbb{R}^n$ à $\\mathbb{R}^n$, ont une interprétation géométrique. Cela est plus facilement visualisé pour les transformations linéaires $\\mathbb{R}^2 \\to \\mathbb{R}^2$, qui sont des transformations linéaires du plan 2D. Elles sont représentées par des matrices $2 \\times 2$. Par exemple,\n",
        "$$\n",
        "\\mathbf{R}_\\theta\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "\\cos\\theta & -\\sin\\theta \\\\\n",
        "\\sin\\theta & \\cos\\theta\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "effectue une rotation des points dans le sens des aiguilles d'une montre d'un angle $\\theta$ autour de l'origine, tandis que\n",
        "$$\n",
        "\\mathbf{P}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "projette les points sur l'axe des abscisses.\n",
        "\n",
        "Ci-dessous, vous trouverez une animation d'une transformation linéaire de l'espace en 2D.\n",
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-wsDUmp9EDNYWUacsbx7oTpHqe9xh6XcWSNGwP9CdX9VkMhO6zzgsv-sXv7zMTbElTzwLHMKAjSxUz5oKiTQC_RT0g7AQ=s1600\"\n",
        "width=\"60%\" />"
      ],
      "metadata": {
        "id": "yxcgnS__2_Yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercices\n",
        "- Montrez que pour tout espace vectoriel $V$, toute application linéaire $f : V \\to V$ satisfait $f(\\mathbf{0}) = \\mathbf{0}$.\n",
        "- Calculez $\\mathbf{R}_\\theta^\\top$.\n",
        "  - Comment cela se rapporte-t-il à $\\mathbf{R}_{-\\theta}$ ? En pensant géométriquement, quelle transformation représente $\\mathbf{R}_\\theta^\\top \\mathbf{R}_\\theta$ ?\n",
        "  - Argumentez (en essayant d'éviter de nouveaux calculs) que $\\mathbf{R}_\\theta$ est une matrice orthogonale.\n",
        "  - Si vous le souhaitez, vous pouvez également vérifier directement que $\\mathbf{R}_\\theta$ est orthogonal.\n",
        "- Calculez la matrice $2 \\times 2$ donnant la transformation dans l'animation.\n",
        "  - Astuce : À partir de l'animation, la transformation envoie\n",
        "  $$\n",
        "  \\begin{pmatrix} 2\\\\2\\end{pmatrix}\n",
        "  \\mapsto\n",
        "  \\begin{pmatrix} 6\\\\-2 \\end{pmatrix}\n",
        "  \\quad\\text{ et }\\quad\n",
        "  \\begin{pmatrix} 1\\\\2\\end{pmatrix}\n",
        "  \\mapsto\n",
        "  \\begin{pmatrix} 5\\\\0 \\end{pmatrix}\n",
        "  $$\n",
        "- Convainquez-vous que résoudre le problème ci-dessus est équivalent à résoudre l'équation matricielle\n",
        "$$\n",
        "\\mathbf{A}\n",
        "\\begin{pmatrix}\n",
        "2 & 1\\\\\n",
        "2 & 2\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "6 & 5\\\\\n",
        "-2 & 0\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "pour $\\mathbf{A}$ et utilisez `jax` pour calculer la solution."
      ],
      "metadata": {
        "id": "m1nAL0cr3TZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Place any code here"
      ],
      "metadata": {
        "id": "YPX-27pC4CYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ressources recommandées"
      ],
      "metadata": {
        "id": "zXsWrkIl4F9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **3Blue1Brown** (bon pour l'intuition)\n",
        "  - [Leçon sur les transformations linéaires](https://www.3blue1brown.com/lessons/linear-transformations)\n",
        "  - [Vidéo sur les transformations en 3D](https://www.3blue1brown.com/lessons/3d-transformations)\n",
        "  - [Leçon sur la multiplication de matrices](https://www.3blue1brown.com/lessons/matrix-multiplication)\n",
        "  - [Vidéo sur les matrices non carrées](https://www.3blue1brown.com/lessons/nonsquare-matrices)\n",
        "- Chapitres 3 et 4 du cours de l'Université de Cambridge [Vecteurs et Matrices](https://dec41.user.srcf.net/notes/IA_M/vectors_and_matrices.pdf) pour des lectures plus avancées sur les bases.\n",
        "- Chapitre 2 du cours de l'Université de Cambridge [Algèbre Linéaire](https://dec41.user.srcf.net/notes/IB_M/linear_algebra.pdf) pour un point de vue plus mathématique."
      ],
      "metadata": {
        "id": "3KTu7Sqf4Qmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Produits scalaires"
      ],
      "metadata": {
        "id": "WjPHLm1Y4nAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le _produit scalaire_ fournit une méthode de multiplication entre vecteurs pour produire un scalaire. Étant donné $\\mathbf{a}, \\mathbf{b}\\in\\mathbb{R}^n$, leur produit scalaire est noté $\\mathbf{a}\\cdot\\mathbf{b}$ ou $\\mathbf{a}^\\top\\mathbf{b}$ et est calculé à l'aide de la formule\n",
        "$$\n",
        "\\mathbf{a}\\cdot\\mathbf{b}\n",
        "= \\sum_{i=1}^n \\mathbf{a}_i\\mathbf{b}_i\n",
        "$$\n",
        "ce qui équivaut à la somme des produits des entrées correspondantes dans les vecteurs. Le produit scalaire est défini uniquement si les deux vecteurs ont la même dimension. Le produit scalaire est parfois appelé _produit scalaire_ ou _produit intérieur euclidien_.\n",
        "\n",
        "Voici quelques exemples :\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1\\\\2\\\\3\\\\4\n",
        "\\end{pmatrix}\n",
        "\\cdot\n",
        "\\begin{pmatrix}\n",
        "5\\\\6\\\\7\\\\8\n",
        "\\end{pmatrix}\n",
        "=\n",
        "5 + 12 + 21 + 32\n",
        "=  70\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1\\\\1\\\\1\\\\1\n",
        "\\end{pmatrix}\n",
        "\\cdot\n",
        "\\begin{pmatrix}\n",
        "1\\\\2\\\\3\\\\4\n",
        "\\end{pmatrix}\n",
        "=\n",
        "1 + 2 + 3 + 4\n",
        "=  10\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1\\\\0\n",
        "\\end{pmatrix}\n",
        "\\cdot\n",
        "\\begin{pmatrix}\n",
        "0\\\\ 1\n",
        "\\end{pmatrix}\n",
        "=\n",
        "0 + 0\n",
        "=  0\n",
        "$$"
      ],
      "metadata": {
        "id": "XnRproAR4p99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notation"
      ],
      "metadata": {
        "id": "NsHnndA241pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est très important de noter que lorsque $\\mathbf{a}$ et $\\mathbf{b}$ sont des vecteurs dans $\\mathbb{R}^n$, alors les deux notations\n",
        "$$\\mathbf{a}\\cdot\\mathbf{b}$$\n",
        "et\n",
        "$$\n",
        "\\mathbf{a}^\\top \\mathbf{b}\n",
        "$$\n",
        "_signifient exactement la même chose_. Cela peut ne pas sembler idéal si vous voyez le produit scalaire pour la première fois, mais les deux notations sont utilisées de manière interchangeable dans la littérature, vous les verrez donc toutes les deux.\n",
        "\n",
        "Vous pouvez vérifier la formule\n",
        "$$\n",
        "\\mathbf{a}^\\top\\mathbf{b}\n",
        "= \\sum_{i=1}^n \\mathbf{a}_i\\mathbf{b}_i\n",
        "$$\n",
        "en considérant $\\mathbf{a}^\\top$ comme une matrice $1\\times n$ (parfois appelée vecteur ligne)\n",
        "et $\\mathbf{b}$ comme une matrice $n\\times 1$\n",
        "(parfois appelée vecteur colonne), puis en appliquant la formule\n",
        "de multiplication de matrices (essayez-le !).\n",
        "\n",
        "Conformément à la littérature, **nous utiliserons les deux notations dans cette séance pratique**."
      ],
      "metadata": {
        "id": "_Y6VQW6143gX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Produit scalaire en Machine Learning"
      ],
      "metadata": {
        "id": "O-JhnRBC5NUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le produit scalaire est très courant en apprentissage automatique.\n",
        "- La prédiction d'un modèle linéaire avec des poids $\\mathbf{w}$ et des caractéristiques $\\mathbf{x}$ est\n",
        "$$\n",
        "\\mathbf{w}^\\top \\mathbf{x}\n",
        "$$\n",
        "- La sortie d'une couche de réseau de neurones à propagation avant avec une fonction d'activation $\\sigma$,\n",
        "des poids $\\mathbf{w}$, un biais $b$ et des entrées $\\mathbf{x}$\n",
        "est\n",
        "$$\n",
        "\\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)\n",
        "$$\n",
        "- Le mécanisme d'attention est calculé en prenant le produit scalaire\n",
        "des vecteurs clé et valeur. Par exemple, consultez\n",
        "[L'attention est tout ce dont vous avez besoin](https://arxiv.org/pdf/1706.03762.pdf)."
      ],
      "metadata": {
        "id": "i33LCZkv5jxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calcul du produit scalaire avec `jax`\n",
        "- Calcul du produit scalaire avec `jax` : jnp.dot"
      ],
      "metadata": {
        "id": "pKtaYHrM5Bfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.array([1, 2, 3, 4])\n",
        "b = jnp.array([5, 6, 7, 8])\n",
        "jnp.dot(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lhK8Rur58KN",
        "outputId": "1dda44e4-a8e3-4fa1-cf77-e297188de867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(70, dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice\n",
        "Calculez $\\mathbf{a}\\cdot\\mathbf{b}$ lorsque\n",
        "\n",
        "$$\n",
        "\\mathbf{a} =\n",
        "\\begin{pmatrix}\n",
        "1\\\\0\\\\2\\\\-1\n",
        "\\end{pmatrix}\n",
        "\\quad\\text{ et }\\quad\n",
        "\\mathbf{b} =\n",
        "\\begin{pmatrix}\n",
        "-2\\\\100\\\\0.5\\\\3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "et vérifiez votre réponse en utilisant `jax`."
      ],
      "metadata": {
        "id": "HFB2lNHL6Jjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placez votre code ici"
      ],
      "metadata": {
        "id": "qlmFjenc6R4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propriétés"
      ],
      "metadata": {
        "id": "Y517NJfY61kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il y a quelques propriétés du produit scalaire qui sont importantes de connaître. Toutes sont faciles à vérifier en utilisant la formule (essayez !).\n",
        "\n",
        "Soient $\\mathbf{a}, \\mathbf{b}\\in \\mathbb{R}^n$\n",
        "des vecteurs, alors\n",
        "- $\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{b}\\cdot\\mathbf{a}$ (commutativité)\n",
        "- $\\mathbf{a}\\cdot\\mathbf{a} \\ge 0$\n",
        "- $\\mathbf{a}\\cdot\\mathbf{a} = 0$ si et seulement si $\\mathbf{a}=\\mathbf{0}$"
      ],
      "metadata": {
        "id": "J8qQ3YOq64P9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice"
      ],
      "metadata": {
        "id": "9szEfB8j7OlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soient $\\mathbf{a}, \\mathbf{b}, \\mathbf{c} \\in \\mathbb{R}^n$\n",
        "et soient $\\lambda, \\mu\\in\\mathbb{R}$.\n",
        "\n",
        "Utilisez la formule du produit scalaire pour prouver que\n",
        "1. $\\mathbf{a}\\cdot(\\mathbf{b} + \\mathbf{c}) =\n",
        "\\mathbf{a}\\cdot\\mathbf{b} + \\mathbf{a}\\cdot\\mathbf{c}$\n",
        "2. $(\\lambda \\mathbf{a})\\cdot (\\mu \\mathbf{b}) = (\\lambda \\mu)  (\\mathbf{a}\\cdot\\mathbf{b})$"
      ],
      "metadata": {
        "id": "v4kxAb8E7TCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "DyYvbs7h7mWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Solution :\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbf{a}\\cdot(\\mathbf{b} + \\mathbf{c})\n",
        "&= \\sum_{i=1}^n\n",
        "\\mathbf{a}_i(\\mathbf{b}+\\mathbf{c})_i\\\\\n",
        "&= \\sum_{i=1}^n\n",
        "\\mathbf{a}_i(\\mathbf{b}_i+\\mathbf{c}_i)\\\\\n",
        "&= \\sum_{i=1}^n \\mathbf{a}_i\\mathbf{b}_i\n",
        " + \\sum_{i=1}^n \\mathbf{a}_i\\mathbf{c}_i\\\\\n",
        "&=\n",
        "\\mathbf{a}\\cdot\\mathbf{b} + \\mathbf{a}\\cdot\\mathbf{c}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "2. Solution :\n",
        "$$\n",
        "\\begin{align*}\n",
        "(\\lambda \\mathbf{a})\\cdot(\\mu\\mathbf{b})\n",
        "&= \\sum_{i=1}^n\n",
        "\\lambda\\mathbf{a}_i\\mu\\mathbf{b}\\\\\n",
        "&= \\lambda \\mu\\sum_{i=1}^n\n",
        "\\mathbf{a}_i\\mathbf{b}\\\\\n",
        "&= (\\lambda\\mu)(\\mathbf{a}\\cdot\\mathbf{b})\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "vzetBp5N7ogE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relation à la Transposition de Matrice [Optionnel]"
      ],
      "metadata": {
        "id": "5hlb3WRE8AdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le produit scalaire offre une interprétation de la transposition de matrice.\n",
        "Soient $\\mathbf{a}$ et $\\mathbf{b}$ des vecteurs dans $\\mathbb{R}^n$, et $\\mathbf{M}$ une matrice $n \\times n$.\n",
        "\n",
        "Alors nous avons la relation\n",
        "$$\n",
        "(\\mathbf{Ma}) \\cdot \\mathbf{b}\n",
        "=\n",
        "\\mathbf{a} \\cdot (\\mathbf{M}^\\top\\mathbf{b})\n",
        "$$\n",
        "\n",
        "Ainsi, si vous appliquez une matrice d'un côté du produit scalaire, c'est équivalent à appliquer la transposée de cette matrice de l'autre côté du produit scalaire.\n",
        "\n",
        "Une autre façon d'écrire la même chose (en utilisant la notation de la transposée)\n",
        "est\n",
        "$$\n",
        "(\\mathbf{Ma})^\\top \\mathbf{b}\n",
        "=\n",
        "\\mathbf{a}^\\top (\\mathbf{M}^\\top\\mathbf{b})\n",
        "=\n",
        "\\mathbf{a}^\\top \\mathbf{M}^\\top\\mathbf{b}\n",
        "$$\n",
        "\n",
        "Cela signifie que les matrices orthogonales préservent le produit scalaire.\n",
        "Rappelez-vous que la matrice $\\mathbf{R}$ est orthogonale si $\\mathbf{R}^\\top \\mathbf{R} = \\mathbf{I}$, alors\n",
        "$$\n",
        "\\mathbf{Ra} \\cdot \\mathbf{Rb}\n",
        "=\n",
        "\\mathbf{a} \\cdot (\\mathbf{R}^\\top\\mathbf{R})\\mathbf{b}\n",
        "=\n",
        "\\mathbf{a} \\cdot \\mathbf{b}\n",
        "$$"
      ],
      "metadata": {
        "id": "wPs3s1Nz8A1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vecteurs Orthogonaux"
      ],
      "metadata": {
        "id": "E-u1Tnuw881E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si $\\mathbf{a}\\cdot\\mathbf{b}=0$, alors nous disons que $\\mathbf{a}$ et $\\mathbf{b}$ sont _orthogonaux_.\n",
        "\n",
        "L'orthogonalité est l'un des concepts les plus importants en algèbre linéaire intermédiaire/avancée, mais au niveau débutant, vous avez juste besoin de connaître la signification du mot et d'avoir une intuition de base.\n",
        "Nous entrerons dans plus de détails dans la section intermédiaire du module pratique.\n",
        "\n",
        "L'intuition de l'orthogonalité provient d'une intuition géométrique pour le produit scalaire que nous aborderons plus tard dans la section \"Géométrie vectorielle de base\".\n",
        "En apprentissage automatique, le produit scalaire est souvent utilisé comme mesure de similarité ou de dépendance statistique entre des caractéristiques, des représentations ou des plongements représentés par des vecteurs.\n",
        "Vous verrez cela régulièrement dans la littérature, par exemple dans les applications de regroupement.\n",
        "Dans ce contexte, les représentations correspondant à des vecteurs orthogonaux sont aussi indépendantes que possible.\n",
        "\n",
        "**Note [Optionnelle]**\n",
        "\n",
        "Plus tôt, nous avons appris les matrices orthogonales.\n",
        "Une définition équivalente d'une matrice orthogonale est une matrice dont les colonnes (considérées comme des vecteurs) sont orthogonales les unes aux autres.\n",
        "(Pouvez-vous le prouver ?)"
      ],
      "metadata": {
        "id": "TBnVtVq-8-0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ressources recommandées"
      ],
      "metadata": {
        "id": "ZdGIww289Uwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Vidéo 3Blue1Brown sur les produits scalaires](https://www.3blue1brown.com/lessons/dot-products)\n",
        "- Sections 3.1-3.4 du livre [Math for ML Book](https://mml-book.github.io/book/mml-book.pdf) sur les normes et les produits internes\n",
        "- Chapitres 2 du cours de Cambridge [Vectors and Matrices](https://dec41.user.srcf.net/notes/IA_M/vectors_and_matrices.pdf) pour une lecture plus avancée sur les bases\n",
        "- (Avancé) Chapitre 8 du cours de Cambridge [Linear Algebra](https://dec41.user.srcf.net/notes/IB_M/linear_algebra.pdf) pour une perspective plus mathématique"
      ],
      "metadata": {
        "id": "9WZL937i9WLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La magnitude d'un vecteur"
      ],
      "metadata": {
        "id": "hUOWo8ud9bZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definition"
      ],
      "metadata": {
        "id": "QAjvxqpQ-BAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La _magnitude_ d'un vecteur est une mesure de sa taille.\n",
        "Soit $\\mathbf{a}\\in\\mathbb{R}^n$, sa magnitude est définie comme\n",
        "$$\n",
        "||\\mathbf{a}||\n",
        "=\n",
        "\\sqrt{\\mathbf{a}\\cdot\\mathbf{a}}\n",
        "$$\n",
        "La magnitude d'un vecteur est parfois appelée\n",
        "la _norme euclidienne_ du vecteur ou\n",
        "la _longueur_ du vecteur (ne confondez pas cela avec le nombre d'éléments dans le tableau !).\n",
        "\n",
        "Rappelez-vous des propriétés du produit scalaire que pour tous les vecteurs\n",
        "$\\mathbf{a}\\in \\mathbb{R}^n$\n",
        "$$\n",
        "\\mathbf{a}\\cdot\\mathbf{a} \\ge 0\n",
        "$$\n",
        "Cela signifie que la racine carrée dans la définition est toujours valide."
      ],
      "metadata": {
        "id": "03L7BwXl-FQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notation"
      ],
      "metadata": {
        "id": "psYFpImA-PbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Notez que\n",
        "$$\n",
        "||\\mathbf{a}||^2\n",
        "=\n",
        "\\mathbf{a}^\\top\\mathbf{a}\n",
        "$$\n",
        "Vous verrez souvent la magnitude d'un vecteur ecrit de cette facon.\n"
      ],
      "metadata": {
        "id": "fn3wh1ri-SKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemple"
      ],
      "metadata": {
        "id": "WgFV14Qn-knj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Soit\n",
        "$$\\mathbf{a} = \\begin{pmatrix} 1\\\\2\\\\3\\\\4 \\end{pmatrix}$$\n",
        "Alors\n",
        "$$\n",
        "||\\mathbf{a}||\n",
        "= \\sqrt{\\mathbf{a}\\cdot\\mathbf{a}}\n",
        "= \\sqrt{1 + 2^2 + 3^2 + 4^2}\n",
        "= \\sqrt{30}\n",
        "$$\n",
        "\n",
        "- Soit\n",
        "$$\\mathbf{a} = \\begin{pmatrix} 1\\\\0\\\\0 \\end{pmatrix}$$\n",
        "Alors\n",
        "$$\n",
        "||\\mathbf{a}||\n",
        "= 1\n",
        "$$\n",
        "\n",
        "- Soit\n",
        "$$\\mathbf{a} = \\begin{pmatrix} 1\\\\-1 \\end{pmatrix}$$\n",
        "Alors\n",
        "$$\n",
        "||\\mathbf{a}||\n",
        "= \\sqrt{1^2 + (-1)^2} = \\sqrt{2}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "nDaRjUbO-rs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice"
      ],
      "metadata": {
        "id": "VKeoEy6A-2IH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilisez la formule du produit scalaire pour montrer que\n",
        "$$\n",
        "||\\mathbf{a}||\n",
        "= \\sqrt{\\sum_{i=1}^n \\mathbf{a}_i^2}\n",
        "$$\n",
        "\n",
        "Ceci est une formule très importante et utile !\n",
        "Elle est parfois utilisée comme définition de la magnitude du vecteur."
      ],
      "metadata": {
        "id": "roJbfUYY-5V5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "AwAMBCMH_FIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "||\\mathbf{a}||^2 = \\mathbf{a}\\cdot\\mathbf{a} = \\sum_{i=1}^n \\mathbf{a}_i^2\n",
        "$$\n",
        "donc\n",
        "$$\n",
        "||\\mathbf{a}|| = \\sqrt{\\sum_{i=1}^n \\mathbf{a}_i^2}\n",
        "$$"
      ],
      "metadata": {
        "id": "sSgCmC3M_KTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice"
      ],
      "metadata": {
        "id": "gn7DGLtj_aYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilisez la formule ci-dessus pour montrer que si $\\mathbf{a} \\in \\mathbb{R}^n$ est le vecteur de tous les $1$, c'est-à-dire $\\mathbf{a}_i = 1$ pour $i=1, 2, \\dots, n$, alors\n",
        "$$\n",
        "||\\mathbf{a}|| = \\sqrt{n}\n",
        "$$\n",
        "\n",
        "Par exemple, cela implique que si\n",
        "$$\n",
        "\\mathbf{a}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "1\\\\1\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "alors $||\\mathbf{a}|| = \\sqrt{2}$."
      ],
      "metadata": {
        "id": "FNVff8zU_xVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "LJzH1hIs_jRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "||\\mathbf{a}||\n",
        "= \\sqrt{\\sum_{i=1}^n \\mathbf{a}_i^2}\n",
        "= \\sqrt{\\sum_{i=1}^n 1^2}\n",
        "= \\sqrt{n}\n",
        "$$"
      ],
      "metadata": {
        "id": "P251ygd4_jrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calcul de la magnitude d'un vecteur avec `jax`"
      ],
      "metadata": {
        "id": "TEj0Bsa5_8P4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "La magnitude d'un vecteur peut de calculer facilement avec `jax` en utilisant la fonction `jnp.linalg.norm`"
      ],
      "metadata": {
        "id": "1vAh0VPzAJed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.array([3, 4])\n",
        "jnp.linalg.norm(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYo1QU0lAdQ9",
        "outputId": "3bc47131-609f-442d-f6c6-fce6bed8bc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(5., dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.array([0.1, 0.5, 0.6, 50])\n",
        "jnp.linalg.norm(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm5sExTJAdV1",
        "outputId": "72b1edfa-69b1-4595-83f0-9afdb65a6a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(50.0062, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice"
      ],
      "metadata": {
        "id": "heuvk3a-AsoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choisir un vecteur $\\mathbf{a}$ et\n",
        "utilise `jax` pour verifier la formule suivante dans ce cas\n",
        "$$\n",
        "||\\mathbf{a}||\n",
        "=\n",
        "\\sqrt{\\sum_{i=1}^n \\mathbf{a}_i^2}\n",
        "$$"
      ],
      "metadata": {
        "id": "jXtcrk0dAvMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placer votre code ici"
      ],
      "metadata": {
        "id": "g9eZ00v9A9Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "m9DUwsc3BGH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.array([1, 2, 3, 4, 5]) # any old vector\n",
        "lhs = jnp.linalg.norm(a) # calculate the magnitude, left hand side of the formula\n",
        "rhs = jnp.sqrt(jnp.sum(a**2)) # calculate the right hand side of the formula\n",
        "assert jnp.allclose(lhs, rhs) # assert that the formula holds within numerical tolerance"
      ],
      "metadata": {
        "id": "Ys1mWor9BI66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propriétés de la Magnitude du Vecteur"
      ],
      "metadata": {
        "id": "a5952FVtBjWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### La magnitude d'un vecteur est toujours positive"
      ],
      "metadata": {
        "id": "K8AnmQ6cBLi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour tout $\\mathbf{a}\\in\\mathbb{R}^n$\n",
        "$$\n",
        "||\\mathbf{a}|| \\ge 0\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "jyKs2c1NBe1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Preuve"
      ],
      "metadata": {
        "id": "p5-BZYIGBwMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "D'après les propriétés du produit scalaire, nous savons que pour tout $\\mathbf{a}\\in \\mathbb{R}^n$, nous avons $\\mathbf{a}\\cdot\\mathbf{a} \\ge 0$. Ainsi,\n",
        "$$\n",
        "||\\mathbf{a}|| = \\sqrt{\\mathbf{a}\\cdot\\mathbf{a}} \\ge 0\n",
        "$$"
      ],
      "metadata": {
        "id": "jFwDZNtbByfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### La magnitude vaut $0$ si et seulement si le vecteur is $\\mathbf{0}$\n"
      ],
      "metadata": {
        "id": "wXP32XWtB9b4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Pour tout $\\mathbf{a}\\in\\mathbb{R}^n$\n",
        "$$\n",
        "||\\mathbf{a}|| = 0\n",
        "\\quad\\text{if and only if}\\quad\n",
        "\\mathbf{a}=\\mathbf{0}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "boaIg9qcCOF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Preuve"
      ],
      "metadata": {
        "id": "eGw9FGzUCRpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "D'après les propriétés du produit scalaire, nous savons que pour tout $\\mathbf{a}\\in \\mathbb{R}^n$, nous avons $\\mathbf{a}\\cdot\\mathbf{a} = 0$ si et seulement si $\\mathbf{a}=\\mathbf{0}$, puis utilisons la formule $||\\mathbf{a}||=\\mathbf{a}\\cdot\\mathbf{a}$."
      ],
      "metadata": {
        "id": "bN_UAXFVCSCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalisation d'un vecteur"
      ],
      "metadata": {
        "id": "rq8itDZaCSTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La magnitude du vecteur satisfait la propriété suivante. Pour tout $\\lambda \\in \\mathbb{R}$ et tous les vecteurs $\\mathbf{a}$, nous avons\n",
        "$$\n",
        "||\\lambda \\mathbf{a}|| = |\\lambda| \\times||\\mathbf{a}||\n",
        "$$\n",
        "Cela signifie que _n'importe quel vecteur peut être mis à l'échelle pour avoir une magnitude de 1_.\n",
        "En particulier, le vecteur\n",
        "$$\n",
        "{\\mathbf{\\hat{a}}}= \\frac{1}{||\\mathbf{a}||}\\mathbf{a}\n",
        "$$\n",
        "a toujours $||{\\mathbf{\\hat{a}}}|| = 1$.\n",
        "Un vecteur de magnitude 1 est parfois appelé un _vecteur de norme unitaire_."
      ],
      "metadata": {
        "id": "vxYXA3CdCSkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Preuve"
      ],
      "metadata": {
        "id": "yrfDrPYCCSwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align*}\n",
        "||\\lambda \\mathbf{a}||\n",
        "&=\n",
        "\\sqrt{\n",
        "  \\sum_{i=1}^n \\lambda^2 \\mathbf{a}_i^2\n",
        "}\\\\\n",
        "&=\n",
        "|\\lambda|\n",
        "\\sqrt{\n",
        "  \\sum_{i=1}^n \\mathbf{a}_i^2\n",
        "}\\\\\n",
        "&=\n",
        "|\\lambda| \\times ||\\mathbf{a}||\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "3agFJbqVCS_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inégalité triangulaire [Optionnel]"
      ],
      "metadata": {
        "id": "kk4VxRB4CTNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour tout vecteurs $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n$, nous avons\n",
        "$$\n",
        "||\\mathbf{a} + \\mathbf{b}|| \\le ||\\mathbf{a}|| + ||\\mathbf{b}||\n",
        "$$\n",
        "avec égalité si et seulement si $\\mathbf{a}$ et $\\mathbf{b}$ sont orthogonaux.\n",
        "\n",
        "La règle du triangle est une inégalité incroyablement importante."
      ],
      "metadata": {
        "id": "-mj7R_8sCTbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inégalité de Cauchy-Schwarz [Optionnel]"
      ],
      "metadata": {
        "id": "IgJYJOoxCTsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour tout vecteurs $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n$, nous avons\n",
        "$$\n",
        "|\\mathbf{a}\\cdot \\mathbf{b}| \\le ||\\mathbf{a}||||\\mathbf{b}||\n",
        "$$\n",
        "avec égalité si et seulement si $\\mathbf{a} = \\lambda \\mathbf{b}$\n",
        "pour un scalaire $\\lambda \\in \\mathbf{R}$ (c'est-à-dire que les vecteurs sont _parallèles_).\n",
        "\n",
        "L'inégalité de Cauchy-Schwarz est également une inégalité incroyablement importante."
      ],
      "metadata": {
        "id": "MMp76yXzCUEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ressources recommandées"
      ],
      "metadata": {
        "id": "XZHqox_tCUUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sections 3.1-3.4 de [Math for ML Book](https://mml-book.github.io/book/mml-book.pdf) sur les nromes et produits scalaires"
      ],
      "metadata": {
        "id": "TzAditugEIqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Géométrie de Base des Vecteurs [Optionnel] [À CONFIRMER]\n",
        "\n",
        "(pour fournir une certaine intuition géométrique sur les vecteurs...)\n",
        "\n",
        "définition de l'angle avec le produit scalaire\n",
        "\n",
        "- interprétation géométrique en 2D et 3D en termes d'angles\n",
        "- utilisation dans les projections (peut-être cela viendra plus tard ?), calcul de la\n",
        "composante d'un vecteur dans une certaine direction\n",
        "- transformations orthogonales ?\n",
        "- magnitude du vecteur en tant que longueur du vecteur dans l'espace"
      ],
      "metadata": {
        "id": "dkbrmNdaEhSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Les vecteurs en tant qu'objets géométriques"
      ],
      "metadata": {
        "id": "qeHGkxhlEhjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "À l'école, vous avez peut-être été introduit aux vecteurs en tant qu'objets géométriques représentant une ligne dirigée de l'origine à une position dans l'espace. Ces vecteurs géométriques peuvent être représentés sous forme de tableaux en exprimant leurs composantes par rapport à un système de coordonnées. Par exemple,\n",
        "$$\\begin{pmatrix} 1\\\\4\\\\2\\end{pmatrix}$$\n",
        "représente une ligne dirigée de l'origine au point de coordonnées $x=1$, $y=4$, $z=2$. Les opérations vectorielles d'addition, de soustraction et de multiplication par un scalaire ont la signification géométrique indiquée dans la figure ci-dessous.\n",
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-zIgBFg0YE3LHIMjTbnv-gXsY9wHuKggmrJ3wl9exGr4xE5Z7bo__3gypuiuSvZJw4lrRgNGMWiiNyez-A8YUivC1ns=s1600\"\n",
        "width=\"30%\" />\n",
        "\n",
        "Tous les vecteurs ne peuvent pas être pensés de cette manière, mais presque tous les cas que vous traitez en apprentissage automatique sont compatibles avec cette interprétation."
      ],
      "metadata": {
        "id": "FmPCBb_JEhuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemple : Régression Ridge"
      ],
      "metadata": {
        "id": "jKh_mMBIEiFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est temps de mettre en œuvre tous les outils que nous avons développés jusqu'à présent ! Cette section sera structurée comme suit : Dans chaque sous-section, nous illustrerons les mathématiques suivies d'un exemple pratique."
      ],
      "metadata": {
        "id": "i_vqVU3YEie_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Un peu de contexte sur la régression linéaire"
      ],
      "metadata": {
        "id": "PBhPjmT8GHCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supposons que nous ayons $n$ échantillons $\\{ x_i, y_i \\}^n_{i=1}$ où chaque $x_i= (x_{i1}, \\dots x_{ip})$ est un vecteur d'entrée avec $p$ caractéristiques et chaque $y_i \\in \\mathbb{R}$ est la cible correspondante à chaque $x_i$. Le problème de la régression linéaire consiste ensuite à approximer la valeur cible en tant que combinaison linéaire de ses caractéristiques.\n",
        "\n",
        "$$ \\eta(x_i) = \\beta_0 + \\sum_{j=1}^p x_{ij} \\beta_j $$\n",
        "\n",
        "Ici, $\\eta$ est notre modèle et ses paramètres sont les poids de régression $\\beta = (\\beta_1, \\dots, \\beta_p) \\in \\mathbb{R}^N$ et le terme de biais $\\beta_0 \\in \\mathbb{R}$.\n",
        "\n",
        "#### Quelque chose à noter ici\n",
        "\n",
        "Supposons que nous n'ayons qu'une seule caractéristique par variable d'entrée, $p=1$, et que nous essayions de la faire correspondre à une variable cible correspondante. L'équation de régression linéaire pour une seule entrée devient simplement :\n",
        "\n",
        "$$\\eta(x_i) = \\beta_0 + x \\beta_1$$\n",
        "\n",
        "En remplaçant les noms des variables $\\eta \\rightarrow y$, $\\beta_0 \\rightarrow b$ et $\\beta_1 \\rightarrow m$ ici, cela devient l'équation familière d'une ligne droite !\n",
        "\n",
        "$$ y = mx + b $$\n",
        "\n",
        "En fait, c'est exactement ce que nous essayons de faire dans la régression linéaire. Notre objectif est de trouver une 'ligne' qui correspond au mieux à nos données ! C'est bien sûr une simplification excessive car nous essayons en réalité de trouver une 'combinaison linéaire' de chaque entrée de manière à pouvoir produire la sortie correspondante. Mais l'équation de la ligne droite démontre d'où vient le terme 'linéaire' dans la régression linéaire."
      ],
      "metadata": {
        "id": "3aop3J4pEi1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### La fonction objective de régression linéaire"
      ],
      "metadata": {
        "id": "mnfusYVDEjQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour quantifier à quel point notre ligne s'adapte aux données, nous avons besoin d'une métrique. La plus courante est l'erreur quadratique moyenne, qui n'est rien d'autre que la norme euclidienne moyenne entre la valeur prédite par notre modèle et la vraie valeur cible. Cette fonction objectif $J(\\beta)$ peut être exprimée comme suit :\n",
        "\n",
        "$$ J(\\beta) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j \\right)^2 $$\n",
        "\n",
        "L'objectif est alors de trouver un ensemble de paramètres pour notre modèle qui minimiseront cette fonction objectif. Cela peut être exprimé comme suit :\n",
        "\n",
        "$$ \\underset{\\beta_0, \\beta}{\\text{arg min}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j \\right)^2 \\right\\} $$\n",
        "\n",
        "En d'autres termes, nous cherchons les valeurs de $\\beta_0$ et $\\beta$ qui rendent cette expression aussi petite que possible. Cela signifie que nous cherchons les coefficients qui minimisent l'écart entre les valeurs prédites par notre modèle et les valeurs réelles dans le jeu de données."
      ],
      "metadata": {
        "id": "uJk9ljo-Ejr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fonction objectif de la régression Ridge"
      ],
      "metadata": {
        "id": "HtrhC71ZEkAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La régression Ridge étend la notion de régression linéaire en ajoutant un terme supplémentaire à la fonction objectif que nous essayons de minimiser. Nous n'entrerons pas ici dans les détails de la raison pour laquelle cela est fait, mais le terme ajouté est le carré de la norme L2 (ou le carré de la norme euclidienne) des paramètres du modèle à l'exception du terme de biais. Notre fonction de coût devient alors :\n",
        "\n",
        "$$ J(\\beta) = \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j \\right)^2 \\right\\} + \\lambda \\sum_{j=1}^p \\beta_j^2$$\n",
        "\n",
        "L'objectif est de trouver $\\beta$ de manière à ce que :\n",
        "\n",
        "$$ \\underset{\\beta_0, \\beta}{\\text{arg min}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j \\right)^2 \\right\\} + \\lambda \\sum_{j=1}^p \\beta_j^2$$\n",
        "\n",
        "Cela revient à ajuster les paramètres $\\beta$ de manière à ce que le modèle s'adapte bien aux données tout en limitant les valeurs des coefficients grâce à la régularisation L2."
      ],
      "metadata": {
        "id": "ziabdzY9Ekba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fonction objectif de la régression Ridge en tant qu'équation matricielle"
      ],
      "metadata": {
        "id": "vOnW3ztFH12-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons jusqu'à présent exprimé notre fonction objectif de régression Ridge uniquement sous forme de sommes explicites. Vous avez peut-être remarqué que certaines de ces sommes ressemblent beaucoup à celles que nous avons vues dans la section sur les matrices. Comme nous regroupons généralement nos vecteurs d'entrée dans une matrice de conception $X$ et que nous aurons un vecteur de cibles $Y$, nous pouvons réexprimer notre fonction objectif en utilisant la notation matricielle :\n",
        "$$ \\begin{equation}\n",
        "\\begin{aligned}\n",
        "J(\\beta) &= \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j \\right)^2 \\right\\} + \\lambda \\sum_{j=1}^p \\beta_j^2 \\\\\n",
        "& = (Y - X \\beta)^T (Y - X \\beta) + \\lambda \\beta^T \\beta\\\\\n",
        "& = \\| Y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2\n",
        "\\end{aligned}\n",
        "\\end{equation} $$\n",
        "\n",
        "Ici, lorsque nous utilisons le terme $\\beta$, nous faisons référence uniquement aux paramètres du modèle, en excluant le terme de biais $\\beta_0$. Le terme de biais doit être traité avec soin."
      ],
      "metadata": {
        "id": "gx6X8RViH2Ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tache de groupe"
      ],
      "metadata": {
        "id": "aAx5KMkTH2at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discutez avec les personnes autour de vous de la manière dont nous pouvons gérer le terme de biais."
      ],
      "metadata": {
        "id": "IOHptFBwH2xS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minimisation de la fonction objectif de la régression Ridge"
      ],
      "metadata": {
        "id": "aSPsiMfFH3Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minimiser la fonction objectif de la régression Ridge\n",
        "\n",
        "Pour minimiser une fonction, nous devons annuler sa première dérivée. Nous le faisons de la manière suivante. Nous commençons par développer la fonction objectif pour qu'elle devienne :\n",
        "$$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "J(\\beta)&=(Y - X \\beta)^T (Y - X \\beta) + \\lambda \\beta^T \\beta \\\\\n",
        "J(\\beta)&=Y^T Y - Y^T X \\beta - \\beta^T X^T Y +  \\beta^T X^T X \\beta + \\lambda \\beta^T \\beta\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "\n",
        "Puisque $Y^T X \\beta$ et $\\beta^T X^T Y$ sont des valeurs scalaires, elles sont égales. Ainsi, nous pouvons simplifier ce qui précède comme suit :\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "J(\\beta)&=Y^T Y - 2 \\beta^T X^T Y +  \\beta^T X^T X \\beta + \\lambda \\beta^T \\beta\n",
        "\\end{aligned}\n",
        "\\end{equation}$$\n",
        "\n",
        "Nous prenons maintenant la dérivée de ce qui précède par rapport à $\\beta$ et l'égalons à zéro pour obtenir\n",
        "$$\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial J(\\beta)}{\\partial \\beta}  \n",
        "&= - 2 X^T Y +  2X^T X \\beta + 2\\lambda \\beta = 0\n",
        "\\end{aligned}\n",
        "\\end{equation}$$"
      ],
      "metadata": {
        "id": "gKQ5nN4FH3rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voyez si vous pouvez résoudre l'équation linéaire dérivée ci-dessus : $$ - 2 X^T Y +  2X^T X \\beta + 2\\lambda \\beta = 0 $$ pour les paramètres optimaux du modèle $\\beta$."
      ],
      "metadata": {
        "id": "ocx92H4mH4D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "IxvtT73UH4hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En partant de l'équation linéaire que nous avons dérivée :\n",
        "$$ - 2 X^T Y +  2X^T X \\beta + 2\\lambda \\beta = 0 $$\n",
        "qui peut être réarrangée comme suit :\n",
        "$$2X^T X \\beta + 2\\lambda \\beta = 2 X^T Y $$\n",
        "En divisant par 2, on obtient :\n",
        "$$X^T X \\beta + \\lambda \\beta = X^T Y $$\n",
        "Nous introduisons maintenant la matrice identité $I$ :\n",
        "$$X^T X \\beta + \\lambda I \\beta = X^T Y $$\n",
        "En regroupant les termes similaires, on obtient :\n",
        "$$(X^T X + \\lambda I) \\beta = X^T Y $$\n",
        "Comme $X^T X + \\lambda I$ est une matrice inversible, nous pouvons multiplier notre équation par l'inverse de la matrice à gauche pour obtenir :\n",
        "$$(X^T X + \\lambda I)^{-1}(X^T X + \\lambda I) \\beta =(X^T X + \\lambda I)^{-1} X^T Y $$\n",
        "Ce qui donne notre résultat pour $\\beta$ :\n",
        "$$\\beta =(X^T X + \\lambda I)^{-1} X^T Y $$"
      ],
      "metadata": {
        "id": "tvVRKigFH4_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice d'application pratique"
      ],
      "metadata": {
        "id": "YSDE_YbmH5by"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supposons que nous ayons 3 exemples avec 3 caractéristiques chacun, donnés comme suit : $x_1 = [1.2, 3.2, 0.9]$, $x_2 = [0.2, 3.1, 2.4]$ et $x_3 = [10.9, 4.0, 2.1]$. Regroupons-les dans une matrice de conception."
      ],
      "metadata": {
        "id": "66jvLfxILsDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "xTQsY9LhLcYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\mathbf{X} =\n",
        "\\begin{bmatrix}\n",
        "1.2 & 3.2 & 0.9\\\\\n",
        "0.2 & 3.1 & 2.4 \\\\\n",
        "10.9 & 4.0 & 2.1\n",
        "\\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "-Z4fzGeuLc1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice de code :\n",
        "Essayons maintenant de créer cette matrice de conception sous forme de code :"
      ],
      "metadata": {
        "id": "feXTpinZLdQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_1 = jnp.array([1.2, 3.2, 0.9])\n",
        "x_2 = jnp.array([0.2, 3.1, 2.4])\n",
        "x_3 = jnp.array([10.9, 4.0, 2.1])\n",
        "\n",
        "X = ..."
      ],
      "metadata": {
        "id": "_x6XsXr5L9Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "yP1wReioLdrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = jnp.stack([x_1, x_2, x_3])"
      ],
      "metadata": {
        "id": "d_OA-KFGMEmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice"
      ],
      "metadata": {
        "id": "2qQD4YC9LeHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant, étant donné un vecteur cible $Y = [3.95, 15.62, 130.07]$, voyez si vous pouvez résoudre l'équation que nous avons précédemment dérivée :\n",
        "$$\\beta = (X^T X + \\lambda I)^{-1}X^T Y$$\n",
        "pour obtenir les paramètres optimaux du modèle $\\beta$ afin d'ajuster la fonction en utilisant JAX. Pour cet exemple, utilisez une valeur $\\lambda$ de 0.01."
      ],
      "metadata": {
        "id": "Gn7V2HDKMZTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = jnp.array([3.95, 15.62,130.07])\n",
        "lambda_ = 0.01\n",
        "X_T = ...\n",
        "I = ...\n",
        "\n",
        "beta = ..."
      ],
      "metadata": {
        "id": "pAqZWxAzMzBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "xXskvco7M3I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute X transpose\n",
        "X_T = X.T\n",
        "# Create an identity matrix with the correct shape.\n",
        "# Since we have 3 examples, with 3 features each calling `X.shape` will\n",
        "# yield a tuple of shape (3, 3). Hence we can construct the identity matrix\n",
        "# by using the first dimension of `X.shape`\n",
        "I = np.eye(X.shape[0])\n",
        "\n",
        "beta = jnp.linalg.inv(X_T @ X + lambda_ * I) @ (X_T @ Y)"
      ],
      "metadata": {
        "id": "0FDHHKQVM6Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algèbre Linéaire II <font color='orange'>`Intermédiaire`</font>\n",
        "\n",
        "<!--\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-wdkXm3HR6ycdwKX5C7Mu2pvqHXSqXDzHn8rN_F4xWXRANc-5Upq9A9mjBEL3vyYA-VcWZ-9oyb35Pq1zFkuVD5kK49=s1600\"\n",
        "width=\"100%\" /> -->\n",
        "\n",
        "Imaginez-vous face à un dédale de données - d'innombrables points qui défient une compréhension facile. La réduction de la dimensionnalité est votre carte pour naviguer dans ce labyrinthe. C'est comme regarder un grand tableau à travers un prisme, capturant son essence en se concentrant sur les couleurs et les formes clés.\n",
        "\n",
        "Avec des techniques comme l'ACP, nous réduisons des données complexes en dimensions essentielles, simplifiant tout en préservant les motifs. Pensez à un ensemble de données sur les fruits : la réduction de la dimensionnalité pourrait révéler que la couleur, la taille et la douceur capturent la plupart des variations.\n",
        "\n",
        "`Projections`, `déterminants et traces`, `décomposition en valeurs propres` et `ACP` tiennent chacun une pièce du puzzle. Les projections condensent les données, les déterminants dévoilent les transformations, la décomposition en valeurs propres révèle les structures, et l'ACP réunit ces idées."
      ],
      "metadata": {
        "id": "cKr40fDn-LU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Projections**\n"
      ],
      "metadata": {
        "id": "evBjpgHzYVoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intuition de la Projection**\n",
        "\n",
        "> Imaginez que vous ayez une lampe de poche qui ne peut éclairer que dans une direction. Maintenant, disons que vous avez un faisceau de lumière (que nous appellerons un vecteur) et un mur (notre sous-espace). Vous voulez diriger la lumière vers le mur de manière à ce qu'elle soit aussi proche que possible du mur. C'est là que réside l'essence de la projection !\n",
        "1. **Obtenir le Plus Proche :** Lorsque vous voulez placer le faisceau lumineux aussi près que possible du mur, vous devez minimiser la distance qui le sépare du mur. Nous mesurons cette distance en utilisant une règle spéciale.\n",
        "2. **Convivialité Orthogonale :** Le faisceau se rapproche beaucoup lorsque le chemin qu'il emprunte (faisceau moins l'ombre du mur) forme un angle droit avec le mur. C'est comme si vous teniez la lampe de poche juste à côté du mur, sans laisser la lumière le toucher directement.\n",
        "3. **Langage Mathématique :** Lorsque nous parlons en chiffres, cela signifie que le chemin de la lumière (faisceau moins l'ombre) n'entraîne pas de déplacement du mur. En termes mathématiques, c'est comme si l'on multipliait le faisceau par un nombre (λ) de sorte qu'il s'adapte parfaitement au mur.\n",
        "4. **Création d'un Outil :** Pour nous aider à réaliser cela pour n'importe quel faisceau lumineux (vecteur) et n'importe quel mur (sous-espace), nous créons un outil spécial appelé la \"matrice de projection\". Cet outil prend n'importe quel faisceau lumineux et l'ajuste pour qu'il soit très proche du mur, sans changer sa direction.\n",
        ">\n",
        "> Ainsi, la projection revient à tenir votre lampe de poche de manière à ce qu'elle éclaire aussi près que possible du mur, tout en respectant les règles du mur. Cela nous aide à comprendre comment les vecteurs et les espaces interagissent de manière organisée et élégante.\n",
        "\n",
        "**Projection Mathématiquement**\n",
        "\n",
        "> ***\n",
        "> <font color='Yellow'>`DÉFINITION`</font> `Projection`\n",
        ">\n",
        "> Soit $V$ un espace vectoriel et $U \\subseteq V$ un sous-espace de $V$.\n",
        ">\n",
        "> - Une application linéaire $\\pi : V \\rightarrow U$ est appelée une projection si $\\pi^2 := \\pi \\circ \\pi = \\pi$.\n",
        "> - $\\mathbf{P}_\\pi$ est une matrice de projection si $\\mathbf{P}_\\pi^2 = \\mathbf{P}_\\pi$.\n",
        "> ***"
      ],
      "metadata": {
        "id": "TxdbgqUEYkq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche de Groupe :**\n",
        "\n",
        "Discutez en groupe de ce que cette définition signifie et pourquoi elle a du sens, en gardant à l'esprit l'intuition donnée.\n",
        "\n",
        ">Indice : La notation $\\pi^2$ ou $\\pi \\circ \\pi$ signifie appliquer la fonction $\\pi$ deux fois (c'est-à-dire $\\pi^2 := \\pi \\circ \\pi := \\pi(\\pi(x))$)."
      ],
      "metadata": {
        "id": "abmr0JykYw5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Projection sur les Sous-espaces Unidimensionnels (Ligne, $\\mathbb{R}^1$) - <font color='green'>`Débutant`</font>"
      ],
      "metadata": {
        "id": "dYmvP07nZB1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-yth8ydsmqZyqNZBNDktk9plo8525uElAtDf3KTt3MT_uCh8FPGp0z_yDiY2qZj3Y_QowAxvyP5_vVNkb3X-gw4ifs-GA=s1600\"\n",
        "width=\"60%\" />\n",
        "\n",
        "La projection $\\pi_U(\\mathbf{x})$ est la plus proche de $\\mathbf{x}$ lorsque la distance $\\left\\|\\mathbf{x}-\\pi_U(\\mathbf{x})\\right\\|$ entre eux est minimale. Cela se produit lorsque la différence $\\pi_U(\\mathbf{x})-\\mathbf{x}$ est perpendiculaire (orthogonale) au sous-espace $U$. Cette perpendicularité est évidente dans le fait que leur produit scalaire $\\left\\langle\\pi_U(\\mathbf{x})-\\mathbf{x}, \\mathbf{b}\\right\\rangle$ est nul.\n",
        "\n",
        "Étant donné que la projection $\\pi_U(\\mathbf{x})$ mappe $\\mathbf{x}$ dans le sous-espace $U$, elle peut être représentée comme un multiple scalaire du vecteur de base $\\mathbf{b}$, noté $\\lambda \\mathbf{b}$, où $\\lambda$ est un nombre réel.\n",
        "\n",
        "Ainsi, pour construire la matrice de projection $\\mathbf{P}_\\pi$ qui mappe n'importe quel $\\mathbf{x} \\in \\mathbb{R}^n$ sur $U$, les étapes suivantes sont utilisées :\n",
        "- Déterminer la coordonnée $\\lambda$\n",
        "- Déterminer $\\pi_U(\\mathbf{x}) \\in U$\n",
        "- Enfin, construire $\\mathbf{P}_\\pi$\n",
        "\n",
        "> Trouver la coordonnée $\\lambda$\n",
        "- En utilisant la condition d'orthogonalité, nous avons\n",
        "$$\n",
        "\\begin{aligned}\n",
        "0 & =\\left\\langle\\mathbf{x}-\\pi_u(\\mathbf{x}), \\mathbf{b}\\right\\rangle \\\\\n",
        "& =\\langle\\mathbf{x}-\\lambda \\mathbf{b}, \\mathbf{b}\\rangle \\\\\n",
        "& =\\langle\\mathbf{x}, \\mathbf{b}\\rangle-\\lambda\\langle\\mathbf{b}, \\mathbf{b}\\rangle \\text { d'après la bilinéarité de }\\langle\\cdot, \\cdot\\rangle\n",
        "\\end{aligned}\n",
        "$$\n",
        "- Nous pouvons réorganiser pour obtenir\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\lambda & =\\frac{\\langle\\mathbf{x}, \\mathbf{b}\\rangle}{\\langle\\mathbf{b}, \\mathbf{b}\\rangle}=\\frac{\\langle\\mathbf{b}, \\mathbf{x}\\rangle}{\\|\\mathbf{b}\\|^2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "- En termes plus simples, le coefficient $\\lambda$ est trouvé en divisant le produit scalaire de $\\mathbf{x}$ et $\\mathbf{b}$ par le produit scalaire de $\\mathbf{b}$ avec lui-même, ce qui est équivalent à la longueur au carré de $\\mathbf{b}$.\n",
        "\n",
        "> Trouver le point de projection $\\pi_U(\\mathbf{x}) \\in U$\n",
        "- Étant donné que $\\pi_U(\\mathbf{x})=\\lambda \\mathbf{b}$, il en découle immédiatement que\n",
        "$$\n",
        "\\pi_U(\\mathbf{x})=\\lambda \\mathbf{b}=\\frac{\\langle\\mathbf{b}, \\mathbf{x}\\rangle}{\\|\\mathbf{b}\\|^2} \\mathbf{b}\\\\\n",
        "\\quad\\quad\\quad\\quad~ =\\frac{\\mathbf{b}^T \\mathbf{x}}{\\|\\mathbf{b}\\|^2} \\mathbf{b} \\quad \\text{(en supposant le produit scalaire)}\n",
        "$$\n",
        "\n",
        "> Trouver la matrice de projection $\\mathbf{P}_\\pi$\n",
        "- En utilisant le produit scalaire, nous pouvons faire l'observation suivante\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\pi_U(\\mathbf{x})=\\lambda \\mathbf{b} & =\\mathbf{b} \\lambda \\\\\n",
        "& =\\mathbf{b} \\frac{\\mathbf{b}^T \\mathbf{x}}{\\|\\mathbf{b}\\|^2} \\\\\n",
        "& =\\frac{\\mathbf{b} \\mathbf{b}^T}{\\|\\mathbf{b}\\|^2} \\mathbf{x}\n",
        "\\end{aligned}\n",
        "$$\n",
        "Rappelez-vous que $\\mathbf{b}$ est une matrice $n \\times 1$ et $\\mathbf{b}^T$ est une matrice $1 \\times n$, donc $\\mathbf{b b}^T$ est une matrice $n \\times n$ (et est également symétrique). Nous appelons $\\mathbf{b b}^T$ un \"produit extérieur\" (similaire au produit intérieur $\\mathbf{b^T b}$).\n",
        "- Il s'ensuit que\n",
        "$$\n",
        "\\mathbf{P}_\\pi=\\frac{\\mathbf{b} \\mathbf{b}^T}{\\|\\mathbf{b}\\|^2}\n",
        "$$\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`EXEMPLE`</font>\n",
        "\n",
        "Prenons l'exemple que nous avons donné précédemment où nous voulons projeter un point en 2D $x=(64,17.76)$ sur la ligne engendrée par le vecteur de base\n",
        "$b = \\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "0.41\n",
        "\\end{array}\\right]$.\n",
        "\n",
        "Matrice de projection :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\boldsymbol{P}_\\pi=\\frac{\\boldsymbol{b} b^T}{\\|b\\|^2}=\\frac{1}{\\boldsymbol{b}^T \\boldsymbol{b}} \\boldsymbol{b} \\boldsymbol{b}^{\\boldsymbol{T}} \\\\\n",
        "& \\boldsymbol{P}_\\pi=\\frac{1}{\\left[\\begin{array}{ll}\n",
        "1 & 0.41\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "0.41\n",
        "\\end{array}\\right]}\\left(\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "0.41\n",
        "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
        "1 & 0.41\n",
        "\\end{array}\\right]\\right) \\\\\n",
        "& \\boldsymbol{P}_\\pi=\\frac{1}{1.17}\\left(\\left[\\begin{array}{cc}\n",
        "1 & 0.41 \\\\\n",
        "0.41 & 0.17\n",
        "\\end{array}\\right]\\right) \\\\\n",
        "& \\boldsymbol{P}_\\pi=\\left[\\begin{array}{ll}\n",
        "0.85 & 0.35 \\\\\n",
        "0.35 & 0.15\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "Point projeté :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\pi_U(x)=\\boldsymbol{P}_\\pi x=\\left[\\begin{array}{ll}\n",
        "0.85 & 0.35 \\\\\n",
        "0.35 & 0.15\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "64 \\\\\n",
        "17.76\n",
        "\\end{array}\\right] & =\\left[\\begin{array}{c}\n",
        "60.62 \\\\\n",
        "25.06\n",
        "\\end{array}\\right] \\\\\n",
        "& =60.62\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "0.41\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "YNwMnI5QZn9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche Mathématique :**\n",
        "\n",
        "Dans un projet d'apprentissage automatique, un data scientist souhaite réduire la dimensionnalité de son ensemble de données en projetant chaque point de données sur un sous-espace euclidien engendré par un ensemble de vecteurs de base. Supposons qu'un point de données $x=\\left[\\begin{array}{c}-1 \\\\ 3 \\\\ 1\\end{array}\\right]$ soit donné, et que le sous-espace $U$ soit engendré par le vecteur de base $b=\\left[\\begin{array}{c}0 \\\\ -2 \\\\ 3\\end{array}\\right]$.\n",
        "\n",
        "a) Déterminez la projection orthogonale $\\pi_U(x)$ de $\\mathrm{x}$ sur $U$\n",
        "\n",
        "b) Déterminez la distance $d(x, U)$."
      ],
      "metadata": {
        "id": "zEYZOSzgbCtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien sûr, je peux vous aider à résoudre ce problème étape par étape.\n",
        "\n",
        "a) Pour déterminer la projection orthogonale $\\pi_U(x)$ du vecteur $x$ sur le sous-espace $U$ engendré par les vecteurs de base $u_1$ et $u_2$, nous suivons le même processus qu'auparavant.\n",
        "\n",
        "Matrice de projection :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\boldsymbol{P}_\\pi=\\frac{\\boldsymbol{b} b^T}{\\|b\\|^2}=\\frac{1}{\\boldsymbol{b}^T \\boldsymbol{b}} \\boldsymbol{b} \\boldsymbol{b}^{\\boldsymbol{T}} \\\\\n",
        "& \\boldsymbol{P}_\\pi=\\frac{1}{\\left[\\begin{array}{lll}\n",
        "0 & -2 & 3\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "0 \\\\\n",
        "-2 \\\\\n",
        "3\n",
        "\\end{array}\\right]}\\left(\\left[\\begin{array}{c}\n",
        "0 \\\\\n",
        "-2 \\\\\n",
        "3\n",
        "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
        "0 & -2 & 3\n",
        "\\end{array}\\right]\\right) \\\\\n",
        "& \\boldsymbol{P}_\\pi=\\frac{1}{13}\\left(\\left[\\begin{array}{ccc}\n",
        "0 & 0 & 0 \\\\\n",
        "0 & 4 & -6 \\\\\n",
        "0 & -6 & 9\n",
        "\\end{array}\\right]\\right) \\\\\n",
        "& \\boldsymbol{P}_\\pi=\\left[\\begin{array}{lll}\n",
        "0 & 0 & 0 \\\\\n",
        "0 & \\frac{4}{13} & \\frac{-6}{13} \\\\\n",
        "0 & \\frac{-6}{13} & \\frac{9}{13}\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "Point projeté :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\pi_U(x)=\\boldsymbol{P}_\\pi x=\\left[\\begin{array}{lll}\n",
        "0 & 0 & 0 \\\\\n",
        "0 & \\frac{4}{13} & \\frac{-6}{13} \\\\\n",
        "0 & \\frac{-6}{13} & \\frac{9}{13}\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "-1 \\\\\n",
        "3 \\\\\n",
        "1\n",
        "\\end{array}\\right] & =\\left[\\begin{array}{c}\n",
        "0 \\\\\n",
        "\\frac{6}{13} \\\\\n",
        "\\frac{-9}{13}\n",
        "\\end{array}\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Ainsi, la projection orthogonale $\\pi_U(x)$ du vecteur $x$ sur le sous-espace $U$ est $\\left[\\begin{array}{c}\n",
        "0 \\\\\n",
        "\\frac{6}{13} \\\\\n",
        "\\frac{-9}{13}\n",
        "\\end{array}\\right]$.\n",
        "\n",
        "b) Pour déterminer la distance $d(x, U)$ entre le vecteur $x$ et le sous-espace $U$, nous pouvons utiliser la formule :\n",
        "\n",
        "$$d(x, U) = \\|x - \\pi_U(x)\\|$$\n",
        "\n",
        "Où $\\|\\cdot\\|$ représente la norme euclidienne (magnitude) d'un vecteur.\n",
        "\n",
        "Tout d'abord, calculez la différence entre $x$ et sa projection :\n",
        "\n",
        "$$x - \\pi_U(x) = \\left[\\begin{array}{c}-1 \\\\ 3 \\\\ 1\\end{array}\\right] - \\left[\\begin{array}{c}0 \\\\ \\frac{6}{13} \\\\ \\frac{-9}{13}\\end{array}\\right] = \\left[\\begin{array}{c}-1 \\\\ \\frac{33}{13} \\\\ \\frac{4}{13}\\end{array}\\right]$$\n",
        "\n",
        "Ensuite, calculez la norme euclidienne de cette différence :\n",
        "\n",
        "$$\\|x - \\pi_U(x)\\| = \\sqrt{(-1)^2 + \\left(\\frac{33}{13}\\right)^2 + \\left(\\frac{4}{13}\\right)^2} = 2.75$$\n",
        "\n",
        "Ainsi, la distance entre le vecteur $x$ et le sous-espace $U$ est d'environ 3."
      ],
      "metadata": {
        "id": "tad6JbvQbOxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Projection sur des sous-espaces généraux ($\\mathbb{R}^n$) - <font color='orange'>Intermédiaire</font>"
      ],
      "metadata": {
        "id": "5wdq-POzb7a_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-zEGASdI7YZJ-iaHSysatou2W2FwdIochnElA5cNw6QHlP2vElrjCh86dAd-__wHiwd_dfRZUFh3DuhOoQjtxrsNXxJNQ=s1600\"\n",
        "width=\"60%\" />\n",
        "\n",
        "Assumez que $\\left(\\mathbf{b}_1, \\ldots, \\mathbf{b}_m\\right)$ est une base ordonnée de l'espace $U$\n",
        "- Nous savons que pour tout $\\mathbf{x} \\in \\mathbb{R}^n$, $\\pi_U(\\mathbf{x}) \\in U$,\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\pi_U(\\mathbf{x})=\\sum_{i=1}^m \\lambda_i \\mathbf{b}_i=\\mathbf{B} \\boldsymbol{\\lambda}, \\\\\n",
        "& \\mathbf{B}=\\left[\\mathbf{b}_1, \\ldots, \\mathbf{b}_m\\right] \\in \\mathbb{R}^{n \\times m}, \\lambda=\\left[\\lambda_1, \\ldots, \\lambda_m\\right]^T \\in \\mathbb{R}^m\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Ainsi, nous pouvons suivre la même procédure qu'auparavant pour trouver la matrice de projection, généralisée aux sous-espaces de dimension $n$. Cela signifie,\n",
        "- Déterminer la coordonnée $\\lambda$\n",
        "- Déterminer $\\pi_U(\\mathbf{x}) \\in U$\n",
        "- Enfin construire $\\mathbf{P}_\\pi$.\n",
        "\n",
        "> Trouver la coordonnée $\\lambda$\n",
        "- À partir de la condition d'orthogonalité, nous avons que\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{b}_1^T(\\mathbf{x}-\\mathbf{B} \\boldsymbol{\\lambda}) & =0 \\\\\n",
        "\\vdots & \\\\\n",
        "\\mathbf{b}_m^T(\\mathbf{x}-\\mathbf{B} \\boldsymbol{\\lambda}) & =0\n",
        "\\end{aligned}\n",
        "$$\n",
        "forme un système d'équations linéaires homogènes. Par conséquent,\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& {\\left[\\begin{array}{c}\n",
        "\\mathbf{b}_1^T \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{b}_m^T\n",
        "\\end{array}\\right][\\mathbf{x}-\\mathbf{B} \\boldsymbol{\\lambda}]=\\mathbf{0} } \\\\\n",
        "\\Longleftrightarrow & \\mathbf{B}^T(\\mathbf{x}-\\mathbf{B} \\boldsymbol{\\lambda})=\\mathbf{0} \\\\\n",
        "\\Longleftrightarrow & \\mathbf{B}^T \\mathbf{B} \\boldsymbol{\\lambda}=\\mathbf{B}^T \\mathbf{x} \\\\\n",
        "\\lambda & =\\left(\\mathbf{B}^T \\mathbf{B}\\right)^{-1} \\mathbf{B}^T \\mathbf{x}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "> Trouver le point de projection $\\pi_U(\\mathbf{x}) \\in U$ :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\pi_U(\\mathbf{x})=\\sum_{i=1}^m \\lambda_i \\mathbf{b}_i & =\\mathbf{B} \\boldsymbol{\\lambda} \\\\\n",
        "& =\\mathbf{B}\\left(\\mathbf{B}^T \\mathbf{B}\\right)^{-1} \\mathbf{B}^T \\mathbf{x}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "> Trouver la matrice de projection $\\mathbf{P}_\\pi$ :\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\mathbf{P}_\\pi \\mathbf{x}=\\pi_U(\\mathbf{x}) \\\\\n",
        "\\mathbf{P}_\\pi=\\mathbf{B}\\left(\\mathbf{B}^T \\mathbf{B}\\right)^{-1} \\mathbf{B}^T\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "Cette capacité à effectuer des projections nous offre une perspective intéressante pour résoudre des systèmes d'équations linéaires $Ax=b$.\n",
        "- Si $\\boldsymbol{A}$ est inversible :\n",
        " - $\\mathrm{x}=\\mathrm{A}^{-1} \\mathrm{~b}$\n",
        "- Si $\\mathbf{A}$ n'est pas inversible :\n",
        " - $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A x}=\\mathbf{A}^{\\mathrm{T}} \\mathrm{b}$\n",
        " - $\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{-1} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A x}=\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{-\\mathbf{1}} \\mathbf{A}^{\\mathrm{T}} \\mathrm{b}$\n",
        " - $\\mathrm{x}=\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{-\\mathbf{1}} \\mathbf{A}^{\\mathrm{T}} \\mathrm{b}$ sous des hypothèses raisonnables.\n",
        "- $\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{-\\mathbf{1}} \\mathbf{A}^{\\mathrm{T}}$ est appelée pseudo-inverse de Moore-Penrose et $\\mathrm{x}$ est la meilleure solution approximative.\n",
        "- Perspective de projection : $\\boldsymbol{\\lambda}=\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{-\\mathbf{1}} \\mathbf{A}^{\\mathrm{T}} \\mathrm{b}$ est la solution des moindres carrés du système."
      ],
      "metadata": {
        "id": "N10LeIP8b9nG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche de code :**"
      ],
      "metadata": {
        "id": "pNXeUtK8c7MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def projection_matrix(B):\n",
        "    \"\"\"Compute the projection matrix onto the space spanned by `B`\n",
        "    Args:\n",
        "        B: ndarray of dimension (D, M), the basis for the subspace\n",
        "\n",
        "    Returns:\n",
        "        P: the projection matrix\n",
        "    \"\"\"\n",
        "    return np.eye(B.shape[0]) # <-- EDIT THIS to compute the projection matrix\n"
      ],
      "metadata": {
        "id": "FhHoyqZq2lko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exécutez-moi pour tester votre code.\n",
        "\n",
        "def test_projection_matrix(B):\n",
        "  assert (projection_matrix(B) == (B @ np.linalg.inv(B.T @ B) @ B.T)).all(), \"The projection matrix is incorrect\"\n",
        "  print(\"Nice! Your answer looks correct.\")\n",
        "\n",
        "B = np.array([[1,2,3],[4,5,6]])\n",
        "test_projection_matrix(B)"
      ],
      "metadata": {
        "id": "B2PN3zBDdgXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Réponse à la tâche de code (Essayez de ne pas jeter un coup d'œil avant d'avoir essayé sérieusement !)\n",
        "\n",
        "def projection_matrix(B):\n",
        "    \"\"\"Compute the projection matrix onto the space spanned by `B`\n",
        "    Args:\n",
        "        B: ndarray of dimension (D, M), the basis for the subspace\n",
        "\n",
        "    Returns:\n",
        "        P: the projection matrix\n",
        "    \"\"\"\n",
        "    return (B @ np.linalg.inv(B.T @ B) @ B.T)\n",
        "\n",
        "test_projection_matrix(B)"
      ],
      "metadata": {
        "id": "QwvVjXi-d2rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Base Orthonormale - <font color='green'>`Avancé`</font>"
      ],
      "metadata": {
        "id": "08CofsKeeLg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En utilisant notre nouvelle capacité à projeter des vecteurs sur des sous-espaces, nous pouvons transformer n'importe quelle base (ensemble de vecteurs de base) en un ensemble de vecteurs de base orthonormaux.\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`DÉFINITION`</font> `Base Orthonormale`\n",
        "\n",
        "Considérons un espace vectoriel $V$ de dimension $n$ et une base $\\left\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\right\\}$ de $V$. Si\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\left\\langle\\mathbf{b}_i, \\mathbf{b}_j\\right\\rangle=0 \\text { pour } i \\neq j \\\\\n",
        "& \\left\\langle\\mathbf{b}_i, \\mathbf{b}_j\\right\\rangle=1 \\text { pour } i=j\n",
        "\\end{aligned}\n",
        "$$\n",
        "est vrai pour tout $i, j=1, \\ldots, n$, alors la base est appelée une base orthonormale (BON).\n",
        "***\n",
        "\n",
        "Elles sont très utiles dans plusieurs situations. Par exemple, si nous projetons des vecteurs en utilisant une base orthonormale pour $U$, alors\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\pi_U(\\mathbf{x}) & =\\mathbf{B} \\boldsymbol{\\lambda} \\\\\n",
        "& =\\mathbf{B}\\left(\\mathbf{B}^T \\mathbf{B}\\right)^{-1} \\mathbf{B}^T \\mathbf{x} \\\\\n",
        "& =\\mathbf{B I}^{-1} \\mathbf{B}^T \\mathbf{x} \\\\\n",
        "& =\\mathbf{B} \\mathbf{B}^T \\mathbf{x}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "ce qui nécessite beaucoup moins de calculs.\n",
        "\n",
        "Maintenant, comment obtenir une base orthonormale ? Une approche populaire est l'`orthogonalisation de Gram-Schmidt`. Elle permet d'obtenir une base orthogonale $\\left(\\mathbf{u}_1, \\ldots, \\mathbf{u}_n\\right)$ à partir de n'importe quelle base $\\left(\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\right)$ de $V$ comme suit :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\mathbf{u}_1:=\\mathbf{b}_1 \\\\\n",
        "& \\mathbf{u}_k:=\\mathbf{b}_k-\\pi_{\\text {span }\\left[\\mathbf{u}_1, \\ldots, \\mathbf{u}_{k-1}\\right]}\\left(\\mathbf{b}_k\\right), k=2, \\ldots, n\n",
        "\\end{aligned}\n",
        "$$\n",
        "Si nous allons plus loin et utilisons plutôt $\\hat{\\mathbf{u}}_k=\\frac{\\mathbf{u}_k}{\\left\\|\\mathbf{u}_k\\right\\|}$ nous obtenons une base orthonormale !\n",
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-xuvxK1QVMSvjcRLoPDShh8MIBV4lpEug8HedPtJvbYfrnIKjgN0e5CthxgJLEmwAEJMoUInVAHt_2vfYForMSojggu8w=s1600\"\n",
        "width=\"60%\" />\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`EXEMPLE`</font>\n",
        "\n",
        "Considérons une base $\\left(\\boldsymbol{b}_1, \\boldsymbol{b}_2\\right)$ de $\\mathbb{R}^2$, où\n",
        "$$\n",
        "\\boldsymbol{b}_1=\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "0\n",
        "\\end{array}\\right], \\quad \\boldsymbol{b}_2=\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "1\n",
        "\\end{array}\\right].\n",
        "$$\n",
        "Alors\n",
        "$$\n",
        "\\boldsymbol{u}_1:=\\boldsymbol{b}_1=\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "0\n",
        "\\end{array}\\right], \\\\\n",
        "\\boldsymbol{u}_2:=\\boldsymbol{b}_2-\\pi_{\\operatorname{span}\\left[\\boldsymbol{u}_1\\right]}\\left(\\boldsymbol{b}_2\\right)=\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "1\n",
        "\\end{array}\\right]-\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "1\n",
        "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "Vérifiez que $u_1$ et $u_2$ sont effectivement orthogonaux en vérifiant si leur produit scalaire est nul (ce qui signifie que leur angle est de 90 degrés). C'est-à-dire, si $u_1^Tu_2=0$.\n",
        "***"
      ],
      "metadata": {
        "id": "stFyKluPePFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche mathématique :**\n",
        "\n",
        "Dans un projet financier, un data scientist souhaite construire un portefeuille d'actions qui minimise les risques et maximise les rendements. Pour ce faire, le data scientist peut utiliser une technique appelée analyse en composantes principales, qui implique de projeter les rendements des actions individuelles sur une base orthogonale. Supposons que les rendements de 4 actions soient représentés par la base de vecteurs linéairement indépendants $b_1=\\left[\\begin{array}{c}3 \\\\ 6 \\\\ -1 \\\\ -3\\end{array}\\right]$ et $b_2=\\left[\\begin{array}{c}-2 \\\\ -3 \\\\ -4 \\\\ 6\\end{array}\\right]$. Utilisez la méthode de Gram-Schmidt pour convertir cette base en une base orthogonale $\\left(c_1, c_2\\right)$.\n"
      ],
      "metadata": {
        "id": "6oBxaXJxeoiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Réponse :**\n",
        "\n",
        "**1. Trouver $c_1$ :**\n",
        "$c_1$ est simplement $b_1$, donc nous pouvons le conserver tel quel :\n",
        "$$c_1 = \\begin{bmatrix} 3 \\\\ 6 \\\\ -1 \\\\ -3 \\end{bmatrix}$$\n",
        "\n",
        "**2. Trouver la projection orthogonale de $b_2$ sur $c_1$ :**\n",
        "La projection orthogonale de $b_2$ sur $c_1$ peut être calculée comme suit :\n",
        "$$c_2 = b_2 - \\frac{b_2 \\cdot c_1}{c_1 \\cdot c_1} c_1 = b_2 - \\frac{b_2^T c_1}{c_1^T c_1} c_1$$\n",
        "\n",
        "En substituant les valeurs, en calculant les produits scalaires et en simplifiant :\n",
        "$$c_2 = \\left[\\begin{array}{c}\n",
        "\\frac{4}{55} \\\\\n",
        "\\frac{63}{55} \\\\\n",
        "-\\frac{258}{55} \\\\\n",
        "\\frac{216}{55}\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "Ainsi, la base orthogonale est $\\{c_1, c_2\\}$ :\n",
        "$$c_1 = \\begin{bmatrix} 3 \\\\ 6 \\\\ -1 \\\\ -3 \\end{bmatrix}, \\quad c_2 = \\left[\\begin{array}{c}\n",
        "\\frac{4}{55} \\\\\n",
        "\\frac{63}{55} \\\\\n",
        "-\\frac{258}{55} \\\\\n",
        "\\frac{216}{55}\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "Note : Les vecteurs $c_1$ et $c_2$ ne sont pas normalisés dans cette approche."
      ],
      "metadata": {
        "id": "NeK7pDC_ezUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Déterminants et Traces**"
      ],
      "metadata": {
        "id": "6a0W1OPAfVrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imaginez que vous êtes un jardinier occupé à entretenir une paire de parterres de fleurs. Vous avez deux types de fleurs, des roses et des tulipes, et vous voulez comprendre comment la croissance des fleurs est influencée par la lumière du soleil et l'eau. Vous enregistrez les données suivantes dans une matrice :\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-yYjQcoB7PLHmdOmhIAEZj4UZP40ZeCB8bJb1RQ_-PJRbm6vkdiVMNqC_BQX0vscvE1EHOXgolBdZDoBIzW8tXQqGxoTA=s1600\"\n",
        "width=\"60%\" />\n",
        "\n",
        "Dans ce contexte de jardinage, le déterminant de la matrice pourrait représenter une mesure de la santé globale et de la vitalité des parterres de fleurs. Une valeur de déterminant positive, comme 10, pourrait signifier que à la fois la lumière du soleil et l'eau contribuent positivement à la croissance à la fois des roses et des tulipes, ce qui se traduit par des parterres de fleurs florissants. En revanche, une valeur de déterminant négative comme -10 pourrait indiquer un déséquilibre dans les facteurs de croissance, ce qui pourrait conduire à une croissance non saine ou entravée des fleurs.\n",
        "\n",
        "Le déterminant de cette matrice est -1."
      ],
      "metadata": {
        "id": "-8mGE9umfYV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Déterminant - <font color='blue'>`Débutant`</font>"
      ],
      "metadata": {
        "id": "ddpwOOL5frAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Géométriquement, le déterminant d'une matrice représente le facteur d'échelle par lequel la matrice transforme la surface (ou le volume dans des dimensions supérieures) d'une forme. Il indique dans quelle mesure la forme est étirée ou comprimée lors de la transformation. Si le déterminant est positif, la forme se dilate ; s'il est négatif, la forme s'inverse et se dilate ; et s'il est nul, la transformation réduit la forme dans des dimensions inférieures.\n",
        "\n",
        "Supposons que nous soyons dans l'espace en 2D ($n=2$), et nous cherchons le déterminant de la matrice\n",
        "$\\boldsymbol{A}=\\left[\\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{array}\\right]$.\n",
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-xIyfDYWTJ0bK2pBIvUTmEPNuceXe4Z89vck6wKY0kgq_lhckkHpuETS9qfXIlxwghpFnBaprLVf-bZs2SLWj3juz0X5Q=s1600\"\n",
        "width=\"40%\" />\n",
        "\n",
        "Alors, $\\operatorname{det}(\\boldsymbol{A})=\\left|\\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{array}\\right|=a_{11} a_{22}-a_{12} a_{21}$\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`EXEMPLE`</font>\n",
        "\n",
        "$\\begin{aligned} \\operatorname{det}(\\boldsymbol{A})=\\left|\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right| =(1*4-2*3) =(4-6) =-2\\end{aligned}$\n",
        "\n",
        "$\\begin{aligned} \\operatorname{det}(\\boldsymbol{A})=\\left|\\begin{array}{ll}1 & 0 \\\\ 3 & 4\\end{array}\\right| =(1*4-0*3) =(4-0) =4\\end{aligned}$\n",
        "\n",
        "$\\begin{aligned} \\operatorname{det}(\\boldsymbol{A})=\\left|\\begin{array}{ll}1 & 2 \\\\ 3 & 0\\end{array}\\right| =(1*0-2*3) =(1-6) =-6\\end{aligned}$\n",
        "***\n",
        "\n",
        "En général,\n",
        "\n",
        "\n",
        "$\\begin{aligned} \\text{ Si } n=1,& \\\\ & \\operatorname{det}(A)=\\left|a_{11}\\right|=a_{11}\\end{aligned}$\n",
        "\n",
        "$\\begin{aligned} \\text{ Si } n=2,& \\\\ & \\operatorname{det}(\\boldsymbol{A})=\\left|\\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{array}\\right|=a_{11} a_{22}-a_{12} a_{21}\\end{aligned}$\n",
        "\n",
        "$\\begin{aligned} \\text{ Si } n=3,& \\\\ & \\operatorname{det}(\\boldsymbol{A})=\\left|\\begin{array}{lll}a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33}\\end{array}\\right| \\\\ & =(-1)^{1+1} a_{11}\\left|\\begin{array}{ll}a_{22} & a_{23} \\\\ a_{32} & a_{33}\\end{array}\\right|+(-1)^{1+2} a_{12}\\left|\\begin{array}{ll}a_{21} & a_{23} \\\\ a_{31} & a_{33}\\end{array}\\right|+(-1)^{1+3} a_{13}\\left|\\begin{array}{ll}a_{21} & a_{22} \\\\ a_{31} & a_{32}\\end{array}\\right| \\\\ & \\end{aligned}$\n",
        "\n",
        "Enfin, le déterminant peut également être utilisé pour déterminer si une matrice est inversible.\n",
        "Cela est dû au fait que le déterminant d'une matrice et de son inverse sont inversement proportionnels. À mesure que le déterminant d'une matrice augmente, l'échelle de sa transformation augmente également, et l'échelle de son inverse diminue en conséquence. Si une matrice a un grand déterminant, son inverse aura un déterminant plus petit, et vice versa. Lorsque le déterminant est nul, la matrice n'est pas inversible.\n",
        "Pour comprendre précisément pourquoi, dérivons la formule pour l'inverse d'une matrice $2\\times2$ en utilisant l'élimination gaussienne.\n",
        "\n",
        "$\\boldsymbol{A}=\\left[\\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{array}\\right]$\n",
        "\n",
        "$\\left[\\begin{array}{ll|ll}a_{11} & a_{12} & 1 & 0 \\\\ a_{21} & a_{22} & 0 & 1\\end{array}\\right]$\n",
        "\n",
        "$\\left[\\begin{array}{cc|cc}1 & 0 & \\frac{a_{22}}{a_{11} a_{22}-a_{12} a_{21}} & -\\frac{a_{12}}{a_{11} a_{22}-a_{12} a_{21}} \\\\ 0 & 1 & \\frac{-a_{21}}{a_{11} a_{22}-a_{12} a_{21}} & \\frac{a_{11}}{a_{11} a_{22}-a_{12} a_{21}}\\end{array}\\right]$\n",
        "\n",
        "$\\begin{aligned} \\boldsymbol{A}^{-\\mathbf{1}} & =\\frac{1}{a_{11} a_{22}-a_{12} a_{21}}\\left[\\begin{array}{cc}a_{22} & -a_{12} \\\\ -a_{21} & a_{11}\\end{array}\\right] \\\\ \\boldsymbol{A}^{-\\mathbf{1}} & =\\frac{1}{\\operatorname{det}(\\boldsymbol{A})}\\left[\\begin{array}{cc}a_{22} & -a_{12} \\\\ -a_{21} & a_{11}\\end{array}\\right]\\end{aligned}$\n",
        "\n",
        "Nous pouvons voir comment le déterminant d'une matrice et de son inverse sont inversement proportionnels, et pourquoi l'inverse n'existe pas\n",
        "\n",
        " lorsque $det(A)=0$.\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`THÉORÈME`</font> `Inversibilité`\n",
        "\n",
        "Pour toute matrice carrée $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, on a $\\mathbf{A}$ est inversible si et seulement si $\\operatorname{det}(\\mathbf{A}) \\neq 0$.\n",
        "***"
      ],
      "metadata": {
        "id": "U2CxWiJ9f4Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche mathématique :**\n",
        "\n",
        "Vous êtes un astronaute explorant une planète lointaine réputée pour ses formations de cristaux particulières. Dans votre analyse, vous tombez sur une magnifique matrice de cristaux qui semble détenir les secrets de l'énergie de la planète. La matrice décrit la relation entre deux types de sources d'énergie : les rayons solaires et les impulsions magnétiques.\n",
        "\n",
        "La matrice de cristaux que vous avez rassemblée ressemble à ceci :\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "5 & 2 \\\\\n",
        "-3 & 7\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Votre directive de mission est de calculer le déterminant de cette matrice de cristaux pour déterminer l'équilibre énergétique et la stabilité potentielle de l'écosystème de la planète. Calculez le déterminant pour dévoiler les forces énigmatiques qui gouvernent l'harmonie énergétique de ce monde extraterrestre."
      ],
      "metadata": {
        "id": "V-tBrzS0gVOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Trace - <font color='blue'>`Débutant`</font>"
      ],
      "metadata": {
        "id": "FV03wpuUgjwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le trace est une autre propriété importante d'une matrice de transformation. Le trace d'une matrice représente la somme de ses éléments diagonaux. Il correspond à la somme des étirements le long des axes principaux d'une transformation. Le trace capture l'effet global d'échelle de la transformation.\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`DÉFINITION`</font> `TRACE`\n",
        "\n",
        "Le trace d'une matrice carrée $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ est défini comme\n",
        "$$\n",
        "\\operatorname{tr}(\\mathbf{A})=\\sum_{i=1}^n a_{i i}\n",
        "$$\n",
        "***\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`EXEMPLE`</font>\n",
        "\n",
        "$\\begin{aligned} \\operatorname{tr}(\\boldsymbol{A})=\\operatorname{tr}\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 4\\end{array}\\right] = 1+4 =5\\end{aligned}$\n",
        "\n",
        "$\\begin{aligned} \\operatorname{tr}(\\boldsymbol{A})=\\operatorname{tr}\\left[\\begin{array}{ll}1 & 0 \\\\ 3 & 4\\end{array}\\right] = 1+4 =5\\end{aligned}$\n",
        "\n",
        "$\\begin{aligned} \\operatorname{tr}(\\boldsymbol{A})=\\operatorname{tr}\\left[\\begin{array}{ll}1 & 2 \\\\ 3 & 0\\end{array}\\right] = 1+0 = 1\\end{aligned}$\n",
        "***"
      ],
      "metadata": {
        "id": "bue4Iu_Agyd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche mathématique :**\n",
        "\n",
        "Calculez le trace de la matrice suivante à partir de la tâche précédente :\n",
        "$$\n",
        "A=\\begin{bmatrix}\n",
        "5 & 2 \\\\\n",
        "-3 & 7\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "kwYLsJQ5hHhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ANSWER`\n",
        "Pour calculer le trace d'une matrice, il suffit de faire la somme de ses éléments diagonaux. Dans ce cas, le trace de la matrice $A$ serait :\n",
        "\n",
        "$$\n",
        "\\operatorname{tr}(A) = 5 + 7 = 12\n",
        "$$\n",
        "\n",
        "Ainsi, le trace de la matrice $A$ est égal à $12$."
      ],
      "metadata": {
        "id": "oEozd_SEhgh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Valeurs propres et vecteurs propres**"
      ],
      "metadata": {
        "id": "mDU4JsnFhyV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imaginez que vous êtes un archéologue enquêtant sur une collection de poteries d'une ancienne civilisation. Pour comprendre les motifs sous-jacents dans les designs, vous décidez d'appliquer l'analyse en composantes principales (PCA) aux motifs de la collection. Chaque pièce de poterie est ornée de combinaisons uniques de deux motifs : des spirales et des formes géométriques.\n",
        "\n",
        "Après des mesures minutieuses, vous construisez la matrice de covariance suivante basée sur les occurrences de ces motifs :\n",
        "\n",
        "$$\n",
        "\\boldsymbol{A}=\\left[\\begin{array}{cc}\n",
        "0.5 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "Pour effectuer une PCA, vous devez d'abord effectuer une décomposition en valeurs propres de cette matrice de covariance afin de révéler les composants artistiques fondamentaux (vecteurs propres) qui façonnent l'esthétique des poteries ainsi que leur importance artistique correspondante (valeurs propres).\n",
        "\n",
        "L'illustration suivante représente la transformation représentée par $A$ - les points sur la figure de gauche sont transformés en ceux montrés sur la figure de droite. Les flèches au milieu montrent les deux vecteurs propres et chaque $\\lambda$ est leur valeur propre respective.\n",
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-yhWf-K5IS6rw0jpuR20SAjxpGTWTJ6WtGtoY2dNrQoiGuwR9Msld8I_ZeEAiepWOk2kfuRlpfR--QKQyujOxj6RapwGg=s1600\"\n",
        "width=\"60%\" />\n",
        "\n",
        "Une valeur propre de 2 suggère que le motif des formes géométriques présente plus de variation par rapport au motif des spirales, qui a une valeur propre de 0.5. Les vecteurs propres fournissent des informations sur la manière dont les motifs sont liés et sur la manière dont ils contribuent aux variations globales observées dans la collection de poteries.\n",
        "\n",
        "Supposons que la matrice de covariance était\n",
        "$\\boldsymbol{A}=\\left[\\begin{array}{cc}\n",
        "1 & -1 \\\\\n",
        "-1 & 1\n",
        "\\end{array}\\right]$\n",
        "à la place. Alors la figure serait comme indiquée ci-dessous.\n",
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-yxfZ21Sf782jxEJTAX0X7FRKG7O5ByeA3DZO292dBor33Wjpgt6BTwtZEwg0uWSM65grEiFjmQvnavrgCrg2N7pCiWmA=s1600\"\n",
        "width=\"60%\" />\n",
        "\n",
        "Les valeurs propres de 0 et 2 impliquent que la transformation de la matrice s'étire dans une direction (associée à la valeur propre 2) tout en s'effondrant complètement dans une autre direction (associée à la valeur propre 0).\n",
        "\n",
        "Dans le contexte des motifs de poterie, cela signifierait que l'un des motifs (représenté par le vecteur propre associé à la valeur propre 0) n'a pas de variabilité et ne contribue pas aux différences de motifs dans la collection. Cela pourrait signifier qu'un des motifs est constant sur toutes les pièces de poterie.\n",
        "\n",
        "Le motif associé au vecteur propre correspondant à la valeur propre 2 serait responsable de toute la variabilité et des différences de motifs observées dans la collection de poterie. Ce motif serait le contributeur principal aux variations de conception.\n",
        "\n",
        "En termes plus simples, l'un des motifs reste inchangé sur toutes les pièces, tandis que l'autre motif présente des motifs variables qui conduisent aux différences observées. Cette situation pourrait indiquer une situation où l'un des motifs est constant (ou peut-être simplement un élément d'arrière-plan), et l'autre motif est la principale source d'expression artistique et de variation dans la collection.\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`DÉFINITION`</font> `Valeurs Propres et Vecteurs Propres`\n",
        "\n",
        "Soit $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ une matrice carrée.\n",
        "- Alors $\\lambda$ est une valeur propre de $\\mathbf{A}$ et $\\mathbf{x} \\in \\mathbb{R}^n \\backslash\\{\\mathbf{0}\\}$ est le vecteur propre correspondant de $\\mathbf{A}$ si\n",
        "$$\n",
        "\\mathbf{A x}=\\lambda \\mathbf{x}.\n",
        "$$\n",
        "Nous appelons cette équation l'\"équation des valeurs propres\".\n",
        "***"
      ],
      "metadata": {
        "id": "urVzRNz9h1BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Valeurs Propres - <font color='blue'>`Débutant`</font>"
      ],
      "metadata": {
        "id": "FzJeqxlGiOOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Géométriquement, la valeur propre d'une matrice représente le facteur d'échelle selon lequel une transformation de matrice étire ou compresse un vecteur (vecteur propre). Elle indique dans quelle mesure la direction du vecteur change sous la transformation.\n",
        "\n",
        "Pour calculer la valeur propre en utilisant le sens géométrique fourni :\n",
        "\n",
        "1. Commencez avec l'équation : $A x = \\lambda x$, où $A$ est la matrice, $\\lambda$ est la valeur propre et $x$ est le vecteur propre.\n",
        "2. Réarrangez pour obtenir $A x - \\lambda x = 0$.\n",
        "3. Factorisez $x$ pour obtenir $A x - \\lambda I x = 0$, où $I$ est la matrice identité.\n",
        "4. Factorisez à nouveau $x$ pour obtenir $(A - \\lambda I) x = 0$.\n",
        "5. Pour une solution non triviale ($x \\neq 0$), la matrice $A - \\lambda I$ doit être singulière, ce qui signifie que son déterminant est 0.\n",
        "6. Mettez en place l'équation $\\text{det}(A - \\lambda I) = 0$ et résolvez pour $\\lambda$. Cette équation vous donne les valeurs propres de la matrice $A$.\n",
        "\n",
        "En résumé, la valeur propre est une valeur qui satisfait l'équation $A x = \\lambda x$, ce qui signifie que lorsqu'une transformation de matrice est appliquée à un vecteur propre, le vecteur résultant est mis à l'échelle par la valeur propre. La valeur propre peut être trouvée en résolvant l'équation du déterminant $\\text{det}(A - \\lambda I) = 0$.\n",
        "\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`EXEMPLE`</font>\n",
        "\n",
        "$$\n",
        "\\boldsymbol{A}=\\begin{bmatrix}\n",
        "0.5 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Trouver les valeurs propres $\\left(\\lambda_1, \\lambda_2\\right)$ :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\operatorname{det}(\\boldsymbol{A}-\\lambda I)=0 \\\\\n",
        "& \\operatorname{det}\\left(\\begin{bmatrix}\n",
        "0.5 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{bmatrix}-\\lambda\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\\right)=0 \\\\\n",
        "& \\operatorname{det}\\left(\\begin{bmatrix}\n",
        "0.5-\\lambda & 0 \\\\\n",
        "0 & 2-\\lambda\n",
        "\\end{bmatrix}\\right)=0 \\\\\n",
        "& (0.5-\\lambda)(2-\\lambda)=0 \\\\\n",
        "& 0.5-\\lambda=0 \\text { ou } 2-\\lambda=0 \\\\\n",
        "& \\lambda_1=0.5 \\text { et } \\lambda_2=2\n",
        "\\end{aligned}\n",
        "$$\n",
        "***"
      ],
      "metadata": {
        "id": "-WDCCoYXiO1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche mathématique**\n",
        "\n",
        "Calcul des valeurs propres de la matrice suivante :\n",
        "$$\n",
        "\\boldsymbol{A}=\\begin{bmatrix}\n",
        "1 & 0.5 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "6J1NFMBNjKAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Solution`\n",
        "\n",
        "\n",
        "<img src=\n",
        "\"https://lh3.googleusercontent.com/drive-viewer/AITFw-xdFyBFBO8I9XX2FFN6VnD4DK_rKJxneSdhDAh8XtM70qfuMJmxOaFiqCV3doIl6W0gQTNJzmxVYuB9MuW9IXxl9dVEng=s1600\"\n",
        "width=\"60%\" />"
      ],
      "metadata": {
        "id": "pIrdSZRnj258"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Vecteurs propres - <font color='orange'>`Intermédiaire`</font>"
      ],
      "metadata": {
        "id": "mrQ5KSpCj59T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Géométriquement, un vecteur propre d'une matrice représente une direction dans l'espace qui reste inchangée en direction (à une mise à l'échelle près) lorsque la matrice est appliquée en tant que transformation. En d'autres termes, c'est un vecteur qui est seulement étiré ou comprimé par la matrice, sans changer son orientation.\n",
        "\n",
        "Pour calculer le vecteur propre correspondant à une valeur propre donnée d'une matrice $A$ :\n",
        "\n",
        "1. Commencez par l'équation $A x = \\lambda x$, où $A$ est la matrice, $\\lambda$ est la valeur propre, et $x$ est le vecteur propre.\n",
        "2. Réarrangez pour obtenir $(A - \\lambda I) x = 0$, où $I$ est la matrice identité.\n",
        "3. Résolvez le système d'équations linéaires $(A - \\lambda I) x = 0$ pour $x$.\n",
        "4. Le vecteur solution $x$ est le vecteur propre correspondant à la valeur propre donnée $\\lambda$.\n",
        "\n",
        "En résumé, le vecteur propre est trouvé en résolvant l'équation $(A - \\lambda I) x = 0$ pour le vecteur $x$, qui satisfait la relation de transformation $A x = \\lambda x$ avec la valeur propre donnée.\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`EXEMPLE`</font>\n",
        "\n",
        "Souvenez-vous de l'exemple précédent\n",
        "$\\boldsymbol{A}=\\left[\\begin{array}{cc}\n",
        "0.5 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{array}\\right]$\n",
        "avec les valeurs propres $\\lambda_1 =0.5, \\lambda_2 =2$.\n",
        "\n",
        "Trouver les vecteurs propres $\\left(v_1, v_2\\right)$ :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& (A-\\lambda I) x=0 \\\\\n",
        "& {\\left[\\begin{array}{cc}\n",
        "0.5-\\lambda & 0 \\\\\n",
        "0 & 2-\\lambda\n",
        "\\end{array}\\right] x=0}\n",
        "\\end{aligned}\n",
        "$$\n",
        "Pour $\\lambda=0.5$\n",
        "$$\n",
        "\\left[\\begin{array}{cc}\n",
        "0 & 0 \\\\\n",
        "0 & 1.5\n",
        "\\end{array}\\right] x=0, \\quad \\text { Donc } v_1=\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "Pour $\\lambda=2$\n",
        "$$\n",
        "\\left[\\begin{array}{cc}\n",
        "-1.5 & 0 \\\\\n",
        "0 & 0\n",
        "\\end{array}\\right] x=0, \\quad \\text { Donc } v_2=\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "Aeirkprjkfxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche mathématique :**\n",
        "\n",
        "Supposons que vous travailliez sur un projet de vision par ordinateur et que vous souhaitiez réduire la dimensionnalité des données d'image pour améliorer la précision de la classification. Vous pouvez utiliser l'analyse en composantes principales (PCA) pour trouver les directions les plus importantes de variation dans les données. Étant donné la matrice de covariance suivante d'un ensemble de 3 images :\n",
        "\n",
        "$$\n",
        "\\boldsymbol{A}=\\left[\\begin{array}{lll}\n",
        "1 & 0 & 0 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "Effectuez une décomposition en valeurs propres sur la matrice de covariance de l'ensemble de données pour identifier les composantes principales (vecteurs propres) et leurs valeurs propres associées.\n",
        "\n",
        "a) Quelles sont ses valeurs propres $\\lambda_1, \\lambda_2, \\lambda_3$ ?\n",
        "\n",
        "b) Quels sont ses vecteurs propres $v_1, v_2, v_3$ ?"
      ],
      "metadata": {
        "id": "g2q639J6lDTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RÉPONSE**\n",
        "\n",
        "Calcul des valeurs propres $\\left(\\lambda_1, \\lambda_2, \\lambda_3\\right)$ :\n",
        "$$\n",
        "\\operatorname{det}(\\boldsymbol{A}-\\lambda I)=\\mathbf{0}\n",
        "$$\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\operatorname{det}\\left(\\left[\\begin{array}{lll}\n",
        "1 & 0 & 0 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9\n",
        "\\end{array}\\right]-\\lambda\\left[\\begin{array}{lll}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{array}\\right]\\right)=0 \\\\\n",
        "& \\operatorname{det}\\left(\\left[\\begin{array}{ccc}\n",
        "1-\\lambda & 0 & 0 \\\\\n",
        "4 & 5-\\lambda & 6 \\\\\n",
        "7 & 8 & 9-\\lambda\n",
        "\\end{array}\\right]\\right)=0 \\\\\n",
        "& (-1)^{1+1}(1-\\lambda)\\left|\\begin{array}{cc}\n",
        "5-\\lambda \\\\\n",
        "8 & 6-\\lambda\n",
        "\\end{array}\\right|=0 \\\\\n",
        "& (1-\\lambda)((5-\\lambda)(9-\\lambda)-48)=0 \\\\\n",
        "& (1-\\lambda)\\left(45-5 \\lambda-9 \\lambda+\\lambda^2-48\\right)=0 \\\\\n",
        "& (1-\\lambda)\\left(\\lambda^2-14 \\lambda-3\\right)=0 \\\\\n",
        "& 1-\\lambda=0 \\quad \\text { ou } \\lambda^2-14 \\lambda-3=0 \\\\\n",
        "& \\lambda_1=1, \\quad \\lambda_2=7+2 \\sqrt{13}, \\quad \\lambda_3=7-2 \\sqrt{13}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Calcul des vecteurs propres $\\left(v_1, v_2, v_3\\right)$ :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& (A-\\lambda I) x=0 \\\\\n",
        "& {\\left[\\begin{array}{ccc}\n",
        "1-\\lambda & 0 & 0 \\\\\n",
        "4 & 5-\\lambda & 6 \\\\\n",
        "7 & 8 & 9-\\lambda\n",
        "\\end{array}\\right] x=0}\n",
        "\\end{aligned}\n",
        "$$\n",
        "$$\n",
        "\\left[\\begin{array}{ccc}\n",
        "1-\\lambda & 0 & 0 \\\\\n",
        "4 & 5-\\lambda & 6 \\\\\n",
        "7 & 8 & 9-\\lambda\n",
        "\\end{array}\\right] \\boldsymbol{x}=\\mathbf{0}\n",
        "$$\n",
        "Pour $\\lambda=1$\n",
        "$$\n",
        "\\left[\\begin{array}{lll}\n",
        "0 & 0 & 0 \\\\\n",
        "4 & 4 & 6 \\\\\n",
        "7 & 8 & 8\n",
        "\\end{array}\\right] x=0, \\quad \\text { Ainsi } \\quad v_1=\\left[\\begin{array}{c}\n",
        "4 \\\\\n",
        "-\\frac{5}{2} \\\\\n",
        "-1\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\text { Pour } \\lambda=7+2 \\sqrt{13} \\\\\n",
        "& {\\left[\\begin{array}{ccc}\n",
        "-6-2 \\sqrt{13} & 0 & 0 \\\\\n",
        "4 & -2-2 \\sqrt{13} & 6 \\\\\n",
        "7 & 8 & 2-2 \\sqrt{13}\n",
        "\\end{array}\\right] x=0, \\quad \\text { Ainsi } v_2=\\left[\\begin{array}{c}\n",
        "\\mathbf{0} \\\\\n",
        "\\mathbf{1}-\\sqrt{\\mathbf{1 3}} \\\\\n",
        "\\hline \\mathbf{4} \\\\\n",
        "\\mathbf{- 1}\n",
        "\\end{array}\\right]}\n",
        "\\end{aligned}\n",
        "$$\n",
        "Pour $\\lambda=7-2 \\sqrt{13}$\n",
        "$$\n",
        "\\left[\\begin{array}{ccc}\n",
        "-6+2 \\sqrt{13} & 0 & 0 \\\\\n",
        "4 & -2+2 \\sqrt{13} & 6 \\\\\n",
        "7 & 8 & 2+2 \\sqrt{13}\n",
        "\\end{array}\\right] x=0, \\quad \\text { Ainsi } v_3=\\left[\\begin{array}{c}\n",
        "\\mathbf{0} \\\\\n",
        "\\mathbf{1}+\\sqrt{\\mathbf{1 3}} \\\\\n",
        "\\hline \\mathbf{4} \\\\\n",
        "\\mathbf{- 1}\n",
        "\\end{array}\\right]\n",
        "$$"
      ],
      "metadata": {
        "id": "UITLa-XTlFca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Décomposition en Valeurs Propres - <font color='orange'>`Intermédiaire`</font>"
      ],
      "metadata": {
        "id": "h4kYfow-ljoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La décomposition en valeurs propres d'une matrice implique l'expression de la matrice comme une combinaison de ses vecteurs propres et de ses valeurs propres. Elle nous permet de décomposer une transformation complexe en transformations plus simples le long de directions spécifiques, représentées par les vecteurs propres, chacune étant mise à l'échelle par sa valeur propre correspondante. Cela nous donne un aperçu de la façon dont la matrice déforme et met à l'échelle l'espace dans différentes directions.\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`DÉFINITION`</font> `Décomposition en Valeurs Propres`\n",
        "\n",
        "$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ peut être factorisée en\n",
        "$$\n",
        "\\mathbf{A}=\\mathbf{P D P}^{-1}\n",
        "$$\n",
        "où $\\mathbf{P} \\in \\mathbb{R}^{n \\times n}$ est une matrice dont les colonnes sont tous les vecteurs propres, et $\\mathbf{D}$ est une matrice diagonale dont les entrées diagonales sont les valeurs propres de $\\mathbf{A}$\n",
        "- si et seulement si les vecteurs propres de $\\mathbf{A}$ forment une base de $\\mathbf{R}^n$.\n",
        "\n",
        "En bref, seules les matrices non défectueuses peuvent être diagonalisées de cette manière.\n",
        "***\n",
        "\n",
        "***\n",
        "<font color='Yellow'>`EXEMPLE`</font>\n",
        "\n",
        "\\begin{aligned}\n",
        "& \\boldsymbol{A}=\\left[\\begin{array}{cc}\n",
        "0,5 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{array}\\right] ; \\quad \\text { Valeurs Propres } \\lambda_1=0,5, \\lambda_2=2 ; \\quad \\text { Vecteurs Propres } \\boldsymbol{v}_{\\mathbf{1}}=\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right], \\boldsymbol{v}_2=\\left[\\begin{array}{l}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right] \\\\\n",
        "& \\boldsymbol{A}=\\boldsymbol{P} \\boldsymbol{D P}^{-1}=\\left[\\begin{array}{ll}\n",
        "v_1 & v_2\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "\\lambda_1 & 0 \\\\\n",
        "0 & \\lambda_2\n",
        "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
        "v_1 & v_2\n",
        "\\end{array}\\right]^{-1}=\\left[\\begin{array}{cc}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
        "0,5 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{array}\\right]^{-1}\n",
        "\\end{aligned}\n",
        "***"
      ],
      "metadata": {
        "id": "-XqqibpvKO-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imaginez que vous explorez une maison hantée remplie d'objets effrayants. Pour mieux comprendre l'aspect lugubre, vous décidez d'utiliser la décomposition en valeurs propres. Vous collectez des données sur les occurrences de deux types d'objets effrayants : les chauves-souris et les chats noirs.\n",
        "\n",
        "Vous obtenez la matrice de covariance suivante qui résume les relations entre les apparitions de chauves-souris et de chats noirs dans vos données sur plusieurs jours :\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 3 \\\\\n",
        "3 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Effectuez une décomposition en valeurs propres sur cette matrice de covariance pour révéler les composantes spectrales (vecteurs propres) qui définissent les vibrations inquiétantes et leurs intensités spectrales correspondantes (valeurs propres).\n",
        "\n",
        "a) Quelles sont ses valeurs propres $\\lambda_1, \\lambda_2$ ?\n",
        "\n",
        "b) Quels sont ses vecteurs propres $v_1, v_2$ ?\n",
        "\n",
        "c) Quelle est sa décomposition en valeurs propres $A = PDP^{-1}$ ?"
      ],
      "metadata": {
        "id": "1cILYJicKPWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SOLUTION`\n",
        ">\\begin{aligned}\n",
        "\\lambda_1=-2,\n",
        "\\lambda_2=4,\n",
        "v_1 = {\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "-1\n",
        "\\end{array}\\right]} ,\n",
        "v_2 = {\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "1\n",
        "\\end{array}\\right]} ,\n",
        "A = {\\left[\\begin{array}{cc}\n",
        "1 & 1 \\\\\n",
        "-1 & 1\n",
        "\\end{array}\\right]}\n",
        " {\\left[\\begin{array}{cc}\n",
        "-2 & 0 \\\\\n",
        "0 & 4\n",
        "\\end{array}\\right]}\n",
        " {\\left[\\begin{array}{cc}\n",
        "\\frac{1}{2} & -\\frac{1}{2} \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2}\n",
        "\\end{array}\\right]}\n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "eD4Q3-1WKPpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exemple : Analyse en Composantes Principales (ACP)**"
      ],
      "metadata": {
        "id": "LJoKTsGBKP5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous mettrons en œuvre l'algorithme de l'ACP en utilisant la perspective de la projection. Nous implémenterons d'abord l'ACP, puis nous l'appliquerons à l'ensemble de données des chiffres MNIST."
      ],
      "metadata": {
        "id": "OA3jQUFsKQMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imaginez que vous ayez un tas de points de données, comme des images de chiffres manuscrits provenant de différentes personnes (c'est l'ensemble de données MNIST). Chaque point de données ressemble à une petite flèche dans un espace multidimensionnel. Mais parfois, cet espace est trop compliqué, et il est difficile de voir les vrais motifs.\n",
        "\n",
        "L'ACP vient à la rescousse ! C'est comme un tour de magie qui nous aide à simplifier les choses. Voici comment cela fonctionne :\n",
        "\n",
        "1. **Collectez Vos Données :** Supposons que vous ayez des données d'images montrant comment différentes personnes écrivent les chiffres de 0 à 9 (comme l'ensemble de données MNIST). Vous organisez ces données dans une matrice appelée \"X\" où chaque ligne est un vecteur de pixels pour chaque image de chiffre manuscrit :\n",
        "\n",
        "   $$ X = images = \\begin{bmatrix}\n",
        "   \\text{pixel}_1 & \\text{pixel}_1 & \\text{pixel}_1 \\\\\n",
        "   \\text{pixel}_2 & \\text{pixel}_2 & \\text{pixel}_2 \\\\\n",
        "   \\vdots & \\vdots & \\vdots \\\\\n",
        "   \\text{pixel}_n & \\text{pixel}_n & \\text{pixel}_n \\\\\n",
        "   \\end{bmatrix} $$"
      ],
      "metadata": {
        "id": "g9_AkbB2KQev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loding MNIST dataset\n",
        "digits = load_digits(n_class=6) # low-dimensional MNIST dataset\n",
        "images, labels = digits.data, digits.target\n",
        "size = 8\n",
        "if True: # Make it True to use the high-dimensional MNIST dataset\n",
        "    images, labels = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "    size = 28\n",
        "print(\"Matix shape:\", images.shape)\n",
        "print(\"Dimension of each image vector is:\", size*size)\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')"
      ],
      "metadata": {
        "id": "FLJKn5OuMVmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the first digit from the dataset:\n",
        "plt.figure(figsize=(4,4))\n",
        "images = images.to_numpy()\n",
        "plt.imshow(images[0].reshape((size,size)), cmap='gray');\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"First digit from the {size*size}-dimensional digits dataset\", fontsize=16)\n",
        "\n",
        "# Plotting the first 25 digits from the dataset:\n",
        "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
        "for idx, ax in enumerate(axs.ravel()):\n",
        "    ax.imshow(images[idx].reshape((size, size)), cmap=plt.cm.binary)\n",
        "    ax.axis(\"off\")\n",
        "_ = fig.suptitle(f\"A selection from the {size*size}-dimensional digits dataset\", fontsize=16)"
      ],
      "metadata": {
        "id": "KhpO5POHMWcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Normalisation des Données :** Avant de mettre en œuvre l'ACP, nous devrons effectuer un prétraitement des données pour garantir que les points de données sont plus centrés autour de l'origine.\n",
        "Les étapes de prétraitement que nous allons effectuer sont les suivantes :\n",
        " 1. Convertir l'encodage en entier non signé 8 (uint8) des pixels en un nombre à virgule flottante compris entre 0 et 1.\n",
        " 2. Soustraire de chaque image la moyenne $\\boldsymbol \\mu$.\n",
        " 3. Mettre à l'échelle chaque dimension de chaque image par $\\frac{1}{\\sigma}$ où $\\sigma$ est l'écart type.\n",
        "\n",
        " Les étapes ci-dessus garantissent que nos images auront une moyenne nulle et une variance de un. Ces étapes de prétraitement sont également connues sous le nom de [normalisation des données ou mise à l'échelle des caractéristiques](https://fr.wikipedia.org/wiki/Normalisation_de_donn%C3%A9es)."
      ],
      "metadata": {
        "id": "kcyPFsFtKQx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(X):\n",
        "    \"\"\"Normalize the given dataset X\n",
        "    Args:\n",
        "        X: ndarray, dataset\n",
        "\n",
        "    Returns:\n",
        "        (Xbar, mean, std): tuple of ndarray, Xbar is the normalized dataset\n",
        "        with mean 0 and standard deviation 1; mean and std are the\n",
        "        mean and standard deviation respectively.\n",
        "\n",
        "    Note:\n",
        "        You will encounter dimensions where the standard deviation is\n",
        "        zero, for those when you do normalization the normalized data\n",
        "        will be NaN. Handle this by setting using `std = 1` for those\n",
        "        dimensions when doing normalization.\n",
        "    \"\"\"\n",
        "    mu = np.mean(X, axis=0)\n",
        "    std = np.std(X, axis=0)\n",
        "    std_filled = std.copy()\n",
        "    std_filled[std==0] = 1.\n",
        "    Xbar = ((X-mu)/std_filled)\n",
        "    return Xbar, mu, std"
      ],
      "metadata": {
        "id": "OkgkzA2MMyC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Calcul de la Matrice de Covariance :** Vous calculez la \"covariance\" entre les différentes colonnes de vos données centrées. La covariance vous indique comment les colonnes changent ensemble. Mathématiquement, vous calculez la matrice de covariance en multipliant la matrice transposée $X^T$ avec $X$ :\n",
        "\n",
        "   $$ \\text{Matrice de Covariance (S)} = X^T \\cdot X $$\n",
        "\n",
        "4. **Trouver les Vecteurs Propres et les Valeurs Propres :** Cette partie semble complexe, mais suivez-moi. Les vecteurs propres sont des directions spéciales dans l'espace de vos données, et les valeurs propres vous indiquent l'importance de ces directions. Lorsque vous les calculez, vous trouvez les composantes principales. Heureusement, nous savons maintenant comment les calculer à partir des sections précédentes.\n",
        "\n",
        "5. **Trier les Vecteurs Propres par les Valeurs Propres :** Vous arrangez les vecteurs propres dans l'ordre de leurs valeurs propres correspondantes, de la plus grande à la plus petite. De cette manière, vous placez d'abord les composantes les plus importantes.\n",
        "\n",
        "6. **Choisir Combien de Composantes Garder :** Selon la quantité d'informations que vous souhaitez conserver, vous décidez combien de vecteurs propres (composantes principales) à garder. Généralement, vous conservez les premiers qui capturent la majeure partie de la variation des données.\n",
        "\n",
        "7. **Projeter Vos Données :** Vous multipliez maintenant vos données centrées par la matrice de projection des vecteurs propres sélectionnés. Cela transforme vos données en un nouvel espace, où chaque point de données est décrit avec moins de dimensions, les composantes principales.\n",
        "\n",
        "Et voilà ! Vous comprenez maintenant comment utiliser l'ACP pour simplifier vos données. Ces nouvelles dimensions (composantes principales) sont comme de nouvelles façons de regarder vos données qui mettent en évidence l'essentiel. C'est comme transformer un puzzle complexe en une image plus simple.\n",
        "\n",
        "N'oubliez pas, l'ACP est comme un outil qui vous aide à vous concentrer sur l'histoire principale que vos données veulent raconter, sans vous embourber dans tous les détails supplémentaires. Essayons maintenant de la mettre en œuvre."
      ],
      "metadata": {
        "id": "d_p3UJh_KQ_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 ACP pour un ensemble de données de faible dimension - <font color='blue'>`Débutant`</font>"
      ],
      "metadata": {
        "id": "o6D1ou6XKRTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant, nous allons mettre en œuvre l'ACP en suivant le processus décrit ci-dessus. C'est-à-dire, commencez par normaliser les données (`normaliser`). Ensuite, trouvez les valeurs propres et les vecteurs propres correspondants de la matrice de covariance $S$.\n",
        "Triez selon les plus grandes valeurs propres et les vecteurs propres correspondants (`valeurs_propres`).\n",
        "Après ces étapes, nous pouvons ensuite calculer la projection et la reconstruction des données sur l'espace engendré par les $n$ premiers vecteurs propres."
      ],
      "metadata": {
        "id": "qBNuzTvLKRhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche de code :**"
      ],
      "metadata": {
        "id": "EI_CnRrBNQeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PCA(X, num_components):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        X: ndarray of size (N, D), where D is the dimension of the data,\n",
        "           and N is the number of datapoints\n",
        "        num_components: the number of principal components to use.\n",
        "    Returns:\n",
        "        X_reconstruct: ndarray of the reconstruction\n",
        "        of X from the first `num_components` principal components.\n",
        "    \"\"\"\n",
        "    # your solution should take advantage of the functions we have given you and that you have implemented above.\n",
        "\n",
        "    # first perform normalization on the digits so that they have zero mean and unit variance\n",
        "    # Then compute the data covariance matrix S (remember the convariance matrix is given by the dot product)\n",
        "    ### TODO\n",
        "\n",
        "    # Next find eigenvalues and corresponding eigenvectors for S\n",
        "    ### TODO\n",
        "\n",
        "    # find indices for the largest eigenvalues, use them to sort the eigenvalues and\n",
        "    # corresponding eigenvectors. Take a look at the documenation fo `np.argsort`\n",
        "    # (https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html),\n",
        "    # which might be useful\n",
        "    ### TODO\n",
        "\n",
        "    # dimensionality reduction of the original data\n",
        "    ### TODO\n",
        "\n",
        "    # reconstruct the images from the lower dimensional representation\n",
        "    ### TODO\n",
        "\n",
        "    return X # <-- EDIT THIS to return the reconstruction of X"
      ],
      "metadata": {
        "id": "-mmT4D7hNqOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exécutez-moi pour tester votre code.\n",
        "\n",
        "NUM_DATAPOINTS = 1024\n",
        "X = (images.reshape(-1, size * size)[:NUM_DATAPOINTS]) / 255.\n",
        "Xbar, mu, std = normalize(X)\n",
        "\n",
        "def test_PCA(PCA):\n",
        "  for num_component in range(1, 20):\n",
        "    from sklearn.decomposition import PCA as SKPCA\n",
        "    # We can compute a standard solution given by scikit-learn's implementation of PCA\n",
        "    pca = SKPCA(n_components=num_component, svd_solver='full')\n",
        "    sklearn_reconst = pca.inverse_transform(pca.fit_transform(Xbar))\n",
        "    reconst = PCA(Xbar, num_component)\n",
        "    np.testing.assert_almost_equal(reconst, sklearn_reconst)\n",
        "    print(\"number of components:\",num_component,\"reconstruction error:\", np.square(reconst - sklearn_reconst).sum())\n",
        "\n",
        "test_PCA(PCA)"
      ],
      "metadata": {
        "id": "kCu1DLHHNxpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Réponse à la tâche de code (Essayez de ne pas regarder avant d'avoir bien essayé !)\n",
        "\n",
        "def PCA(X, num_components):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        X: ndarray of size (N, D), where D is the dimension of the data,\n",
        "           and N is the number of datapoints\n",
        "        num_components: the number of principal components to use.\n",
        "    Returns:\n",
        "        X_reconstruct: ndarray of the reconstruction\n",
        "        of X from the first `num_components` principal components.\n",
        "    \"\"\"\n",
        "    # first perform normalization on the digits so that they have zero mean and unit variance\n",
        "    # Then compute the data covariance matrix S\n",
        "    S = 1.0/len(X) * np.dot(X.T, X)\n",
        "\n",
        "    # Next find eigenvalues and corresponding eigenvectors for S\n",
        "    eig_vals, eig_vecs = eig(S)\n",
        "\n",
        "    # find indices for the largest eigenvalues, use them to sort the eigenvalues and\n",
        "    # corresponding eigenvectors. Take a look at the documenation fo `np.argsort`\n",
        "    # (https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html),\n",
        "    # which might be useful\n",
        "    eig_vals, eig_vecs = eig_vals[:num_components], eig_vecs[:, :num_components]\n",
        "\n",
        "    # dimensionality reduction of the original data\n",
        "    B = np.real(eig_vecs)\n",
        "    # Z = X.T.dot(W)\n",
        "    # reconstruct the images from the lower dimensional representation\n",
        "    reconst = (projection_matrix(B) @ X.T)\n",
        "    return reconst.T\n",
        "\n",
        "test_PCA(PCA)"
      ],
      "metadata": {
        "id": "liV1aTO8N-Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FÉLICITATIONS !!! Vous comprenez maintenant et savez comment mettre en œuvre l'ACP (une version simplifiée où nous utilisons la décomposition en valeurs propres au lieu de la décomposition en valeurs singulières plus générale), l'une des techniques de réduction de dimension les plus populaires."
      ],
      "metadata": {
        "id": "8yddCH5xNQkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plus nous utilisons de composantes principales, plus notre erreur de reconstruction sera faible. Maintenant, répondons à la question suivante :\n",
        "\n",
        "> Combien de composantes principales avons-nous besoin afin d'atteindre une erreur quadratique moyenne (EQM) de moins de 100 pour notre ensemble de données ?\n",
        "\n",
        "Nous avons fourni une fonction dans la cellule suivante qui calcule l'erreur quadratique moyenne (EQM), ce qui sera utile pour répondre à la question ci-dessus."
      ],
      "metadata": {
        "id": "Mnpyy5AWNQt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(predict, actual):\n",
        "    \"\"\"Helper function for computing the mean squared error (MSE)\"\"\"\n",
        "    return np.square(predict - actual).sum(axis=1).mean()"
      ],
      "metadata": {
        "id": "XtTRpAh8O0gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons maintenant calculer l'erreur de reconstruction pour chaque nombre de composantes principales utilisées."
      ],
      "metadata": {
        "id": "XQvuMqZVNQ3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = []\n",
        "reconstructions = []\n",
        "# iterate over different numbers of principal components, and compute the MSE\n",
        "for num_component in range(1, 100):\n",
        "    reconst = PCA(Xbar, num_component)\n",
        "    error = mse(reconst, Xbar)\n",
        "    reconstructions.append(reconst)\n",
        "    # print('n = {:d}, reconstruction_error = {:f}'.format(num_component, error))\n",
        "    loss.append((num_component, error))\n",
        "\n",
        "reconstructions = np.asarray(reconstructions)\n",
        "reconstructions = reconstructions * std + mu # \"unnormalize\" the reconstructed image\n",
        "loss = np.asarray(loss)\n",
        "print(\"Loss/Error: \", loss)"
      ],
      "metadata": {
        "id": "rWv7D4lsPAhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons également mettre ces nombres en perspective en les représentant graphiquement."
      ],
      "metadata": {
        "id": "JhKNFrUFNRAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(loss[:,0], loss[:,1]);\n",
        "ax.axhline(10**len(str(size)), linestyle='--', color='r', linewidth=2)\n",
        "ax.xaxis.set_ticks(np.arange(1, 100, 5));\n",
        "ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs number of principal components');"
      ],
      "metadata": {
        "id": "ruHTpaU2Q-FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mais les chiffres ne nous disent pas tout ! Qu'est-ce que cela signifie qualitativement pour la perte de passer d'environ 55.0 à moins de 10.0 ? (ou de 550 à 100 pour l'ensemble de données à haute dimension)\n",
        "\n",
        "Découvrons-le ! Dans la cellule suivante, nous dessinons l'image la plus à gauche qui est le chiffre original. Ensuite, nous montrons la reconstruction de l'image à droite, en fonction du nombre décroissant de composantes principales utilisées."
      ],
      "metadata": {
        "id": "DIewayvzPHV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@interact(image_idx=(0, 1000))\n",
        "def show_num_components_reconst(image_idx):\n",
        "    fig, ax = plt.subplots(figsize=(20., 20.))\n",
        "    actual = X[image_idx]\n",
        "    # concatenate the actual and reconstructed images as large image before plotting it\n",
        "    x = np.concatenate([actual[np.newaxis, :], reconstructions[:, image_idx]])\n",
        "    ax.imshow(np.hstack(x.reshape(-1, size, size)[np.arange(10)]),\n",
        "              cmap='gray');\n",
        "    ax.axvline(size, color='orange', linewidth=2)"
      ],
      "metadata": {
        "id": "grQox1NrRQ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons également parcourir les reconstructions pour d'autres chiffres. Une fois de plus, l'interaction devient pratique pour visualiser la reconstruction."
      ],
      "metadata": {
        "id": "Hy_yJomRPHeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@interact(i=(0, 10))\n",
        "def show_pca_digits(i=1):\n",
        "    \"\"\"Show the i th digit and its reconstruction\"\"\"\n",
        "    plt.figure(figsize=(4,4))\n",
        "    actual_sample = X[i].reshape(size,size)\n",
        "    reconst_sample = (reconst[i, :] * std + mu).reshape(size, size)\n",
        "    plt.imshow(np.hstack([actual_sample, reconst_sample]), cmap='gray')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gaWVU1saRenO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 PCA pour un ensemble de données à haute dimension - <font color='orange'>`Intermédiaire`</font>"
      ],
      "metadata": {
        "id": "oepnSu76PHnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parfois, la dimension de notre ensemble de données peut être plus grande que le nombre d'échantillons que nous avons. Dans ce cas, il peut être inefficace d'effectuer la PCA avec notre mise en œuvre ci-dessus. À la place, nous pouvons mettre en œuvre la PCA de manière plus efficace, que nous appelons \"PCA pour les données à haute dimension\" (PCA_high_dim).\n",
        "\n",
        "Voici les étapes pour effectuer la PCA sur un ensemble de données à haute dimension :\n",
        " 1. Calculer la matrice $\\boldsymbol X\\boldsymbol X^T$ (une matrice de taille $N$ par $N$ avec $N \\ll D$)\n",
        " 2. Calculer les valeurs propres $\\lambda$ et les vecteurs propres $V$ pour $\\boldsymbol X\\boldsymbol X^T$\n",
        " 3. Calculer les vecteurs propres pour la matrice de covariance d'origine comme $\\boldsymbol X^T\\boldsymbol V$. Choisir les vecteurs propres associés aux M plus grandes valeurs propres pour former la base du sous-espace principal $U$.\n",
        " 4. Calculer la projection orthogonale des données sur le sous-espace engendré par les colonnes de $\\boldsymbol U$."
      ],
      "metadata": {
        "id": "0i8wFXesPHxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tâche de code :**"
      ],
      "metadata": {
        "id": "Mc9YoFoPPH6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PCA_high_dim(X, n_components):\n",
        "    \"\"\"Compute PCA for small sample size but high-dimensional features.\n",
        "    Args:\n",
        "        X: ndarray of size (N, D), where D is the dimension of the sample,\n",
        "           and N is the number of samples\n",
        "        num_components: the number of principal components to use.\n",
        "    Returns:\n",
        "        X_reconstruct: (N, D) ndarray. the reconstruction\n",
        "        of X from the first `num_components` pricipal components.\n",
        "    \"\"\"\n",
        "    return X # <-- EDIT THIS to return the reconstruction of X"
      ],
      "metadata": {
        "id": "HHG0DJ84SD2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étant donné le même jeu de données, `PCA_high_dim` et `PCA` devraient donner la même sortie. En supposant que nous ayons correctement implémenté `PCA`, nous pouvons alors utiliser `PCA` pour tester la justesse de `PCA_high_dim`. Nous pouvons utiliser cette __invariance__ pour tester notre implémentation de `PCA_high_dim`, en supposant que nous ayons correctement implémenté `PCA`."
      ],
      "metadata": {
        "id": "XhLOF-lwPIEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exécutez-moi pour tester votre code.\n",
        "def test_PCA_high_dim(PCA_high_dim):\n",
        "  np.testing.assert_almost_equal(PCA(Xbar, 2), PCA_high_dim(Xbar, 2))\n",
        "  print(\"Nice! Your answer looks correct.\")\n",
        "\n",
        "test_PCA_high_dim(PCA_high_dim)"
      ],
      "metadata": {
        "id": "zBFKO9YmSSON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Réponse à la tâche de code (Essayez de ne pas regarder avant d'avoir bien essayé !)\n",
        "\n",
        "def PCA_high_dim(X, n_components):\n",
        "    \"\"\"Compute PCA for small sample size but high-dimensional features.\n",
        "    Args:\n",
        "        X: ndarray of size (N, D), where D is the dimension of the sample,\n",
        "           and N is the number of samples\n",
        "        num_components: the number of principal components to use.\n",
        "    Returns:\n",
        "        X_reconstruct: (N, D) ndarray. the reconstruction\n",
        "        of X from the first `num_components` pricipal components.\n",
        "    \"\"\"\n",
        "    N, D = X.shape\n",
        "    M = np.dot(X, X.T) / N\n",
        "    eig_vals, eig_vecs = eig(M)\n",
        "    eig_vals, eig_vecs = eig_vals[:n_components], eig_vecs[:, :n_components]\n",
        "    U = (X.T @ (eig_vecs))\n",
        "    answer = np.zeros((N, D))\n",
        "    answer = ((U @ np.linalg.inv(U.T @ U) @ U.T) @ X.T).T\n",
        "    return answer\n",
        "\n",
        "test_PCA_high_dim(PCA_high_dim)"
      ],
      "metadata": {
        "id": "OD3LRev0STnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant comparons le temps d'exécution entre `PCA` et `PCA_high_dim`.\n",
        "\n",
        "**Conseils pour exécuter des tests de performances ou du code computationnellement coûteux** :\n",
        "Lorsque vous avez des calculs qui prennent un temps non négligeable, essayez de séparer le code qui génère la sortie du code qui analyse les résultats (par exemple, tracez les résultats, calculez les statistiques des résultats). De cette manière, vous n'avez pas besoin de recalculer lorsque vous souhaitez produire davantage d'analyses."
      ],
      "metadata": {
        "id": "RoYWoU-uPINn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_DATAPOINTS = 100\n",
        "X = (images.reshape(-1, size * size)[:NUM_DATAPOINTS]) / 255.\n",
        "Xbar, mu, std = normalize(X)\n",
        "\n",
        "%time PCA(Xbar, 2)\n",
        "%time PCA_high_dim(Xbar, 2)\n",
        "pass"
      ],
      "metadata": {
        "id": "1OWw03bHS9Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retour\n",
        "\n",
        "Veuillez fournir des commentaires que nous pouvons utiliser pour améliorer nos travaux pratiques à l'avenir."
      ],
      "metadata": {
        "id": "1cyntEn7Twzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Générer un formulaire de commentaires. (Exécuter la cellule)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/Cg9aoa7czoZCYqxF7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "VpA6bsKQTkoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ],
      "metadata": {
        "id": "oxUDD9hplhcx"
      }
    }
  ]
}